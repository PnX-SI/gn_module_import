2019-09-12 13:41:33,969 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:41:34,286 :: INFO ::  * Restarting with stat
2019-09-12 13:41:36,493 :: WARNING ::  * Debugger is active!
2019-09-12 13:41:36,493 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:41:42,764 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:41:43,166 :: INFO ::  * Restarting with stat
2019-09-12 13:41:44,876 :: WARNING ::  * Debugger is active!
2019-09-12 13:41:44,876 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:42:33,277 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:42:33,582 :: INFO ::  * Restarting with stat
2019-09-12 13:42:36,217 :: WARNING ::  * Debugger is active!
2019-09-12 13:42:36,218 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:42:40,011 :: INFO :: *** START UPLOAD USER FILE
2019-09-12 13:42:40,011 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-12 13:42:40,170 :: INFO :: File size = 10.37869644165039 Mo
2019-09-12 13:42:40,200 :: DEBUG :: original file name = data_pf_observado_corrected.csv
2019-09-12 13:42:40,200 :: INFO :: Upload : file saved in directory - time : 0:00:00.188687
2019-09-12 13:42:40,200 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-12 13:42:40,201 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-12 13:42:45,275 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'my_date', 'heure', 'date_encodage', 'my_timestamp', 'type_observation', 'nombre', 'my_min', 'my_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'latit', 'longit', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'my_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-12 13:42:45,277 :: DEBUG :: row_count = 26516
2019-09-12 13:42:45,277 :: INFO :: User file validity checked - time : 0:00:05.074845
2019-09-12 13:42:45,278 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-12 13:42:45,308 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_corrected_388
2019-09-12 13:42:45,588 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-12 13:42:46,209 :: INFO :: CSV loaded to DB table - time : 0:00:00.620008
2019-09-12 13:42:46,209 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-12 13:42:46,210 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-12 13:42:46,720 :: INFO :: CSV loaded to DB table - time : 0:00:00.509480
2019-09-12 13:42:46,720 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-12 13:42:46,903 :: INFO :: *** END UPLOAD USER FILE
2019-09-12 13:42:46,910 :: INFO :: Total time to post user file and fill metadata - time : 0:00:06.898876
2019-09-12 13:42:46,916 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:42:46] "POST /import/uploads HTTP/1.1" 200 -
2019-09-12 13:42:46,981 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:42:46] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-12 13:42:49,032 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:42:49] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 13:42:49,058 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 13:42:49,069 :: DEBUG :: import_id = 388
2019-09-12 13:42:49,069 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 13:42:49,069 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 13:42:49,069 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 13:42:49,087 :: WARNING :: ncores used by Dask = 4
2019-09-12 13:42:49,168 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-12 13:42:49,169 :: ERROR :: 'SQLAlchemy' object has no attribute 'execute'
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 550, in postMapping
    df = extract(table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, column_names, index_col, import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/extract/extract.py", line 33, in extract
    taxref_col_names = DB.execute("""
AttributeError: 'SQLAlchemy' object has no attribute 'execute'
2019-09-12 13:42:49,188 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:42:49] "POST /import/mapping/388 HTTP/1.1" 500 -
2019-09-12 13:43:03,919 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:43:04,334 :: INFO ::  * Restarting with stat
2019-09-12 13:43:06,642 :: WARNING ::  * Debugger is active!
2019-09-12 13:43:06,643 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:43:06,782 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:43:06] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 13:43:06,811 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 13:43:06,825 :: DEBUG :: import_id = 388
2019-09-12 13:43:06,825 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 13:43:06,825 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 13:43:06,825 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 13:43:06,847 :: WARNING :: ncores used by Dask = 4
2019-09-12 13:43:17,542 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 13:43:17,543 :: INFO ::  * Restarting with stat
2019-09-12 13:43:20,031 :: WARNING ::  * Debugger is active!
2019-09-12 13:43:20,032 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:43:20,038 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:43:20] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 13:43:20,066 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 13:43:20,090 :: DEBUG :: import_id = 388
2019-09-12 13:43:20,093 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 13:43:20,095 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 13:43:20,096 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 13:43:20,118 :: WARNING :: ncores used by Dask = 4
2019-09-12 13:43:56,994 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:43:57,721 :: INFO ::  * Restarting with stat
2019-09-12 13:43:59,778 :: WARNING ::  * Debugger is active!
2019-09-12 13:43:59,778 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:43:59,953 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:43:59] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 13:43:59,984 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 13:43:59,999 :: DEBUG :: import_id = 388
2019-09-12 13:43:59,999 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 13:43:59,999 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 13:43:59,999 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 13:44:00,017 :: WARNING :: ncores used by Dask = 4
2019-09-12 13:45:41,322 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:45:42,114 :: INFO ::  * Restarting with stat
2019-09-12 13:45:46,073 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 13:45:46,073 :: INFO ::  * Restarting with stat
2019-09-12 13:45:48,239 :: WARNING ::  * Debugger is active!
2019-09-12 13:45:48,240 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:45:48,247 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:45:48] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 13:45:48,268 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 13:45:48,285 :: DEBUG :: import_id = 388
2019-09-12 13:45:48,287 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 13:45:48,287 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 13:45:48,288 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 13:45:48,310 :: WARNING :: ncores used by Dask = 4
2019-09-12 13:46:45,304 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:46:45,879 :: INFO ::  * Restarting with stat
2019-09-12 13:46:48,226 :: WARNING ::  * Debugger is active!
2019-09-12 13:46:48,226 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:47:30,753 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:47:31,046 :: INFO ::  * Restarting with stat
2019-09-12 13:47:33,173 :: WARNING ::  * Debugger is active!
2019-09-12 13:47:33,175 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:47:37,476 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 13:47:37,477 :: INFO ::  * Restarting with stat
2019-09-12 13:47:39,991 :: WARNING ::  * Debugger is active!
2019-09-12 13:47:39,992 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:47:39,997 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:47:39] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 13:47:40,028 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 13:47:40,051 :: DEBUG :: import_id = 388
2019-09-12 13:47:40,051 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 13:47:40,051 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 13:47:40,051 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 13:47:40,079 :: WARNING :: ncores used by Dask = 4
2019-09-12 13:49:33,673 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:49:34,382 :: INFO ::  * Restarting with stat
2019-09-12 13:49:39,766 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 13:49:39,767 :: INFO ::  * Restarting with stat
2019-09-12 13:49:41,717 :: WARNING ::  * Debugger is active!
2019-09-12 13:49:41,717 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:49:41,726 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:49:41] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 13:49:41,749 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 13:49:41,765 :: DEBUG :: import_id = 388
2019-09-12 13:49:41,765 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 13:49:41,765 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 13:49:41,765 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 13:49:41,793 :: WARNING :: ncores used by Dask = 4
2019-09-12 13:53:28,165 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 13:53:28,998 :: INFO ::  * Restarting with stat
2019-09-12 13:53:31,140 :: WARNING ::  * Debugger is active!
2019-09-12 13:53:31,141 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:53:31,817 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:53:31] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 13:53:31,855 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 13:53:31,877 :: DEBUG :: import_id = 388
2019-09-12 13:53:31,877 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 13:53:31,877 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 13:53:31,878 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 13:53:31,911 :: WARNING :: ncores used by Dask = 4
2019-09-12 13:54:24,238 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 13:54:24,238 :: INFO ::  * Restarting with stat
2019-09-12 13:54:26,408 :: WARNING ::  * Debugger is active!
2019-09-12 13:54:26,408 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 13:54:27,193 :: INFO :: 127.0.0.1 - - [12/Sep/2019 13:54:27] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 13:54:27,219 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 13:54:27,232 :: DEBUG :: import_id = 388
2019-09-12 13:54:27,233 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 13:54:27,233 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 13:54:27,233 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 13:54:27,255 :: WARNING :: ncores used by Dask = 4
2019-09-12 14:10:56,968 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 14:10:56,985 :: INFO ::  * Restarting with stat
2019-09-12 14:10:58,631 :: WARNING ::  * Debugger is active!
2019-09-12 14:10:58,652 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 14:11:13,653 :: INFO :: 127.0.0.1 - - [12/Sep/2019 14:11:13] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 14:11:13,700 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 14:11:13,716 :: DEBUG :: import_id = 388
2019-09-12 14:11:13,717 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 14:11:13,717 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 14:11:13,717 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 14:11:13,746 :: WARNING :: ncores used by Dask = 4
2019-09-12 14:11:24,816 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 14:11:25,456 :: INFO ::  * Restarting with stat
2019-09-12 14:11:31,076 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 14:11:31,076 :: INFO ::  * Restarting with stat
2019-09-12 14:11:33,697 :: WARNING ::  * Debugger is active!
2019-09-12 14:11:33,697 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 14:11:33,705 :: INFO :: 127.0.0.1 - - [12/Sep/2019 14:11:33] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 14:11:33,729 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 14:11:33,747 :: DEBUG :: import_id = 388
2019-09-12 14:11:33,747 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 14:11:33,748 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 14:11:33,748 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 14:11:33,768 :: WARNING :: ncores used by Dask = 4
2019-09-12 14:11:53,821 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 14:11:54,851 :: INFO ::  * Restarting with stat
2019-09-12 14:11:59,813 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 14:11:59,813 :: INFO ::  * Restarting with stat
2019-09-12 14:12:02,289 :: WARNING ::  * Debugger is active!
2019-09-12 14:12:02,290 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 14:12:02,301 :: INFO :: 127.0.0.1 - - [12/Sep/2019 14:12:02] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 14:12:02,321 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 14:12:02,340 :: DEBUG :: import_id = 388
2019-09-12 14:12:02,342 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 14:12:02,343 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 14:12:02,343 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 14:12:02,372 :: WARNING :: ncores used by Dask = 4
2019-09-12 14:25:11,654 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 14:25:12,034 :: INFO ::  * Restarting with stat
2019-09-12 14:25:13,650 :: WARNING ::  * Debugger is active!
2019-09-12 14:25:13,713 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 14:27:30,706 :: INFO :: 127.0.0.1 - - [12/Sep/2019 14:27:30] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 14:27:31,639 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 14:27:31,699 :: DEBUG :: import_id = 388
2019-09-12 14:27:31,699 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 14:27:31,699 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 14:27:31,700 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 14:27:31,837 :: WARNING :: ncores used by Dask = 4
2019-09-12 15:10:42,836 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 15:10:43,297 :: INFO ::  * Restarting with stat
2019-09-12 15:10:44,961 :: WARNING ::  * Debugger is active!
2019-09-12 15:10:44,994 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 15:10:53,931 :: INFO :: 127.0.0.1 - - [12/Sep/2019 15:10:53] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 15:10:54,138 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 15:10:54,199 :: DEBUG :: import_id = 388
2019-09-12 15:10:54,199 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 15:10:54,199 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 15:10:54,199 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 15:10:54,317 :: WARNING :: ncores used by Dask = 4
2019-09-12 15:51:09,307 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 15:51:09,491 :: INFO ::  * Restarting with stat
2019-09-12 15:51:11,048 :: WARNING ::  * Debugger is active!
2019-09-12 15:51:11,311 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 15:51:14,684 :: INFO :: 127.0.0.1 - - [12/Sep/2019 15:51:14] "OPTIONS /import/mapping/388 HTTP/1.1" 200 -
2019-09-12 15:51:16,686 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 15:51:17,562 :: DEBUG :: import_id = 388
2019-09-12 15:51:17,563 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_388
2019-09-12 15:51:17,564 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 15:51:17,564 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 15:51:18,022 :: WARNING :: ncores used by Dask = 4
2019-09-12 15:52:26,519 :: INFO :: 127.0.0.1 - - [12/Sep/2019 15:52:26] "GET /gn_commons/modules HTTP/1.1" 200 -
2019-09-12 15:52:26,989 :: INFO :: 127.0.0.1 - - [12/Sep/2019 15:52:26] "GET /gn_commons/modules HTTP/1.1" 200 -
2019-09-12 16:17:45,135 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 16:17:45,795 :: INFO ::  * Restarting with stat
2019-09-12 16:17:47,366 :: WARNING ::  * Debugger is active!
2019-09-12 16:17:47,760 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 16:22:59,279 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 16:22:59,672 :: INFO ::  * Restarting with stat
2019-09-12 16:23:01,455 :: WARNING ::  * Debugger is active!
2019-09-12 16:23:01,457 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 16:23:09,554 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:09] "GET /gn_commons/modules HTTP/1.1" 200 -
2019-09-12 16:23:09,629 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:09] "GET /gn_commons/modules HTTP/1.1" 200 -
2019-09-12 16:23:11,140 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:11] "GET /synthese/for_web?limit=100 HTTP/1.1" 200 -
2019-09-12 16:23:11,729 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:11] "GET /import/delete_step1 HTTP/1.1" 200 -
2019-09-12 16:23:11,910 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:11] "GET /synthese/general_stats HTTP/1.1" 200 -
2019-09-12 16:23:11,969 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:11] "GET /import HTTP/1.1" 200 -
2019-09-12 16:23:13,149 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:13] "GET /import/datasets HTTP/1.1" 200 -
2019-09-12 16:23:23,288 :: INFO :: *** START UPLOAD USER FILE
2019-09-12 16:23:23,289 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-12 16:23:23,461 :: INFO :: File size = 10.37869644165039 Mo
2019-09-12 16:23:23,522 :: DEBUG :: original file name = data_pf_observado_corrected.csv
2019-09-12 16:23:23,523 :: INFO :: Upload : file saved in directory - time : 0:00:00.233724
2019-09-12 16:23:23,523 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-12 16:23:23,523 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-12 16:23:26,386 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'my_date', 'heure', 'date_encodage', 'my_timestamp', 'type_observation', 'nombre', 'my_min', 'my_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'latit', 'longit', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'my_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-12 16:23:26,386 :: DEBUG :: row_count = 26516
2019-09-12 16:23:26,386 :: INFO :: User file validity checked - time : 0:00:02.863029
2019-09-12 16:23:26,386 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-12 16:23:26,503 :: DEBUG :: id_import = 389
2019-09-12 16:23:26,503 :: DEBUG :: id_role = 1
2019-09-12 16:23:26,547 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_corrected_389
2019-09-12 16:23:28,240 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-12 16:23:33,804 :: INFO :: CSV loaded to DB table - time : 0:00:05.563163
2019-09-12 16:23:33,804 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-12 16:23:33,804 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-12 16:23:34,251 :: INFO :: CSV loaded to DB table - time : 0:00:00.446393
2019-09-12 16:23:34,251 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-12 16:23:34,629 :: INFO :: *** END UPLOAD USER FILE
2019-09-12 16:23:34,637 :: INFO :: Total time to post user file and fill metadata - time : 0:00:11.348214
2019-09-12 16:23:34,646 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:34] "POST /import/uploads HTTP/1.1" 200 -
2019-09-12 16:23:35,001 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:35] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-12 16:23:48,464 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:48] "OPTIONS /import/mapping/389 HTTP/1.1" 200 -
2019-09-12 16:23:48,495 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 16:23:48,519 :: DEBUG :: import_id = 389
2019-09-12 16:23:48,519 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_389
2019-09-12 16:23:48,519 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 16:23:48,519 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 16:23:48,712 :: WARNING :: ncores used by Dask = 4
2019-09-12 16:23:49,143 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-12 16:23:49,143 :: ERROR :: Index(...) must be called with a collection of some kind, 'cd_nom' was passed
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 550, in postMapping
    df = extract(table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, column_names, index_col, import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/extract/extract.py", line 42, in extract
    empty_taxref_df = pd.DataFrame(columns='cd_nom', dtype='object')
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/core/frame.py", line 392, in __init__
    mgr = init_dict(data, index, columns, dtype=dtype)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/core/internals/construction.py", line 177, in init_dict
    arrays = Series(data, index=columns, dtype=object)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/core/series.py", line 169, in __init__
    index = ensure_index(index)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 5378, in ensure_index
    return Index(index_like)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 468, in __new__
    cls._scalar_data_error(data)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 3799, in _scalar_data_error
    repr(data)))
TypeError: Index(...) must be called with a collection of some kind, 'cd_nom' was passed
2019-09-12 16:23:49,455 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:23:49] "POST /import/mapping/389 HTTP/1.1" 500 -
2019-09-12 16:24:06,794 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-12 16:24:07,242 :: INFO ::  * Restarting with stat
2019-09-12 16:24:11,513 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 16:24:11,514 :: INFO ::  * Restarting with stat
2019-09-12 16:24:13,342 :: WARNING ::  * Debugger is active!
2019-09-12 16:24:13,343 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 16:24:13,362 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:24:13] "OPTIONS /import/mapping/389 HTTP/1.1" 200 -
2019-09-12 16:24:13,390 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 16:24:13,405 :: DEBUG :: import_id = 389
2019-09-12 16:24:13,405 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_389
2019-09-12 16:24:13,406 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 16:24:13,406 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 16:24:13,421 :: WARNING :: ncores used by Dask = 4
2019-09-12 16:25:26,968 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 16:25:26,968 :: INFO ::  * Restarting with stat
2019-09-12 16:25:28,600 :: WARNING ::  * Debugger is active!
2019-09-12 16:25:28,600 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 16:25:43,440 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:25:43] "OPTIONS /import/mapping/389 HTTP/1.1" 200 -
2019-09-12 16:25:43,465 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 16:25:43,480 :: DEBUG :: import_id = 389
2019-09-12 16:25:43,481 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_389
2019-09-12 16:25:43,481 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 16:25:43,481 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 16:25:43,498 :: WARNING :: ncores used by Dask = 4
2019-09-12 16:26:28,747 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 16:26:28,748 :: INFO ::  * Restarting with stat
2019-09-12 16:26:30,562 :: WARNING ::  * Debugger is active!
2019-09-12 16:26:30,563 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 16:26:30,569 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:26:30] "OPTIONS /import/mapping/389 HTTP/1.1" 200 -
2019-09-12 16:26:30,591 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 16:26:30,604 :: DEBUG :: import_id = 389
2019-09-12 16:26:30,604 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_389
2019-09-12 16:26:30,605 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 16:26:30,605 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 16:26:30,625 :: WARNING :: ncores used by Dask = 4
2019-09-12 16:33:40,513 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-12 16:33:40,598 :: INFO ::  * Restarting with stat
2019-09-12 16:33:42,192 :: WARNING ::  * Debugger is active!
2019-09-12 16:33:42,231 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-12 16:33:46,218 :: INFO :: 127.0.0.1 - - [12/Sep/2019 16:33:46] "OPTIONS /import/mapping/389 HTTP/1.1" 200 -
2019-09-12 16:33:46,290 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-12 16:33:46,475 :: DEBUG :: import_id = 389
2019-09-12 16:33:46,475 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_389
2019-09-12 16:33:46,476 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp'}
2019-09-12 16:33:46,476 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-12 16:33:46,494 :: WARNING :: ncores used by Dask = 4
2019-09-13 10:45:26,129 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-13 10:45:26,130 :: INFO ::  * Restarting with stat
2019-09-13 10:45:27,693 :: WARNING ::  * Debugger is active!
2019-09-13 10:45:27,752 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 10:50:27,392 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:27] "GET /gn_commons/modules HTTP/1.1" 403 -
2019-09-13 10:50:28,980 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:28] "OPTIONS /auth/login HTTP/1.1" 200 -
2019-09-13 10:50:30,163 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:30] "POST /auth/login HTTP/1.1" 200 -
2019-09-13 10:50:30,365 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:30] "GET /gn_commons/modules/GEONATURE HTTP/1.1" 404 -
2019-09-13 10:50:31,767 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:31] "GET /gn_commons/modules HTTP/1.1" 200 -
2019-09-13 10:50:32,333 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:32] "GET /synthese/general_stats HTTP/1.1" 200 -
2019-09-13 10:50:32,598 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:32] "GET /synthese/for_web?limit=100 HTTP/1.1" 200 -
2019-09-13 10:50:51,928 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:51] "GET /import/delete_step1 HTTP/1.1" 200 -
2019-09-13 10:50:51,993 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:51] "GET /import HTTP/1.1" 200 -
2019-09-13 10:50:54,599 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:50:54] "GET /import/datasets HTTP/1.1" 200 -
2019-09-13 10:51:38,342 :: INFO :: *** START UPLOAD USER FILE
2019-09-13 10:51:38,343 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-13 10:51:38,535 :: INFO :: File size = 10.37869644165039 Mo
2019-09-13 10:51:38,589 :: DEBUG :: original file name = data_pf_observado_corrected.csv
2019-09-13 10:51:38,589 :: INFO :: Upload : file saved in directory - time : 0:00:00.245296
2019-09-13 10:51:38,589 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-13 10:51:38,589 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-13 10:51:42,645 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'my_date', 'heure', 'date_encodage', 'my_timestamp', 'type_observation', 'nombre', 'my_min', 'my_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'latit', 'longit', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'my_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-13 10:51:42,646 :: DEBUG :: row_count = 26516
2019-09-13 10:51:42,647 :: INFO :: User file validity checked - time : 0:00:04.057656
2019-09-13 10:51:42,648 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-13 10:51:42,944 :: DEBUG :: id_import = 390
2019-09-13 10:51:42,944 :: DEBUG :: id_role = 1
2019-09-13 10:51:42,976 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_corrected_390
2019-09-13 10:51:44,335 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-13 10:51:46,307 :: INFO :: CSV loaded to DB table - time : 0:00:01.970940
2019-09-13 10:51:46,307 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-13 10:51:46,308 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-13 10:51:46,728 :: INFO :: CSV loaded to DB table - time : 0:00:00.420223
2019-09-13 10:51:46,728 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-13 10:51:47,071 :: INFO :: *** END UPLOAD USER FILE
2019-09-13 10:51:47,081 :: INFO :: Total time to post user file and fill metadata - time : 0:00:08.739038
2019-09-13 10:51:47,090 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:51:47] "POST /import/uploads HTTP/1.1" 200 -
2019-09-13 10:51:47,369 :: INFO :: 127.0.0.1 - - [13/Sep/2019 10:51:47] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-13 11:00:46,491 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:00:46,938 :: INFO ::  * Restarting with stat
2019-09-13 11:00:48,604 :: WARNING ::  * Debugger is active!
2019-09-13 11:00:48,605 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:01:27,055 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:01:27,353 :: INFO ::  * Restarting with stat
2019-09-13 11:01:28,999 :: WARNING ::  * Debugger is active!
2019-09-13 11:01:29,000 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:02:09,798 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:02:09] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:02:09,832 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:02:09,846 :: DEBUG :: import_id = 390
2019-09-13 11:02:09,846 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:02:09,847 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:02:09,847 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:02:09,891 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:02:37,794 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/extract/extract.py', reloading
2019-09-13 11:02:38,692 :: INFO ::  * Restarting with stat
2019-09-13 11:02:56,366 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-13 11:02:56,367 :: INFO ::  * Restarting with stat
2019-09-13 11:02:58,206 :: WARNING ::  * Debugger is active!
2019-09-13 11:02:58,206 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:03:00,153 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:03:00] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:03:00,179 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:03:00,195 :: DEBUG :: import_id = 390
2019-09-13 11:03:00,196 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:03:00,196 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:03:00,196 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:03:00,215 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:03:00,281 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.084857
2019-09-13 11:03:00,281 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:03:00,282 :: INFO :: * START DATA CLEANING
2019-09-13 11:03:00,381 :: INFO :: checking missing values : 
2019-09-13 11:03:00,475 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:03:00,641 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:03:00,794 :: INFO :: - checking missing values for date_min column
2019-09-13 11:03:00,935 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.554659
2019-09-13 11:03:00,936 :: INFO :: checking types : 
2019-09-13 11:03:00,936 :: INFO :: - checking date type for date_min column
2019-09-13 11:03:01,087 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:03:01,207 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.271806
2019-09-13 11:03:01,208 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:03:08,941 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:07.733535
2019-09-13 11:03:08,942 :: INFO :: checking date validity :
2019-09-13 11:03:08,942 :: INFO :: - checking if date_max column is missing
2019-09-13 11:03:08,942 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000270
2019-09-13 11:03:08,942 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:03:09,023 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.080841
2019-09-13 11:03:09,023 :: INFO :: checking count_min and count_max : 
2019-09-13 11:03:09,023 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000200
2019-09-13 11:03:09,023 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-13 11:03:09,024 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000008
2019-09-13 11:03:09,024 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:03:09,024 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:03:09,024 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:03:09,024 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:03:09,024 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:03:09,024 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:03:09,052 :: INFO :: * END DATA CLEANING
2019-09-13 11:03:09,053 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:03:09,053 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:03:15,463 :: INFO :: dask df converted in pandas df - time : 0:00:06.410509
2019-09-13 11:03:15,464 :: INFO :: loading dataframe into DB table:
2019-09-13 11:04:02,237 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-13 11:04:02,238 :: INFO ::  * Restarting with stat
2019-09-13 11:04:03,769 :: WARNING ::  * Debugger is active!
2019-09-13 11:04:03,769 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:04:24,594 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:04:24,956 :: INFO ::  * Restarting with stat
2019-09-13 11:04:26,908 :: WARNING ::  * Debugger is active!
2019-09-13 11:04:26,908 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:04:32,111 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:04:32] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:04:32,140 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:04:32,153 :: DEBUG :: import_id = 390
2019-09-13 11:04:32,153 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:04:32,154 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:04:32,154 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:04:32,169 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:04:32,223 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.069101
2019-09-13 11:04:32,224 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:04:32,224 :: INFO :: * START DATA CLEANING
2019-09-13 11:04:32,270 :: INFO :: checking missing values : 
2019-09-13 11:04:32,354 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:04:32,493 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:04:32,612 :: INFO :: - checking missing values for date_min column
2019-09-13 11:04:32,762 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.492302
2019-09-13 11:04:32,762 :: INFO :: checking types : 
2019-09-13 11:04:32,763 :: INFO :: - checking date type for date_min column
2019-09-13 11:04:32,999 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:04:33,205 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.442735
2019-09-13 11:04:33,205 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:04:37,993 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.787470
2019-09-13 11:04:37,993 :: INFO :: checking date validity :
2019-09-13 11:04:37,993 :: INFO :: - checking if date_max column is missing
2019-09-13 11:04:37,993 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000289
2019-09-13 11:04:37,994 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:04:38,049 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.055219
2019-09-13 11:04:38,049 :: INFO :: checking count_min and count_max : 
2019-09-13 11:04:38,050 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000679
2019-09-13 11:04:38,050 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-13 11:04:38,051 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000016
2019-09-13 11:04:38,051 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:04:38,051 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:04:38,051 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:04:38,051 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:04:38,051 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:04:38,051 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:04:38,082 :: INFO :: * END DATA CLEANING
2019-09-13 11:04:38,082 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:04:38,082 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:04:44,645 :: INFO :: dask df converted in pandas df - time : 0:00:06.562137
2019-09-13 11:04:44,645 :: INFO :: loading dataframe into DB table:
2019-09-13 11:06:58,594 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:06:58,943 :: INFO ::  * Restarting with stat
2019-09-13 11:07:00,628 :: WARNING ::  * Debugger is active!
2019-09-13 11:07:00,629 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:07:01,678 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:07:01,982 :: INFO ::  * Restarting with stat
2019-09-13 11:07:03,958 :: WARNING ::  * Debugger is active!
2019-09-13 11:07:03,958 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:07:08,199 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:07:08] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:07:08,228 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:07:08,243 :: DEBUG :: import_id = 390
2019-09-13 11:07:08,243 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:07:08,244 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:07:08,244 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:07:08,260 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:07:08,330 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.086315
2019-09-13 11:07:08,332 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:07:08,332 :: INFO :: * START DATA CLEANING
2019-09-13 11:07:08,408 :: INFO :: checking missing values : 
2019-09-13 11:07:08,529 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:07:08,702 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:07:08,842 :: INFO :: - checking missing values for date_min column
2019-09-13 11:07:08,981 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.572604
2019-09-13 11:07:08,981 :: INFO :: checking types : 
2019-09-13 11:07:08,981 :: INFO :: - checking date type for date_min column
2019-09-13 11:07:09,132 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:07:09,252 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.271089
2019-09-13 11:07:09,253 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:07:14,107 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.854871
2019-09-13 11:07:14,108 :: INFO :: checking date validity :
2019-09-13 11:07:14,108 :: INFO :: - checking if date_max column is missing
2019-09-13 11:07:14,108 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000291
2019-09-13 11:07:14,108 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:07:14,170 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.062016
2019-09-13 11:07:14,171 :: INFO :: checking count_min and count_max : 
2019-09-13 11:07:14,171 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000270
2019-09-13 11:07:14,171 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-13 11:07:14,171 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000011
2019-09-13 11:07:14,171 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:07:14,172 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:07:14,172 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:07:14,172 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:07:14,172 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:07:14,172 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:07:14,207 :: INFO :: * END DATA CLEANING
2019-09-13 11:07:14,208 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:07:14,208 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:07:20,690 :: INFO :: dask df converted in pandas df - time : 0:00:06.481665
2019-09-13 11:07:20,690 :: INFO :: loading dataframe into DB table:
2019-09-13 11:07:48,128 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:07:48,856 :: INFO ::  * Restarting with stat
2019-09-13 11:07:50,747 :: WARNING ::  * Debugger is active!
2019-09-13 11:07:50,747 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:08:36,320 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:08:36,589 :: INFO ::  * Restarting with stat
2019-09-13 11:08:38,256 :: WARNING ::  * Debugger is active!
2019-09-13 11:08:38,256 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:08:40,363 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:08:40,647 :: INFO ::  * Restarting with stat
2019-09-13 11:08:42,715 :: WARNING ::  * Debugger is active!
2019-09-13 11:08:42,715 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:08:52,597 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-13 11:08:52,597 :: INFO ::  * Restarting with stat
2019-09-13 11:08:54,331 :: WARNING ::  * Debugger is active!
2019-09-13 11:08:54,331 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:08:56,029 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:08:56] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:08:56,059 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:08:56,072 :: DEBUG :: import_id = 390
2019-09-13 11:08:56,073 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:08:56,073 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:08:56,073 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:08:56,086 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:08:56,142 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.068968
2019-09-13 11:08:56,142 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:08:56,142 :: INFO :: * START DATA CLEANING
2019-09-13 11:08:56,200 :: INFO :: checking missing values : 
2019-09-13 11:08:56,286 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:08:56,416 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:08:56,536 :: INFO :: - checking missing values for date_min column
2019-09-13 11:08:56,678 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.478448
2019-09-13 11:08:56,678 :: INFO :: checking types : 
2019-09-13 11:08:56,678 :: INFO :: - checking date type for date_min column
2019-09-13 11:08:56,825 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:08:56,959 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.281026
2019-09-13 11:08:56,960 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:09:01,152 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.192470
2019-09-13 11:09:01,152 :: INFO :: checking date validity :
2019-09-13 11:09:01,153 :: INFO :: - checking if date_max column is missing
2019-09-13 11:09:01,153 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000332
2019-09-13 11:09:01,153 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:09:01,205 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.051636
2019-09-13 11:09:01,205 :: INFO :: checking count_min and count_max : 
2019-09-13 11:09:01,205 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000283
2019-09-13 11:09:01,206 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000015
2019-09-13 11:09:01,206 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000013
2019-09-13 11:09:01,206 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:09:01,206 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:09:01,207 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:09:01,207 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:09:01,207 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:09:01,207 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:09:01,233 :: INFO :: * END DATA CLEANING
2019-09-13 11:09:01,233 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:09:01,233 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:09:07,647 :: INFO :: dask df converted in pandas df - time : 0:00:06.413142
2019-09-13 11:09:07,648 :: INFO :: loading dataframe into DB table:
2019-09-13 11:10:50,519 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:10:51,149 :: INFO ::  * Restarting with stat
2019-09-13 11:11:01,823 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-13 11:11:01,825 :: INFO ::  * Restarting with stat
2019-09-13 11:11:04,048 :: WARNING ::  * Debugger is active!
2019-09-13 11:11:04,048 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:11:04,341 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:11:04] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:11:04,374 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:11:04,386 :: DEBUG :: import_id = 390
2019-09-13 11:11:04,386 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:11:04,387 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:11:04,387 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:11:04,403 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:11:04,460 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.073063
2019-09-13 11:11:04,460 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:11:04,460 :: INFO :: * START DATA CLEANING
2019-09-13 11:11:04,509 :: INFO :: checking missing values : 
2019-09-13 11:11:04,600 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:11:04,733 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:11:04,855 :: INFO :: - checking missing values for date_min column
2019-09-13 11:11:04,980 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.471189
2019-09-13 11:11:04,981 :: INFO :: checking types : 
2019-09-13 11:11:04,981 :: INFO :: - checking date type for date_min column
2019-09-13 11:11:05,094 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:11:05,207 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.226580
2019-09-13 11:11:05,208 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:11:10,070 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.861893
2019-09-13 11:11:10,070 :: INFO :: checking date validity :
2019-09-13 11:11:10,070 :: INFO :: - checking if date_max column is missing
2019-09-13 11:11:10,070 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000512
2019-09-13 11:11:10,071 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:11:10,128 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.057613
2019-09-13 11:11:10,129 :: INFO :: checking count_min and count_max : 
2019-09-13 11:11:10,130 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000471
2019-09-13 11:11:10,130 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000017
2019-09-13 11:11:10,130 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000024
2019-09-13 11:11:10,131 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:11:10,131 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:11:10,131 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:11:10,132 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:11:10,132 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:11:10,132 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:11:10,158 :: INFO :: * END DATA CLEANING
2019-09-13 11:11:10,159 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:11:10,159 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:11:16,520 :: INFO :: dask df converted in pandas df - time : 0:00:06.360151
2019-09-13 11:11:16,521 :: INFO :: loading dataframe into DB table:
2019-09-13 11:19:04,810 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:19:05,356 :: INFO ::  * Restarting with stat
2019-09-13 11:19:07,477 :: WARNING ::  * Debugger is active!
2019-09-13 11:19:07,477 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:19:12,909 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:19:12] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:19:12,938 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:19:12,952 :: DEBUG :: import_id = 390
2019-09-13 11:19:12,953 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:19:12,953 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:19:12,953 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:19:12,972 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:19:13,036 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.083437
2019-09-13 11:19:13,037 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:19:13,037 :: INFO :: * START DATA CLEANING
2019-09-13 11:19:13,107 :: INFO :: checking missing values : 
2019-09-13 11:19:13,204 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:19:13,352 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:19:13,493 :: INFO :: - checking missing values for date_min column
2019-09-13 11:19:13,672 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.565429
2019-09-13 11:19:13,672 :: INFO :: checking types : 
2019-09-13 11:19:13,672 :: INFO :: - checking date type for date_min column
2019-09-13 11:19:13,885 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:19:14,025 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.352637
2019-09-13 11:19:14,026 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:19:18,737 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.711092
2019-09-13 11:19:18,737 :: INFO :: checking date validity :
2019-09-13 11:19:18,737 :: INFO :: - checking if date_max column is missing
2019-09-13 11:19:18,737 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000261
2019-09-13 11:19:18,737 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:19:18,811 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.073892
2019-09-13 11:19:18,812 :: INFO :: checking count_min and count_max : 
2019-09-13 11:19:18,813 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000429
2019-09-13 11:19:18,813 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000014
2019-09-13 11:19:18,813 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000014
2019-09-13 11:19:18,813 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:19:18,814 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:19:18,814 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:19:18,814 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:19:18,814 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:19:18,815 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:19:18,846 :: INFO :: * END DATA CLEANING
2019-09-13 11:19:18,847 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:19:18,847 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:19:25,820 :: INFO :: dask df converted in pandas df - time : 0:00:06.972121
2019-09-13 11:19:25,820 :: INFO :: loading dataframe into DB table:
2019-09-13 11:21:01,488 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-13 11:21:01,489 :: INFO ::  * Restarting with stat
2019-09-13 11:21:03,248 :: WARNING ::  * Debugger is active!
2019-09-13 11:21:03,249 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:21:06,464 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:21:06] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:21:06,487 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:21:06,503 :: DEBUG :: import_id = 390
2019-09-13 11:21:06,503 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:21:06,503 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:21:06,503 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:21:06,519 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:21:06,583 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.079749
2019-09-13 11:21:06,583 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:21:06,583 :: INFO :: * START DATA CLEANING
2019-09-13 11:21:06,634 :: INFO :: checking missing values : 
2019-09-13 11:21:06,726 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:21:06,852 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:21:06,977 :: INFO :: - checking missing values for date_min column
2019-09-13 11:21:07,120 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.485322
2019-09-13 11:21:07,120 :: INFO :: checking types : 
2019-09-13 11:21:07,120 :: INFO :: - checking date type for date_min column
2019-09-13 11:21:07,262 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:21:07,386 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.266474
2019-09-13 11:21:07,386 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:21:11,700 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.313612
2019-09-13 11:21:11,700 :: INFO :: checking date validity :
2019-09-13 11:21:11,701 :: INFO :: - checking if date_max column is missing
2019-09-13 11:21:11,701 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000341
2019-09-13 11:21:11,701 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:21:11,755 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.053443
2019-09-13 11:21:11,755 :: INFO :: checking count_min and count_max : 
2019-09-13 11:21:11,756 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000253
2019-09-13 11:21:11,756 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000012
2019-09-13 11:21:11,756 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000013
2019-09-13 11:21:11,756 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:21:11,757 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:21:11,757 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:21:11,757 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:21:11,757 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:21:11,758 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:21:11,786 :: INFO :: * END DATA CLEANING
2019-09-13 11:21:11,786 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:21:11,786 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:21:19,126 :: INFO :: dask df converted in pandas df - time : 0:00:07.339757
2019-09-13 11:21:19,128 :: INFO :: loading dataframe into DB table:
2019-09-13 11:21:37,068 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:21:37,720 :: INFO ::  * Restarting with stat
2019-09-13 11:21:40,001 :: WARNING ::  * Debugger is active!
2019-09-13 11:21:40,002 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:21:45,038 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-13 11:21:45,038 :: INFO ::  * Restarting with stat
2019-09-13 11:21:46,585 :: WARNING ::  * Debugger is active!
2019-09-13 11:21:46,585 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:21:48,660 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:21:48] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:21:48,683 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:21:48,697 :: DEBUG :: import_id = 390
2019-09-13 11:21:48,697 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:21:48,697 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:21:48,697 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:21:48,713 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:21:48,765 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-13 11:21:48,766 :: ERROR :: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 550, in postMapping
    df = extract(table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, column_names, index_col, import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/extract/extract.py", line 31, in extract
    df = dd.read_sql_table(table=table_name, index_col=index_dask, meta=empty_df, npartitions=ncores, uri=str(DB.engine.url), schema=schema_name, bytes_per_chunk=100000)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/io/sql.py", line 183, in read_sql_table
    "index column type must be numeric or datetime.".format(dtype)
TypeError: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
2019-09-13 11:21:48,814 :: ERROR :: Table gn_imports.i_data_pf_observado_corrected_390 vide  cause d'une erreur de copie, refaire l'upload et le mapping
2019-09-13 11:21:48,820 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:21:48] "POST /import/mapping/390 HTTP/1.1" 500 -
2019-09-13 11:21:52,285 :: INFO :: *** START UPLOAD USER FILE
2019-09-13 11:21:52,286 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-13 11:21:52,416 :: INFO :: File size = 10.37869644165039 Mo
2019-09-13 11:21:52,433 :: DEBUG :: original file name = data_pf_observado_corrected.csv
2019-09-13 11:21:52,433 :: INFO :: Upload : file saved in directory - time : 0:00:00.147344
2019-09-13 11:21:52,434 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-13 11:21:52,434 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-13 11:21:55,507 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'my_date', 'heure', 'date_encodage', 'my_timestamp', 'type_observation', 'nombre', 'my_min', 'my_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'latit', 'longit', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'my_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-13 11:21:55,507 :: DEBUG :: row_count = 26516
2019-09-13 11:21:55,508 :: INFO :: User file validity checked - time : 0:00:03.073814
2019-09-13 11:21:55,508 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-13 11:21:55,518 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_corrected_390
2019-09-13 11:21:55,924 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-13 11:21:56,430 :: INFO :: CSV loaded to DB table - time : 0:00:00.505249
2019-09-13 11:21:56,431 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-13 11:21:56,431 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-13 11:21:57,078 :: INFO :: CSV loaded to DB table - time : 0:00:00.646976
2019-09-13 11:21:57,078 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-13 11:21:57,202 :: INFO :: *** END UPLOAD USER FILE
2019-09-13 11:21:57,207 :: INFO :: Total time to post user file and fill metadata - time : 0:00:04.921544
2019-09-13 11:21:57,212 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:21:57] "POST /import/uploads HTTP/1.1" 200 -
2019-09-13 11:21:57,259 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:21:57] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-13 11:21:59,717 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:21:59] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:21:59,744 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:21:59,755 :: DEBUG :: import_id = 390
2019-09-13 11:21:59,755 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:21:59,755 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:21:59,755 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:21:59,776 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:21:59,843 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.087696
2019-09-13 11:21:59,844 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:21:59,844 :: INFO :: * START DATA CLEANING
2019-09-13 11:21:59,921 :: INFO :: checking missing values : 
2019-09-13 11:22:00,046 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:22:00,205 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:22:00,343 :: INFO :: - checking missing values for date_min column
2019-09-13 11:22:00,485 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.564454
2019-09-13 11:22:00,486 :: INFO :: checking types : 
2019-09-13 11:22:00,486 :: INFO :: - checking date type for date_min column
2019-09-13 11:22:00,625 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:22:00,749 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.263692
2019-09-13 11:22:00,750 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:22:04,810 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.060373
2019-09-13 11:22:04,811 :: INFO :: checking date validity :
2019-09-13 11:22:04,811 :: INFO :: - checking if date_max column is missing
2019-09-13 11:22:04,811 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000562
2019-09-13 11:22:04,811 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:22:04,870 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.058281
2019-09-13 11:22:04,871 :: INFO :: checking count_min and count_max : 
2019-09-13 11:22:04,871 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000618
2019-09-13 11:22:04,872 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000022
2019-09-13 11:22:04,872 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000025
2019-09-13 11:22:04,872 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:22:04,873 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:22:04,873 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:22:04,875 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:22:04,875 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:22:04,875 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:22:04,907 :: INFO :: * END DATA CLEANING
2019-09-13 11:22:04,907 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:22:04,907 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:22:11,242 :: INFO :: dask df converted in pandas df - time : 0:00:06.334054
2019-09-13 11:22:11,242 :: INFO :: loading dataframe into DB table:
2019-09-13 11:22:13,000 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-13 11:22:13,001 :: ERROR :: This transaction is inactive
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 574, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 80, in load
    load_df_to_sql(df2, table_name, full_table_name, engine, schema_name, ';')
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 46, in load_df_to_sql
    trans.commit()
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1631, in commit
    raise exc.InvalidRequestError("This transaction is inactive")
sqlalchemy.exc.InvalidRequestError: This transaction is inactive
2019-09-13 11:22:13,130 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:22:13] "POST /import/mapping/390 HTTP/1.1" 500 -
2019-09-13 11:23:06,149 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:23:06,487 :: INFO ::  * Restarting with stat
2019-09-13 11:23:08,142 :: WARNING ::  * Debugger is active!
2019-09-13 11:23:08,143 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:23:30,892 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:23:30] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:23:30,925 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:23:30,940 :: DEBUG :: import_id = 390
2019-09-13 11:23:30,940 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:23:30,940 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:23:30,941 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:23:30,957 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:23:31,032 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.090680
2019-09-13 11:23:31,032 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:23:31,032 :: INFO :: * START DATA CLEANING
2019-09-13 11:23:31,094 :: INFO :: checking missing values : 
2019-09-13 11:23:31,192 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:23:31,348 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:23:31,508 :: INFO :: - checking missing values for date_min column
2019-09-13 11:23:31,662 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.568136
2019-09-13 11:23:31,663 :: INFO :: checking types : 
2019-09-13 11:23:31,663 :: INFO :: - checking date type for date_min column
2019-09-13 11:23:31,813 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:23:31,932 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.269408
2019-09-13 11:23:31,932 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:23:36,337 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.404549
2019-09-13 11:23:36,337 :: INFO :: checking date validity :
2019-09-13 11:23:36,337 :: INFO :: - checking if date_max column is missing
2019-09-13 11:23:36,338 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000584
2019-09-13 11:23:36,338 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:23:36,395 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.056687
2019-09-13 11:23:36,395 :: INFO :: checking count_min and count_max : 
2019-09-13 11:23:36,395 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000199
2019-09-13 11:23:36,396 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-13 11:23:36,396 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-13 11:23:36,396 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:23:36,396 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:23:36,396 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:23:36,396 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:23:36,396 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:23:36,396 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:23:36,431 :: INFO :: * END DATA CLEANING
2019-09-13 11:23:36,431 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:23:36,431 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:23:43,225 :: INFO :: dask df converted in pandas df - time : 0:00:06.793312
2019-09-13 11:23:43,226 :: INFO :: loading dataframe into DB table:
2019-09-13 11:24:21,162 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:24:21,834 :: INFO ::  * Restarting with stat
2019-09-13 11:24:24,050 :: WARNING ::  * Debugger is active!
2019-09-13 11:24:24,051 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:24:27,780 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:24:27] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:24:27,812 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:24:27,830 :: DEBUG :: import_id = 390
2019-09-13 11:24:27,830 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:24:27,830 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:24:27,830 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:24:27,852 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:24:27,924 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.093671
2019-09-13 11:24:27,925 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:24:27,925 :: INFO :: * START DATA CLEANING
2019-09-13 11:24:27,982 :: INFO :: checking missing values : 
2019-09-13 11:24:28,080 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:24:28,227 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:24:28,374 :: INFO :: - checking missing values for date_min column
2019-09-13 11:24:28,513 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.530844
2019-09-13 11:24:28,514 :: INFO :: checking types : 
2019-09-13 11:24:28,514 :: INFO :: - checking date type for date_min column
2019-09-13 11:24:28,666 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:24:28,796 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.282177
2019-09-13 11:24:28,796 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:24:32,663 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.866582
2019-09-13 11:24:32,663 :: INFO :: checking date validity :
2019-09-13 11:24:32,663 :: INFO :: - checking if date_max column is missing
2019-09-13 11:24:32,663 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000341
2019-09-13 11:24:32,663 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:24:32,722 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.058311
2019-09-13 11:24:32,722 :: INFO :: checking count_min and count_max : 
2019-09-13 11:24:32,722 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000174
2019-09-13 11:24:32,722 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-13 11:24:32,723 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-13 11:24:32,723 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:24:32,723 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:24:32,723 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:24:32,723 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:24:32,723 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:24:32,723 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:24:32,747 :: INFO :: * END DATA CLEANING
2019-09-13 11:24:32,747 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:24:32,748 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:24:39,076 :: INFO :: dask df converted in pandas df - time : 0:00:06.328259
2019-09-13 11:24:39,076 :: INFO :: loading dataframe into DB table:
2019-09-13 11:24:40,579 :: INFO :: dask df loaded to db table - time : 0:00:01.502413
2019-09-13 11:24:40,777 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:08.029683
2019-09-13 11:24:40,810 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:24:40,866 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-13 11:24:40,937 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:24:40] "POST /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:24:57,819 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:24:58,338 :: INFO ::  * Restarting with stat
2019-09-13 11:25:00,316 :: WARNING ::  * Debugger is active!
2019-09-13 11:25:00,316 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:25:02,556 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:25:02] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:25:02,588 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:25:02,602 :: DEBUG :: import_id = 390
2019-09-13 11:25:02,603 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:25:02,603 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:25:02,603 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:25:02,620 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:25:02,683 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.079180
2019-09-13 11:25:02,683 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:25:02,684 :: INFO :: * START DATA CLEANING
2019-09-13 11:25:02,746 :: INFO :: checking missing values : 
2019-09-13 11:25:02,848 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:25:02,996 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:25:03,130 :: INFO :: - checking missing values for date_min column
2019-09-13 11:25:03,266 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.519670
2019-09-13 11:25:03,266 :: INFO :: checking types : 
2019-09-13 11:25:03,266 :: INFO :: - checking date type for date_min column
2019-09-13 11:25:03,398 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:25:03,515 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.249494
2019-09-13 11:25:03,516 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:25:07,450 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.934111
2019-09-13 11:25:07,450 :: INFO :: checking date validity :
2019-09-13 11:25:07,450 :: INFO :: - checking if date_max column is missing
2019-09-13 11:25:07,450 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000358
2019-09-13 11:25:07,451 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:25:07,506 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.055872
2019-09-13 11:25:07,507 :: INFO :: checking count_min and count_max : 
2019-09-13 11:25:07,507 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000148
2019-09-13 11:25:07,507 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-13 11:25:07,507 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000008
2019-09-13 11:25:07,507 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:25:07,508 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:25:07,508 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:25:07,508 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:25:07,508 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:25:07,508 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:25:07,538 :: INFO :: * END DATA CLEANING
2019-09-13 11:25:07,538 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:25:07,538 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:25:13,997 :: INFO :: dask df converted in pandas df - time : 0:00:06.458710
2019-09-13 11:25:13,998 :: INFO :: loading dataframe into DB table:
2019-09-13 11:25:14,916 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-13 11:25:14,917 :: ERROR :: name 'cd' is not defined
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 574, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 80, in load
    load_df_to_sql(df2, table_name, full_table_name, engine, schema_name, ';')
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 43, in load_df_to_sql
    cur.copy_expert(cd, fbuf)
NameError: name 'cd' is not defined
2019-09-13 11:25:14,923 :: ERROR :: Table gn_imports.i_data_pf_observado_corrected_390 vide  cause d'une erreur de copie, refaire l'upload et le mapping
2019-09-13 11:25:15,013 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:25:15] "POST /import/mapping/390 HTTP/1.1" 500 -
2019-09-13 11:29:29,267 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:29:29,690 :: INFO ::  * Restarting with stat
2019-09-13 11:29:31,524 :: WARNING ::  * Debugger is active!
2019-09-13 11:29:31,525 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:29:37,542 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:29:37] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:29:37,570 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:29:37,584 :: DEBUG :: import_id = 390
2019-09-13 11:29:37,585 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:29:37,585 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:29:37,585 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:29:37,601 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:29:37,648 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-13 11:29:37,649 :: ERROR :: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 550, in postMapping
    df = extract(table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, column_names, index_col, import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/extract/extract.py", line 31, in extract
    df = dd.read_sql_table(table=table_name, index_col=index_dask, meta=empty_df, npartitions=ncores, uri=str(DB.engine.url), schema=schema_name, bytes_per_chunk=100000)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/io/sql.py", line 183, in read_sql_table
    "index column type must be numeric or datetime.".format(dtype)
TypeError: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
2019-09-13 11:29:37,651 :: ERROR :: Table gn_imports.i_data_pf_observado_corrected_390 vide  cause d'une erreur de copie, refaire l'upload et le mapping
2019-09-13 11:29:37,653 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:29:37] "POST /import/mapping/390 HTTP/1.1" 500 -
2019-09-13 11:29:40,573 :: INFO :: *** START UPLOAD USER FILE
2019-09-13 11:29:40,573 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-13 11:29:40,688 :: INFO :: File size = 10.37869644165039 Mo
2019-09-13 11:29:40,701 :: DEBUG :: original file name = data_pf_observado_corrected.csv
2019-09-13 11:29:40,701 :: INFO :: Upload : file saved in directory - time : 0:00:00.128015
2019-09-13 11:29:40,702 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-13 11:29:40,702 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-13 11:29:43,393 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'my_date', 'heure', 'date_encodage', 'my_timestamp', 'type_observation', 'nombre', 'my_min', 'my_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'latit', 'longit', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'my_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-13 11:29:43,393 :: DEBUG :: row_count = 26516
2019-09-13 11:29:43,394 :: INFO :: User file validity checked - time : 0:00:02.692024
2019-09-13 11:29:43,394 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-13 11:29:43,485 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_corrected_390
2019-09-13 11:29:43,913 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-13 11:29:44,271 :: INFO :: CSV loaded to DB table - time : 0:00:00.357302
2019-09-13 11:29:44,271 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-13 11:29:44,271 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-13 11:29:45,774 :: INFO :: CSV loaded to DB table - time : 0:00:01.502587
2019-09-13 11:29:45,774 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-13 11:29:46,152 :: INFO :: *** END UPLOAD USER FILE
2019-09-13 11:29:46,159 :: INFO :: Total time to post user file and fill metadata - time : 0:00:05.585798
2019-09-13 11:29:46,169 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:29:46] "POST /import/uploads HTTP/1.1" 200 -
2019-09-13 11:29:46,215 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:29:46] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-13 11:29:48,495 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:29:48] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:29:48,531 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:29:48,541 :: DEBUG :: import_id = 390
2019-09-13 11:29:48,541 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:29:48,542 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:29:48,543 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:29:48,554 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:29:48,601 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.057901
2019-09-13 11:29:48,602 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:29:48,602 :: INFO :: * START DATA CLEANING
2019-09-13 11:29:48,662 :: INFO :: checking missing values : 
2019-09-13 11:29:48,749 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:29:48,878 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:29:49,001 :: INFO :: - checking missing values for date_min column
2019-09-13 11:29:49,120 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.458722
2019-09-13 11:29:49,121 :: INFO :: checking types : 
2019-09-13 11:29:49,121 :: INFO :: - checking date type for date_min column
2019-09-13 11:29:49,243 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:29:49,359 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.238132
2019-09-13 11:29:49,359 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:29:53,215 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.855744
2019-09-13 11:29:53,215 :: INFO :: checking date validity :
2019-09-13 11:29:53,215 :: INFO :: - checking if date_max column is missing
2019-09-13 11:29:53,215 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000316
2019-09-13 11:29:53,215 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:29:53,264 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.048423
2019-09-13 11:29:53,264 :: INFO :: checking count_min and count_max : 
2019-09-13 11:29:53,264 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000141
2019-09-13 11:29:53,264 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-13 11:29:53,264 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000005
2019-09-13 11:29:53,265 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:29:53,265 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:29:53,265 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:29:53,265 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:29:53,265 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:29:53,265 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:29:53,287 :: INFO :: * END DATA CLEANING
2019-09-13 11:29:53,287 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:29:53,288 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:29:58,450 :: INFO :: dask df converted in pandas df - time : 0:00:05.162230
2019-09-13 11:29:58,460 :: INFO :: loading dataframe into DB table:
2019-09-13 11:29:59,137 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-13 11:29:59,137 :: ERROR :: name 'engine' is not defined
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 574, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 82, in load
    load_df_to_sql(df2, table_name, full_table_name, conn, schema_name, ';')
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 33, in load_df_to_sql
    conn = engine.raw_connection()
NameError: name 'engine' is not defined
2019-09-13 11:29:59,205 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:29:59] "POST /import/mapping/390 HTTP/1.1" 500 -
2019-09-13 11:30:28,541 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:30:28,878 :: INFO ::  * Restarting with stat
2019-09-13 11:30:30,611 :: WARNING ::  * Debugger is active!
2019-09-13 11:30:30,612 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:30:35,765 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:30:35] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:30:35,788 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:30:35,804 :: DEBUG :: import_id = 390
2019-09-13 11:30:35,804 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:30:35,804 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:30:35,805 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:30:35,820 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:30:35,865 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.060662
2019-09-13 11:30:35,866 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:30:35,866 :: INFO :: * START DATA CLEANING
2019-09-13 11:30:35,900 :: INFO :: checking missing values : 
2019-09-13 11:30:35,951 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:30:36,035 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:30:36,129 :: INFO :: - checking missing values for date_min column
2019-09-13 11:30:36,234 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.333568
2019-09-13 11:30:36,234 :: INFO :: checking types : 
2019-09-13 11:30:36,234 :: INFO :: - checking date type for date_min column
2019-09-13 11:30:36,348 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:30:36,464 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.230028
2019-09-13 11:30:36,464 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:30:40,381 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.916653
2019-09-13 11:30:40,381 :: INFO :: checking date validity :
2019-09-13 11:30:40,381 :: INFO :: - checking if date_max column is missing
2019-09-13 11:30:40,382 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000265
2019-09-13 11:30:40,382 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:30:40,432 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.050148
2019-09-13 11:30:40,432 :: INFO :: checking count_min and count_max : 
2019-09-13 11:30:40,432 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000111
2019-09-13 11:30:40,432 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-13 11:30:40,432 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000004
2019-09-13 11:30:40,432 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:30:40,433 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:30:40,433 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:30:40,433 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:30:40,433 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:30:40,433 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:30:40,457 :: INFO :: * END DATA CLEANING
2019-09-13 11:30:40,457 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:30:40,457 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:30:47,082 :: INFO :: dask df converted in pandas df - time : 0:00:06.624113
2019-09-13 11:30:47,093 :: INFO :: loading dataframe into DB table:
2019-09-13 11:34:26,633 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 11:34:27,002 :: INFO ::  * Restarting with stat
2019-09-13 11:34:28,637 :: WARNING ::  * Debugger is active!
2019-09-13 11:34:28,637 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 11:35:23,014 :: INFO :: 127.0.0.1 - - [13/Sep/2019 11:35:23] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 11:35:23,043 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 11:35:23,058 :: DEBUG :: import_id = 390
2019-09-13 11:35:23,058 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 11:35:23,058 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 11:35:23,058 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:35:23,075 :: WARNING :: ncores used by Dask = 4
2019-09-13 11:35:23,136 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.077100
2019-09-13 11:35:23,136 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 11:35:23,136 :: INFO :: * START DATA CLEANING
2019-09-13 11:35:23,199 :: INFO :: checking missing values : 
2019-09-13 11:35:23,291 :: INFO :: - checking missing values for cd_nom column
2019-09-13 11:35:23,429 :: INFO :: - checking missing values for nom_cite column
2019-09-13 11:35:23,565 :: INFO :: - checking missing values for date_min column
2019-09-13 11:35:23,725 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.526206
2019-09-13 11:35:23,726 :: INFO :: checking types : 
2019-09-13 11:35:23,726 :: INFO :: - checking date type for date_min column
2019-09-13 11:35:23,877 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 11:35:23,999 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.273218
2019-09-13 11:35:23,999 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 11:35:28,578 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.578327
2019-09-13 11:35:28,578 :: INFO :: checking date validity :
2019-09-13 11:35:28,578 :: INFO :: - checking if date_max column is missing
2019-09-13 11:35:28,578 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000270
2019-09-13 11:35:28,578 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 11:35:28,633 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.055266
2019-09-13 11:35:28,634 :: INFO :: checking count_min and count_max : 
2019-09-13 11:35:28,634 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000351
2019-09-13 11:35:28,635 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000020
2019-09-13 11:35:28,635 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000029
2019-09-13 11:35:28,636 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 11:35:28,636 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 11:35:28,636 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 11:35:28,636 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 11:35:28,637 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 11:35:28,637 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 11:35:28,679 :: INFO :: * END DATA CLEANING
2019-09-13 11:35:28,679 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 11:35:28,680 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 11:35:35,103 :: INFO :: dask df converted in pandas df - time : 0:00:06.422409
2019-09-13 11:35:35,117 :: INFO :: loading dataframe into DB table:
2019-09-13 13:19:54,546 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-13 13:19:54,547 :: INFO ::  * Restarting with stat
2019-09-13 13:19:56,616 :: WARNING ::  * Debugger is active!
2019-09-13 13:19:56,617 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 15:00:21,511 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 15:00:21,805 :: INFO ::  * Restarting with stat
2019-09-13 15:00:24,839 :: WARNING ::  * Debugger is active!
2019-09-13 15:00:24,839 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 15:00:52,542 :: INFO :: 127.0.0.1 - - [13/Sep/2019 15:00:52] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 15:00:52,573 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 15:00:52,587 :: DEBUG :: import_id = 390
2019-09-13 15:00:52,587 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 15:00:52,587 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 15:00:52,587 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 15:00:52,604 :: WARNING :: ncores used by Dask = 4
2019-09-13 15:00:52,662 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.074667
2019-09-13 15:00:52,662 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 15:00:52,662 :: INFO :: * START DATA CLEANING
2019-09-13 15:00:52,725 :: INFO :: checking missing values : 
2019-09-13 15:00:52,803 :: INFO :: - checking missing values for cd_nom column
2019-09-13 15:00:52,927 :: INFO :: - checking missing values for nom_cite column
2019-09-13 15:00:53,068 :: INFO :: - checking missing values for date_min column
2019-09-13 15:00:53,215 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.490178
2019-09-13 15:00:53,216 :: INFO :: checking types : 
2019-09-13 15:00:53,216 :: INFO :: - checking date type for date_min column
2019-09-13 15:00:53,352 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 15:00:53,468 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.251759
2019-09-13 15:00:53,468 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 15:00:57,940 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.472256
2019-09-13 15:00:57,941 :: INFO :: checking date validity :
2019-09-13 15:00:57,941 :: INFO :: - checking if date_max column is missing
2019-09-13 15:00:57,942 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000548
2019-09-13 15:00:57,942 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 15:00:58,128 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.185837
2019-09-13 15:00:58,128 :: INFO :: checking count_min and count_max : 
2019-09-13 15:00:58,128 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000184
2019-09-13 15:00:58,128 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000008
2019-09-13 15:00:58,128 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000007
2019-09-13 15:00:58,129 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 15:00:58,129 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 15:00:58,129 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 15:00:58,129 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 15:00:58,129 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 15:00:58,129 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 15:00:58,185 :: INFO :: * END DATA CLEANING
2019-09-13 15:00:58,185 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 15:00:58,186 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 15:01:04,817 :: INFO :: dask df converted in pandas df - time : 0:00:06.631106
2019-09-13 15:01:04,828 :: INFO :: loading dataframe into DB table:
2019-09-13 15:01:04,908 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-13 15:01:04,908 :: ERROR :: local variable 'conn' referenced before assignment
Traceback (most recent call last):
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1112, in _execute_context
    conn = self.__connection
AttributeError: 'Connection' object has no attribute '_Connection__connection'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1114, in _execute_context
    conn = self._revalidate_connection()
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 429, in _revalidate_connection
    raise exc.ResourceClosedError("This Connection is closed")
sqlalchemy.exc.ResourceClosedError: This Connection is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 29, in load_df_to_sql
    create_empty_table(df, table_name, connection, schema_name)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 57, in create_empty_table
    emp_df[:0].to_sql(table_name, con, schema=schema_name, if_exists='replace', index=False)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/core/generic.py", line 2531, in to_sql
    dtype=dtype, method=method)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/io/sql.py", line 460, in to_sql
    chunksize=chunksize, dtype=dtype, method=method)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/io/sql.py", line 1173, in to_sql
    table.create()
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/io/sql.py", line 572, in create
    if self.exists():
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/io/sql.py", line 560, in exists
    return self.pd_sql.has_table(self.name, self.schema)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/pandas/io/sql.py", line 1201, in has_table
    schema or self.meta.schema,
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1534, in run_callable
    return callable_(self, *args, **kwargs)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/dialects/postgresql/base.py", line 2227, in has_table
    type_=sqltypes.Unicode)]
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 945, in execute
    return meth(self, multiparams, params)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/sql/elements.py", line 263, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1053, in _execute_clauseelement
    compiled_sql, distilled_params
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1121, in _execute_context
    None, None)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1402, in _handle_dbapi_exception
    exc_info
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 203, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 186, in reraise
    raise value.with_traceback(tb)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1114, in _execute_context
    conn = self._revalidate_connection()
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 429, in _revalidate_connection
    raise exc.ResourceClosedError("This Connection is closed")
sqlalchemy.exc.StatementError: (sqlalchemy.exc.ResourceClosedError) This Connection is closed [SQL: 'select relname from pg_class c join pg_namespace n on n.oid=c.relnamespace where n.nspname=%(schema)s and relname=%(name)s']

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 574, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 83, in load
    load_df_to_sql(df2, table_name, full_table_name, engine, conn, schema_name, ';')
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 47, in load_df_to_sql
    conn.rollback()
UnboundLocalError: local variable 'conn' referenced before assignment
2019-09-13 15:01:05,013 :: INFO :: 127.0.0.1 - - [13/Sep/2019 15:01:05] "POST /import/mapping/390 HTTP/1.1" 500 -
2019-09-13 15:01:33,876 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 15:01:34,352 :: INFO ::  * Restarting with stat
2019-09-13 15:01:36,139 :: WARNING ::  * Debugger is active!
2019-09-13 15:01:36,140 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 15:01:39,233 :: INFO :: 127.0.0.1 - - [13/Sep/2019 15:01:39] "OPTIONS /import/mapping/390 HTTP/1.1" 200 -
2019-09-13 15:01:39,263 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-13 15:01:39,279 :: DEBUG :: import_id = 390
2019-09-13 15:01:39,280 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_390
2019-09-13 15:01:39,280 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'date_encodage'}
2019-09-13 15:01:39,280 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 15:01:39,299 :: WARNING :: ncores used by Dask = 4
2019-09-13 15:01:39,371 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.090342
2019-09-13 15:01:39,372 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-13 15:01:39,372 :: INFO :: * START DATA CLEANING
2019-09-13 15:01:39,443 :: INFO :: checking missing values : 
2019-09-13 15:01:39,561 :: INFO :: - checking missing values for cd_nom column
2019-09-13 15:01:39,744 :: INFO :: - checking missing values for nom_cite column
2019-09-13 15:01:39,890 :: INFO :: - checking missing values for date_min column
2019-09-13 15:01:40,044 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.601632
2019-09-13 15:01:40,044 :: INFO :: checking types : 
2019-09-13 15:01:40,045 :: INFO :: - checking date type for date_min column
2019-09-13 15:01:40,185 :: INFO :: - checking varchar type for nom_cite column
2019-09-13 15:01:40,306 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.261158
2019-09-13 15:01:40,306 :: INFO :: checking cd_nom validity for species_id column
2019-09-13 15:01:44,343 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.037114
2019-09-13 15:01:44,343 :: INFO :: checking date validity :
2019-09-13 15:01:44,343 :: INFO :: - checking if date_max column is missing
2019-09-13 15:01:44,344 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000284
2019-09-13 15:01:44,344 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-13 15:01:44,403 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.058917
2019-09-13 15:01:44,403 :: INFO :: checking count_min and count_max : 
2019-09-13 15:01:44,403 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000203
2019-09-13 15:01:44,403 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000014
2019-09-13 15:01:44,404 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000010
2019-09-13 15:01:44,404 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-13 15:01:44,404 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-13 15:01:44,404 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_encodage
2019-09-13 15:01:44,404 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_encodage
2019-09-13 15:01:44,404 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-13 15:01:44,405 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-13 15:01:44,437 :: INFO :: * END DATA CLEANING
2019-09-13 15:01:44,438 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-13 15:01:44,438 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-13 15:01:50,775 :: INFO :: dask df converted in pandas df - time : 0:00:06.337046
2019-09-13 15:01:50,790 :: INFO :: loading dataframe into DB table:
2019-09-13 15:02:05,942 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-13 15:02:05,942 :: INFO ::  * Restarting with stat
2019-09-13 15:02:08,479 :: WARNING ::  * Debugger is active!
2019-09-13 15:02:08,479 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-13 17:42:16,261 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-13 17:42:16,534 :: INFO ::  * Restarting with stat
2019-09-13 17:42:20,716 :: WARNING ::  * Debugger is active!
2019-09-13 17:42:20,716 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:29:46,598 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:29:46] "GET /gn_commons/modules HTTP/1.1" 403 -
2019-09-16 11:29:48,462 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:29:48] "OPTIONS /auth/login HTTP/1.1" 200 -
2019-09-16 11:29:49,199 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:29:49] "POST /auth/login HTTP/1.1" 200 -
2019-09-16 11:29:49,320 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:29:49] "GET /gn_commons/modules HTTP/1.1" 200 -
2019-09-16 11:29:49,486 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:29:49] "GET /synthese/general_stats HTTP/1.1" 200 -
2019-09-16 11:29:49,994 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:29:49] "GET /synthese/for_web?limit=100 HTTP/1.1" 200 -
2019-09-16 11:29:51,884 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:29:51] "GET /import HTTP/1.1" 200 -
2019-09-16 11:29:51,890 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:29:51] "GET /import/delete_step1 HTTP/1.1" 200 -
2019-09-16 11:29:53,309 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:29:53] "GET /import/datasets HTTP/1.1" 200 -
2019-09-16 11:30:20,090 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 11:30:20,090 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 11:30:20,787 :: INFO :: File size = 10.378695487976074 Mo
2019-09-16 11:30:20,801 :: DEBUG :: original file name = data_pf_observado_original.csv
2019-09-16 11:30:20,801 :: INFO :: Upload : file saved in directory - time : 0:00:00.710812
2019-09-16 11:30:20,801 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 11:30:20,801 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 11:30:23,706 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'gn_date', 'heure', 'date_encodage', 'gn_timestamp', 'type_observation', 'nombre', 'gn_min', 'gn_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'lat', 'gn_long', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'gn_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 11:30:23,707 :: DEBUG :: row_count = 26516
2019-09-16 11:30:23,707 :: INFO :: User file validity checked - time : 0:00:02.906150
2019-09-16 11:30:23,707 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 11:30:23,754 :: DEBUG :: id_import = 391
2019-09-16 11:30:23,755 :: DEBUG :: id_role = 1
2019-09-16 11:30:23,797 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_original_391
2019-09-16 11:30:25,054 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 11:30:25,504 :: INFO :: CSV loaded to DB table - time : 0:00:00.449373
2019-09-16 11:30:25,505 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 11:30:25,505 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 11:30:25,931 :: INFO :: CSV loaded to DB table - time : 0:00:00.426195
2019-09-16 11:30:25,931 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 11:30:26,227 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 11:30:26,234 :: INFO :: Total time to post user file and fill metadata - time : 0:00:06.144167
2019-09-16 11:30:26,245 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:30:26] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 11:30:26,303 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:30:26] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 11:30:46,875 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:30:46] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 11:30:46,900 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 11:30:46,915 :: DEBUG :: import_id = 391
2019-09-16 11:30:46,915 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 11:30:46,915 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 11:30:46,915 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:30:46,932 :: WARNING :: ncores used by Dask = 4
2019-09-16 11:30:46,993 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.077668
2019-09-16 11:30:46,994 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:30:46,994 :: INFO :: * START DATA CLEANING
2019-09-16 11:30:47,053 :: INFO :: checking missing values : 
2019-09-16 11:30:47,154 :: INFO :: - checking missing values for cd_nom column
2019-09-16 11:30:47,311 :: INFO :: - checking missing values for nom_cite column
2019-09-16 11:30:47,469 :: INFO :: - checking missing values for date_min column
2019-09-16 11:30:47,637 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.583551
2019-09-16 11:30:47,637 :: INFO :: checking types : 
2019-09-16 11:30:47,637 :: INFO :: - checking date type for date_min column
2019-09-16 11:30:47,795 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 11:30:47,931 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.293985
2019-09-16 11:30:47,932 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 11:30:53,652 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.720171
2019-09-16 11:30:53,652 :: INFO :: checking date validity :
2019-09-16 11:30:53,653 :: INFO :: - checking if date_max column is missing
2019-09-16 11:30:53,653 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000639
2019-09-16 11:30:53,653 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 11:30:53,705 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.051937
2019-09-16 11:30:53,706 :: INFO :: checking count_min and count_max : 
2019-09-16 11:30:53,706 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000270
2019-09-16 11:30:53,706 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000015
2019-09-16 11:30:53,706 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000009
2019-09-16 11:30:53,707 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 11:30:53,707 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 11:30:53,707 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 11:30:53,707 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 11:30:53,707 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 11:30:53,707 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 11:30:53,732 :: INFO :: * END DATA CLEANING
2019-09-16 11:30:53,732 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 11:30:53,732 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 11:31:00,301 :: INFO :: dask df converted in pandas df - time : 0:00:06.568915
2019-09-16 11:31:00,301 :: INFO :: loading dataframe into DB table:
2019-09-16 11:31:01,735 :: INFO :: dask df loaded to db table - time : 0:00:01.433436
2019-09-16 11:31:01,991 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:08.259385
2019-09-16 11:31:02,018 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 11:31:02,067 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 11:31:02,110 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:31:02] "POST /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 11:32:42,498 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 11:32:42,498 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 11:32:42,634 :: INFO :: File size = 10.378695487976074 Mo
2019-09-16 11:32:42,666 :: DEBUG :: original file name = data_pf_observado_original.csv
2019-09-16 11:32:42,667 :: INFO :: Upload : file saved in directory - time : 0:00:00.168050
2019-09-16 11:32:42,667 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 11:32:42,667 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 11:32:45,595 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'gn_date', 'heure', 'date_encodage', 'gn_timestamp', 'type_observation', 'nombre', 'gn_min', 'gn_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'lat', 'gn_long', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'gn_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 11:32:45,595 :: DEBUG :: row_count = 26516
2019-09-16 11:32:45,596 :: INFO :: User file validity checked - time : 0:00:02.928658
2019-09-16 11:32:45,596 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 11:32:45,610 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_original_391
2019-09-16 11:32:45,900 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 11:32:46,360 :: INFO :: CSV loaded to DB table - time : 0:00:00.459863
2019-09-16 11:32:46,360 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 11:32:46,360 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 11:32:46,815 :: INFO :: CSV loaded to DB table - time : 0:00:00.454986
2019-09-16 11:32:46,815 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 11:32:47,290 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 11:32:47,299 :: INFO :: Total time to post user file and fill metadata - time : 0:00:04.800463
2019-09-16 11:32:47,310 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:32:47] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 11:32:47,366 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:32:47] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 11:33:08,955 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:33:08] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 11:33:08,983 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 11:33:08,998 :: DEBUG :: import_id = 391
2019-09-16 11:33:08,998 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 11:33:08,998 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 11:33:08,998 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:33:09,015 :: WARNING :: ncores used by Dask = 4
2019-09-16 11:33:09,089 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.090331
2019-09-16 11:33:09,089 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:33:09,089 :: INFO :: * START DATA CLEANING
2019-09-16 11:33:09,171 :: INFO :: checking missing values : 
2019-09-16 11:33:09,292 :: INFO :: - checking missing values for cd_nom column
2019-09-16 11:33:09,466 :: INFO :: - checking missing values for nom_cite column
2019-09-16 11:33:09,635 :: INFO :: - checking missing values for date_min column
2019-09-16 11:33:09,779 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.607923
2019-09-16 11:33:09,779 :: INFO :: checking types : 
2019-09-16 11:33:09,779 :: INFO :: - checking date type for date_min column
2019-09-16 11:33:09,925 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 11:33:10,048 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.268557
2019-09-16 11:33:10,048 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 11:33:13,805 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.756520
2019-09-16 11:33:13,805 :: INFO :: checking date validity :
2019-09-16 11:33:13,806 :: INFO :: - checking if date_max column is missing
2019-09-16 11:33:13,806 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000872
2019-09-16 11:33:13,807 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 11:33:13,860 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.052562
2019-09-16 11:33:13,860 :: INFO :: checking count_min and count_max : 
2019-09-16 11:33:13,860 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000159
2019-09-16 11:33:13,860 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 11:33:13,860 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 11:33:13,860 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 11:33:13,861 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 11:33:13,861 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 11:33:13,861 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 11:33:13,861 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 11:33:13,861 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 11:33:13,890 :: INFO :: * END DATA CLEANING
2019-09-16 11:33:13,891 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 11:33:13,891 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 11:33:20,282 :: INFO :: dask df converted in pandas df - time : 0:00:06.390746
2019-09-16 11:33:20,283 :: INFO :: loading dataframe into DB table:
2019-09-16 11:33:22,147 :: INFO :: dask df loaded to db table - time : 0:00:01.863670
2019-09-16 11:33:22,285 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:08.393997
2019-09-16 11:33:22,313 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 11:33:22,401 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 11:33:22,466 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:33:22] "POST /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 11:37:56,387 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 11:37:56,790 :: INFO ::  * Restarting with stat
2019-09-16 11:38:01,473 :: WARNING ::  * Debugger is active!
2019-09-16 11:38:01,474 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:38:14,910 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 11:38:15,339 :: INFO ::  * Restarting with stat
2019-09-16 11:38:17,368 :: WARNING ::  * Debugger is active!
2019-09-16 11:38:17,368 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:48:53,016 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 11:48:53,326 :: INFO ::  * Restarting with stat
2019-09-16 11:48:55,517 :: WARNING ::  * Debugger is active!
2019-09-16 11:48:55,518 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:52:19,473 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 11:52:20,017 :: INFO ::  * Restarting with stat
2019-09-16 11:52:22,957 :: WARNING ::  * Debugger is active!
2019-09-16 11:52:22,958 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:52:32,150 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 11:52:32,558 :: INFO ::  * Restarting with stat
2019-09-16 11:52:36,184 :: WARNING ::  * Debugger is active!
2019-09-16 11:52:36,185 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:52:48,534 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 11:52:49,151 :: INFO ::  * Restarting with stat
2019-09-16 11:52:52,139 :: WARNING ::  * Debugger is active!
2019-09-16 11:52:52,139 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:53:00,330 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 11:53:00,773 :: INFO ::  * Restarting with stat
2019-09-16 11:53:03,527 :: WARNING ::  * Debugger is active!
2019-09-16 11:53:03,528 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:53:12,650 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 11:53:12,651 :: INFO ::  * Restarting with stat
2019-09-16 11:53:14,644 :: WARNING ::  * Debugger is active!
2019-09-16 11:53:14,644 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:53:20,786 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:53:20] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 11:53:20,818 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 11:53:20,834 :: DEBUG :: import_id = 391
2019-09-16 11:53:20,834 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 11:53:20,834 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 11:53:20,846 :: DEBUG :: row number = 26515
2019-09-16 11:53:20,847 :: INFO :: type of dataframe = pandas
2019-09-16 11:53:20,847 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:53:20,867 :: WARNING :: ncores used by Dask = 4
2019-09-16 11:53:20,933 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.085859
2019-09-16 11:53:22,380 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:53:22,381 :: INFO :: * START DATA CLEANING
2019-09-16 11:53:22,394 :: INFO :: checking missing values : 
2019-09-16 11:53:22,454 :: INFO :: - checking missing values for cd_nom column
2019-09-16 11:53:22,461 :: INFO :: - checking missing values for nom_cite column
2019-09-16 11:53:22,468 :: INFO :: - checking missing values for date_min column
2019-09-16 11:53:22,471 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.077496
2019-09-16 11:53:22,471 :: INFO :: checking types : 
2019-09-16 11:53:22,472 :: INFO :: - checking date type for date_min column
2019-09-16 11:53:22,684 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 11:53:22,727 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.255603
2019-09-16 11:53:22,727 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 11:53:24,331 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:01.603533
2019-09-16 11:53:24,331 :: INFO :: checking date validity :
2019-09-16 11:53:24,331 :: INFO :: - checking if date_max column is missing
2019-09-16 11:53:24,332 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000366
2019-09-16 11:53:24,332 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 11:53:24,604 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.271815
2019-09-16 11:53:24,604 :: INFO :: checking count_min and count_max : 
2019-09-16 11:53:24,604 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000163
2019-09-16 11:53:24,604 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000012
2019-09-16 11:53:24,604 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000008
2019-09-16 11:53:24,604 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 11:53:24,801 :: INFO :: * END DATA CLEANING
2019-09-16 11:53:24,801 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 11:53:24,801 :: INFO :: loading dataframe into DB table:
2019-09-16 11:54:19,515 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 11:54:20,013 :: INFO ::  * Restarting with stat
2019-09-16 11:54:21,989 :: WARNING ::  * Debugger is active!
2019-09-16 11:54:21,990 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:55:28,650 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 11:55:28,651 :: INFO ::  * Restarting with stat
2019-09-16 11:55:30,224 :: WARNING ::  * Debugger is active!
2019-09-16 11:55:30,225 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:55:40,432 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 11:55:40,732 :: INFO ::  * Restarting with stat
2019-09-16 11:56:31,364 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 11:56:31,364 :: INFO ::  * Restarting with stat
2019-09-16 11:56:33,338 :: WARNING ::  * Debugger is active!
2019-09-16 11:56:33,339 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:56:37,511 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:56:37] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 11:56:37,536 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 11:56:37,551 :: DEBUG :: import_id = 391
2019-09-16 11:56:37,551 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 11:56:37,551 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 11:56:37,562 :: DEBUG :: row number = 26515
2019-09-16 11:56:37,562 :: INFO :: type of dataframe = pandas
2019-09-16 11:56:37,562 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:56:37,580 :: WARNING :: ncores used by Dask = 4
2019-09-16 11:56:37,635 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.073157
2019-09-16 11:56:39,049 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:56:39,050 :: INFO :: * START DATA CLEANING
2019-09-16 11:56:39,061 :: INFO :: checking missing values : 
2019-09-16 11:56:39,130 :: INFO :: - checking missing values for cd_nom column
2019-09-16 11:56:39,135 :: INFO :: - checking missing values for nom_cite column
2019-09-16 11:56:39,142 :: INFO :: - checking missing values for date_min column
2019-09-16 11:56:39,146 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.084373
2019-09-16 11:56:39,146 :: INFO :: checking types : 
2019-09-16 11:56:39,146 :: INFO :: - checking date type for date_min column
2019-09-16 11:56:39,312 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 11:56:39,385 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.239313
2019-09-16 11:56:39,386 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 11:56:40,948 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:01.562528
2019-09-16 11:56:40,948 :: INFO :: checking date validity :
2019-09-16 11:56:40,949 :: INFO :: - checking if date_max column is missing
2019-09-16 11:56:40,949 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000249
2019-09-16 11:56:40,949 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 11:56:41,155 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.206372
2019-09-16 11:56:41,155 :: INFO :: checking count_min and count_max : 
2019-09-16 11:56:41,156 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000130
2019-09-16 11:56:41,156 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 11:56:41,156 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000007
2019-09-16 11:56:41,156 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 11:56:41,350 :: INFO :: * END DATA CLEANING
2019-09-16 11:56:41,350 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 11:56:52,753 :: INFO :: loading dataframe into DB table:
2019-09-16 11:58:11,075 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 11:58:11,685 :: INFO ::  * Restarting with stat
2019-09-16 11:58:13,925 :: WARNING ::  * Debugger is active!
2019-09-16 11:58:13,926 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 11:58:15,672 :: INFO :: 127.0.0.1 - - [16/Sep/2019 11:58:15] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 11:58:15,697 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 11:58:15,718 :: DEBUG :: import_id = 391
2019-09-16 11:58:15,718 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 11:58:15,719 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 11:58:15,734 :: DEBUG :: row number = 26515
2019-09-16 11:58:15,734 :: INFO :: type of dataframe = pandas
2019-09-16 11:58:15,734 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:58:15,757 :: WARNING :: ncores used by Dask = 4
2019-09-16 11:58:15,841 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.106448
2019-09-16 11:58:17,418 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 11:58:17,419 :: INFO :: * START DATA CLEANING
2019-09-16 11:58:17,434 :: INFO :: checking missing values : 
2019-09-16 11:58:17,499 :: INFO :: - checking missing values for cd_nom column
2019-09-16 11:58:17,508 :: INFO :: - checking missing values for nom_cite column
2019-09-16 11:58:17,514 :: INFO :: - checking missing values for date_min column
2019-09-16 11:58:17,518 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.084093
2019-09-16 11:58:17,518 :: INFO :: checking types : 
2019-09-16 11:58:17,518 :: INFO :: - checking date type for date_min column
2019-09-16 11:58:17,799 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 11:58:17,856 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.338215
2019-09-16 11:58:17,856 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 11:58:19,870 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:02.013771
2019-09-16 11:58:19,870 :: INFO :: checking date validity :
2019-09-16 11:58:19,871 :: INFO :: - checking if date_max column is missing
2019-09-16 11:58:19,871 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000342
2019-09-16 11:58:19,871 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 11:58:20,251 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.379842
2019-09-16 11:58:20,251 :: INFO :: checking count_min and count_max : 
2019-09-16 11:58:20,251 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000137
2019-09-16 11:58:20,251 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-16 11:58:20,251 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 11:58:20,252 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 11:58:20,450 :: INFO :: * END DATA CLEANING
2019-09-16 11:58:20,450 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 11:58:33,849 :: INFO :: loading dataframe into DB table:
2019-09-16 11:59:30,524 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 11:59:30,524 :: INFO ::  * Restarting with stat
2019-09-16 11:59:32,193 :: WARNING ::  * Debugger is active!
2019-09-16 11:59:32,193 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:00:02,708 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 12:00:02,968 :: INFO ::  * Restarting with stat
2019-09-16 12:00:05,119 :: WARNING ::  * Debugger is active!
2019-09-16 12:00:05,119 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:00:06,972 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:00:06] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:00:07,002 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:00:07,018 :: DEBUG :: import_id = 391
2019-09-16 12:00:07,018 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:00:07,018 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:00:07,029 :: DEBUG :: row number = 26515
2019-09-16 12:00:07,029 :: INFO :: type of dataframe = pandas
2019-09-16 12:00:07,030 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:00:07,053 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:00:07,125 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.095047
2019-09-16 12:00:08,708 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:00:08,709 :: INFO :: * START DATA CLEANING
2019-09-16 12:00:08,729 :: INFO :: checking missing values : 
2019-09-16 12:00:08,825 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:00:08,837 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:00:08,846 :: INFO :: - checking missing values for date_min column
2019-09-16 12:00:08,854 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.125272
2019-09-16 12:00:08,854 :: INFO :: checking types : 
2019-09-16 12:00:08,855 :: INFO :: - checking date type for date_min column
2019-09-16 12:00:09,239 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:00:09,372 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.518201
2019-09-16 12:00:09,373 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:00:11,505 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:02.132305
2019-09-16 12:00:11,505 :: INFO :: checking date validity :
2019-09-16 12:00:11,506 :: INFO :: - checking if date_max column is missing
2019-09-16 12:00:11,506 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000334
2019-09-16 12:00:11,506 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:00:11,856 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.349962
2019-09-16 12:00:11,857 :: INFO :: checking count_min and count_max : 
2019-09-16 12:00:11,857 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000211
2019-09-16 12:00:11,857 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-16 12:00:11,857 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000009
2019-09-16 12:00:11,857 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:00:12,177 :: INFO :: * END DATA CLEANING
2019-09-16 12:00:12,177 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:00:12,177 :: INFO :: loading dataframe into DB table:
2019-09-16 12:00:32,021 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 12:00:32,620 :: INFO ::  * Restarting with stat
2019-09-16 12:00:34,476 :: WARNING ::  * Debugger is active!
2019-09-16 12:00:34,476 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:00:52,814 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 12:00:53,279 :: INFO ::  * Restarting with stat
2019-09-16 12:00:58,015 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 12:00:58,015 :: INFO ::  * Restarting with stat
2019-09-16 12:00:59,600 :: WARNING ::  * Debugger is active!
2019-09-16 12:00:59,601 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:01:01,738 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:01:01] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:01:01,768 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:01:01,782 :: DEBUG :: import_id = 391
2019-09-16 12:01:01,782 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:01:01,782 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:01:01,792 :: DEBUG :: row number = 26515
2019-09-16 12:01:01,793 :: INFO :: type of dataframe = pandas
2019-09-16 12:01:01,793 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:01:01,810 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:01:01,876 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.082372
2019-09-16 12:01:03,443 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:01:03,445 :: INFO :: * START DATA CLEANING
2019-09-16 12:01:03,461 :: INFO :: checking missing values : 
2019-09-16 12:01:03,523 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:01:03,528 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:01:03,533 :: INFO :: - checking missing values for date_min column
2019-09-16 12:01:03,536 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.074707
2019-09-16 12:01:03,536 :: INFO :: checking types : 
2019-09-16 12:01:03,536 :: INFO :: - checking date type for date_min column
2019-09-16 12:01:03,751 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:01:03,816 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.280075
2019-09-16 12:01:03,816 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:01:05,673 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:01.856172
2019-09-16 12:01:05,673 :: INFO :: checking date validity :
2019-09-16 12:01:05,673 :: INFO :: - checking if date_max column is missing
2019-09-16 12:01:05,673 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000337
2019-09-16 12:01:05,673 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:01:05,901 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.227683
2019-09-16 12:01:05,901 :: INFO :: checking count_min and count_max : 
2019-09-16 12:01:05,902 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000199
2019-09-16 12:01:05,902 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 12:01:05,902 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 12:01:05,902 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:01:06,104 :: INFO :: * END DATA CLEANING
2019-09-16 12:01:06,104 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:01:06,104 :: INFO :: loading dataframe into DB table:
2019-09-16 12:04:02,672 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 12:04:02,672 :: INFO ::  * Restarting with stat
2019-09-16 12:04:04,316 :: WARNING ::  * Debugger is active!
2019-09-16 12:04:04,316 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:04:16,524 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 12:04:16,776 :: INFO ::  * Restarting with stat
2019-09-16 12:04:18,999 :: WARNING ::  * Debugger is active!
2019-09-16 12:04:18,999 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:04:21,636 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:04:21] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:04:21,655 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:04:21,671 :: DEBUG :: import_id = 391
2019-09-16 12:04:21,671 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:04:21,671 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:04:21,684 :: DEBUG :: row number = 26515
2019-09-16 12:04:21,684 :: INFO :: type of dataframe = dask
2019-09-16 12:04:21,684 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:04:21,706 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:04:21,776 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.091010
2019-09-16 12:04:21,776 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:04:21,777 :: INFO :: * START DATA CLEANING
2019-09-16 12:04:21,858 :: INFO :: checking missing values : 
2019-09-16 12:04:22,005 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:04:22,169 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:04:22,342 :: INFO :: - checking missing values for date_min column
2019-09-16 12:04:22,501 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.643480
2019-09-16 12:04:22,501 :: INFO :: checking types : 
2019-09-16 12:04:22,501 :: INFO :: - checking date type for date_min column
2019-09-16 12:04:22,673 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:04:22,803 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.301798
2019-09-16 12:04:22,803 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:04:29,591 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:06.787169
2019-09-16 12:04:29,591 :: INFO :: checking date validity :
2019-09-16 12:04:29,591 :: INFO :: - checking if date_max column is missing
2019-09-16 12:04:29,591 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000393
2019-09-16 12:04:29,592 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:04:29,653 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.061736
2019-09-16 12:04:29,654 :: INFO :: checking count_min and count_max : 
2019-09-16 12:04:29,654 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000125
2019-09-16 12:04:29,654 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 12:04:29,654 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000007
2019-09-16 12:04:29,654 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 12:04:29,654 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 12:04:29,654 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 12:04:29,655 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 12:04:29,655 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 12:04:29,655 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:04:29,687 :: INFO :: * END DATA CLEANING
2019-09-16 12:04:29,687 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:04:29,687 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 12:04:37,606 :: INFO :: dask df converted in pandas df - time : 0:00:07.918810
2019-09-16 12:04:37,607 :: INFO :: loading dataframe into DB table:
2019-09-16 12:05:43,792 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 12:05:43,793 :: INFO ::  * Restarting with stat
2019-09-16 12:05:45,946 :: WARNING ::  * Debugger is active!
2019-09-16 12:05:45,946 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:05:46,942 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 12:05:46,942 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 12:05:47,115 :: INFO :: File size = 10.378695487976074 Mo
2019-09-16 12:05:47,147 :: DEBUG :: original file name = data_pf_observado_original.csv
2019-09-16 12:05:47,147 :: INFO :: Upload : file saved in directory - time : 0:00:00.205261
2019-09-16 12:05:47,148 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 12:05:47,148 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 12:05:50,615 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'gn_date', 'heure', 'date_encodage', 'gn_timestamp', 'type_observation', 'nombre', 'gn_min', 'gn_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'lat', 'gn_long', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'gn_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 12:05:50,615 :: DEBUG :: row_count = 26516
2019-09-16 12:05:50,615 :: INFO :: User file validity checked - time : 0:00:03.467350
2019-09-16 12:05:50,615 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 12:05:50,625 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_original_391
2019-09-16 12:05:50,898 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 12:05:51,426 :: INFO :: CSV loaded to DB table - time : 0:00:00.527114
2019-09-16 12:05:51,426 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 12:05:51,426 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 12:05:51,878 :: INFO :: CSV loaded to DB table - time : 0:00:00.451466
2019-09-16 12:05:51,878 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 12:05:52,673 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 12:05:52,675 :: INFO :: Total time to post user file and fill metadata - time : 0:00:05.733523
2019-09-16 12:05:52,681 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:05:52] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 12:05:52,724 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:05:52] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 12:05:56,090 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:05:56] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:05:56,109 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:05:56,121 :: DEBUG :: import_id = 391
2019-09-16 12:05:56,121 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:05:56,122 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:05:56,136 :: DEBUG :: row number = 26515
2019-09-16 12:05:56,137 :: INFO :: type of dataframe = dask
2019-09-16 12:05:56,137 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:05:56,159 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:05:56,232 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.094924
2019-09-16 12:05:56,232 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:05:56,232 :: INFO :: * START DATA CLEANING
2019-09-16 12:05:56,291 :: INFO :: checking missing values : 
2019-09-16 12:05:56,390 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:05:56,544 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:05:56,702 :: INFO :: - checking missing values for date_min column
2019-09-16 12:05:56,853 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.561285
2019-09-16 12:05:56,853 :: INFO :: checking types : 
2019-09-16 12:05:56,854 :: INFO :: - checking date type for date_min column
2019-09-16 12:05:57,024 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:05:57,166 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.312450
2019-09-16 12:05:57,166 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:06:02,553 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.386374
2019-09-16 12:06:02,553 :: INFO :: checking date validity :
2019-09-16 12:06:02,553 :: INFO :: - checking if date_max column is missing
2019-09-16 12:06:02,553 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000348
2019-09-16 12:06:02,553 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:06:02,616 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.062459
2019-09-16 12:06:02,616 :: INFO :: checking count_min and count_max : 
2019-09-16 12:06:02,616 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000134
2019-09-16 12:06:02,616 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-16 12:06:02,617 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000013
2019-09-16 12:06:02,617 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 12:06:02,617 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 12:06:02,617 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 12:06:02,617 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 12:06:02,618 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 12:06:02,618 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:06:02,646 :: INFO :: * END DATA CLEANING
2019-09-16 12:06:02,647 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:06:02,648 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 12:06:10,042 :: INFO :: dask df converted in pandas df - time : 0:00:07.393615
2019-09-16 12:06:10,043 :: INFO :: loading dataframe into DB table:
2019-09-16 12:06:28,049 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 12:06:28,049 :: INFO ::  * Restarting with stat
2019-09-16 12:06:29,881 :: WARNING ::  * Debugger is active!
2019-09-16 12:06:29,882 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:07:50,682 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:07:50] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:07:50,708 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:07:50,723 :: DEBUG :: import_id = 391
2019-09-16 12:07:50,724 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:07:50,724 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:07:50,735 :: DEBUG :: row number = 26515
2019-09-16 12:07:50,735 :: INFO :: type of dataframe = dask
2019-09-16 12:07:50,735 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:07:50,752 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:07:50,812 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.076534
2019-09-16 12:07:50,812 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:07:50,812 :: INFO :: * START DATA CLEANING
2019-09-16 12:07:50,875 :: INFO :: checking missing values : 
2019-09-16 12:07:50,965 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:07:51,137 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:07:51,297 :: INFO :: - checking missing values for date_min column
2019-09-16 12:07:51,443 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.568357
2019-09-16 12:07:51,443 :: INFO :: checking types : 
2019-09-16 12:07:51,444 :: INFO :: - checking date type for date_min column
2019-09-16 12:07:51,617 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:07:51,739 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.296174
2019-09-16 12:07:51,740 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:07:57,099 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.358888
2019-09-16 12:07:57,099 :: INFO :: checking date validity :
2019-09-16 12:07:57,099 :: INFO :: - checking if date_max column is missing
2019-09-16 12:07:57,099 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000314
2019-09-16 12:07:57,099 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:07:57,156 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.057136
2019-09-16 12:07:57,157 :: INFO :: checking count_min and count_max : 
2019-09-16 12:07:57,158 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000431
2019-09-16 12:07:57,158 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000013
2019-09-16 12:07:57,158 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000013
2019-09-16 12:07:57,158 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 12:07:57,159 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 12:07:57,159 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 12:07:57,159 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 12:07:57,159 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 12:07:57,159 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:07:57,188 :: INFO :: * END DATA CLEANING
2019-09-16 12:07:57,188 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:07:57,188 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 12:08:04,571 :: INFO :: dask df converted in pandas df - time : 0:00:07.382116
2019-09-16 12:08:04,571 :: INFO :: loading dataframe into DB table:
2019-09-16 12:09:38,557 :: INFO :: 192.168.1.170 - - [16/Sep/2019 12:09:38] "GET / HTTP/1.1" 404 -
2019-09-16 12:10:02,949 :: INFO :: 192.168.1.170 - - [16/Sep/2019 12:10:02] "GET / HTTP/1.1" 404 -
2019-09-16 12:10:38,299 :: INFO :: 192.168.1.170 - - [16/Sep/2019 12:10:38] "GET / HTTP/1.1" 404 -
2019-09-16 12:10:41,238 :: INFO :: 192.168.1.170 - - [16/Sep/2019 12:10:41] "GET / HTTP/1.1" 404 -
2019-09-16 12:50:09,266 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 12:50:09,267 :: INFO ::  * Restarting with stat
2019-09-16 12:50:12,012 :: WARNING ::  * Debugger is active!
2019-09-16 12:50:12,013 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:50:18,090 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:50:18] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:50:18,118 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:50:18,133 :: DEBUG :: import_id = 391
2019-09-16 12:50:18,133 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:50:18,133 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:50:18,146 :: DEBUG :: row number = 26515
2019-09-16 12:50:18,146 :: INFO :: type of dataframe = dask
2019-09-16 12:50:18,146 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:50:18,166 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:50:18,230 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.084236
2019-09-16 12:50:18,231 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:50:18,232 :: INFO :: * START DATA CLEANING
2019-09-16 12:50:18,295 :: INFO :: checking missing values : 
2019-09-16 12:50:18,383 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:50:18,532 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:50:18,694 :: INFO :: - checking missing values for date_min column
2019-09-16 12:50:18,846 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.551555
2019-09-16 12:50:18,847 :: INFO :: checking types : 
2019-09-16 12:50:18,847 :: INFO :: - checking date type for date_min column
2019-09-16 12:50:18,993 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:50:19,117 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.270177
2019-09-16 12:50:19,117 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:50:24,191 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.074032
2019-09-16 12:50:24,192 :: INFO :: checking date validity :
2019-09-16 12:50:24,194 :: INFO :: - checking if date_max column is missing
2019-09-16 12:50:24,195 :: INFO :: Data cleaning : dates checked - time : 0:00:00.002346
2019-09-16 12:50:24,195 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:50:24,267 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.071561
2019-09-16 12:50:24,268 :: INFO :: checking count_min and count_max : 
2019-09-16 12:50:24,268 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000517
2019-09-16 12:50:24,268 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000018
2019-09-16 12:50:24,269 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000016
2019-09-16 12:50:24,269 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 12:50:24,269 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 12:50:24,270 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 12:50:24,270 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 12:50:24,270 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 12:50:24,270 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:50:24,303 :: INFO :: * END DATA CLEANING
2019-09-16 12:50:24,303 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:50:24,303 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 12:50:31,461 :: INFO :: dask df converted in pandas df - time : 0:00:07.157384
2019-09-16 12:50:31,461 :: INFO :: loading dataframe into DB table:
2019-09-16 12:51:51,528 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 12:51:51,529 :: INFO ::  * Restarting with stat
2019-09-16 12:51:53,293 :: WARNING ::  * Debugger is active!
2019-09-16 12:51:53,294 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:53:53,605 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 12:53:53,902 :: INFO ::  * Restarting with stat
2019-09-16 12:53:55,600 :: WARNING ::  * Debugger is active!
2019-09-16 12:53:55,600 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:53:59,523 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:53:59] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:53:59,552 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:53:59,568 :: DEBUG :: import_id = 391
2019-09-16 12:53:59,568 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:53:59,568 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:53:59,577 :: DEBUG :: row number = 26515
2019-09-16 12:53:59,578 :: INFO :: type of dataframe = dask
2019-09-16 12:53:59,578 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:53:59,602 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:53:59,686 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.106959
2019-09-16 12:53:59,686 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:53:59,686 :: INFO :: * START DATA CLEANING
2019-09-16 12:53:59,757 :: INFO :: checking missing values : 
2019-09-16 12:53:59,883 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:54:00,044 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:54:00,179 :: INFO :: - checking missing values for date_min column
2019-09-16 12:54:00,319 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.561324
2019-09-16 12:54:00,319 :: INFO :: checking types : 
2019-09-16 12:54:00,319 :: INFO :: - checking date type for date_min column
2019-09-16 12:54:00,456 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:54:00,579 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.259632
2019-09-16 12:54:00,579 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:54:05,695 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.115617
2019-09-16 12:54:05,695 :: INFO :: checking date validity :
2019-09-16 12:54:05,695 :: INFO :: - checking if date_max column is missing
2019-09-16 12:54:05,696 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000489
2019-09-16 12:54:05,696 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:54:05,749 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.052657
2019-09-16 12:54:05,749 :: INFO :: checking count_min and count_max : 
2019-09-16 12:54:05,749 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000358
2019-09-16 12:54:05,750 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000008
2019-09-16 12:54:05,750 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000009
2019-09-16 12:54:05,750 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 12:54:05,750 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 12:54:05,750 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 12:54:05,751 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 12:54:05,751 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 12:54:05,751 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:54:05,779 :: INFO :: * END DATA CLEANING
2019-09-16 12:54:05,779 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:54:05,779 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 12:54:13,067 :: INFO :: dask df converted in pandas df - time : 0:00:07.287547
2019-09-16 12:55:15,580 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 12:55:16,543 :: INFO ::  * Restarting with stat
2019-09-16 12:55:18,432 :: WARNING ::  * Debugger is active!
2019-09-16 12:55:18,433 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:55:23,277 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:55:23] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:55:23,303 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:55:23,319 :: DEBUG :: import_id = 391
2019-09-16 12:55:23,319 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:55:23,319 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:55:23,331 :: DEBUG :: row number = 26515
2019-09-16 12:55:23,332 :: INFO :: type of dataframe = dask
2019-09-16 12:55:23,332 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:55:23,348 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:55:23,413 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.080531
2019-09-16 12:55:23,413 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:55:23,414 :: INFO :: * START DATA CLEANING
2019-09-16 12:55:23,487 :: INFO :: checking missing values : 
2019-09-16 12:55:23,598 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:55:23,781 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:55:23,949 :: INFO :: - checking missing values for date_min column
2019-09-16 12:55:24,130 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.643732
2019-09-16 12:55:24,130 :: INFO :: checking types : 
2019-09-16 12:55:24,131 :: INFO :: - checking date type for date_min column
2019-09-16 12:55:24,318 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:55:24,469 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.338817
2019-09-16 12:55:24,470 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:55:29,159 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.689373
2019-09-16 12:55:29,159 :: INFO :: checking date validity :
2019-09-16 12:55:29,160 :: INFO :: - checking if date_max column is missing
2019-09-16 12:55:29,160 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000706
2019-09-16 12:55:29,161 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:55:29,217 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.056400
2019-09-16 12:55:29,217 :: INFO :: checking count_min and count_max : 
2019-09-16 12:55:29,217 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000157
2019-09-16 12:55:29,218 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 12:55:29,218 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 12:55:29,218 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 12:55:29,218 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 12:55:29,218 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 12:55:29,218 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 12:55:29,218 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 12:55:29,218 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:55:29,251 :: INFO :: * END DATA CLEANING
2019-09-16 12:55:29,251 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:55:29,252 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 12:55:35,733 :: INFO :: dask df converted in pandas df - time : 0:00:06.481089
2019-09-16 12:55:37,338 :: INFO :: loading dataframe into DB table:
2019-09-16 12:56:50,893 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 12:56:50,894 :: INFO ::  * Restarting with stat
2019-09-16 12:56:52,667 :: WARNING ::  * Debugger is active!
2019-09-16 12:56:52,667 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:56:55,769 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:56:55] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:56:55,796 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:56:55,810 :: DEBUG :: import_id = 391
2019-09-16 12:56:55,810 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:56:55,811 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:56:55,820 :: DEBUG :: row number = 26515
2019-09-16 12:56:55,820 :: INFO :: type of dataframe = dask
2019-09-16 12:56:55,820 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:56:55,841 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:56:55,904 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.083956
2019-09-16 12:56:55,905 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:56:55,905 :: INFO :: * START DATA CLEANING
2019-09-16 12:56:55,965 :: INFO :: checking missing values : 
2019-09-16 12:56:56,061 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:56:56,213 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:56:56,361 :: INFO :: - checking missing values for date_min column
2019-09-16 12:56:56,515 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.549703
2019-09-16 12:56:56,515 :: INFO :: checking types : 
2019-09-16 12:56:56,515 :: INFO :: - checking date type for date_min column
2019-09-16 12:56:56,663 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:56:56,793 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.277283
2019-09-16 12:56:56,793 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:57:01,908 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.115081
2019-09-16 12:57:01,909 :: INFO :: checking date validity :
2019-09-16 12:57:01,909 :: INFO :: - checking if date_max column is missing
2019-09-16 12:57:01,910 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000820
2019-09-16 12:57:01,910 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:57:01,965 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.055470
2019-09-16 12:57:01,966 :: INFO :: checking count_min and count_max : 
2019-09-16 12:57:01,966 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000286
2019-09-16 12:57:01,966 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 12:57:01,966 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000012
2019-09-16 12:57:01,967 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 12:57:01,967 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 12:57:01,967 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 12:57:01,967 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 12:57:01,967 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 12:57:01,967 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:57:01,995 :: INFO :: * END DATA CLEANING
2019-09-16 12:57:01,996 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:57:01,996 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 12:57:09,482 :: INFO :: dask df converted in pandas df - time : 0:00:07.486315
2019-09-16 12:57:10,867 :: INFO :: loading dataframe into DB table:
2019-09-16 12:57:42,285 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 12:57:42,861 :: INFO ::  * Restarting with stat
2019-09-16 12:57:45,407 :: WARNING ::  * Debugger is active!
2019-09-16 12:57:45,408 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:57:56,565 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 12:57:56,566 :: INFO ::  * Restarting with stat
2019-09-16 12:57:58,617 :: WARNING ::  * Debugger is active!
2019-09-16 12:57:58,618 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:58:49,572 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 12:58:49,886 :: INFO ::  * Restarting with stat
2019-09-16 12:58:52,068 :: WARNING ::  * Debugger is active!
2019-09-16 12:58:52,068 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 12:58:53,623 :: INFO :: 127.0.0.1 - - [16/Sep/2019 12:58:53] "OPTIONS /import/mapping/391 HTTP/1.1" 200 -
2019-09-16 12:58:53,653 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 12:58:53,667 :: DEBUG :: import_id = 391
2019-09-16 12:58:53,667 :: DEBUG :: DB tabel name = i_data_pf_observado_original_391
2019-09-16 12:58:53,667 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 12:58:53,675 :: DEBUG :: row number = 26515
2019-09-16 12:58:53,676 :: INFO :: type of dataframe = dask
2019-09-16 12:58:53,676 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:58:53,694 :: WARNING :: ncores used by Dask = 4
2019-09-16 12:58:53,754 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.078216
2019-09-16 12:58:53,755 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 12:58:53,755 :: INFO :: * START DATA CLEANING
2019-09-16 12:58:53,815 :: INFO :: checking missing values : 
2019-09-16 12:58:53,904 :: INFO :: - checking missing values for cd_nom column
2019-09-16 12:58:54,035 :: INFO :: - checking missing values for nom_cite column
2019-09-16 12:58:54,184 :: INFO :: - checking missing values for date_min column
2019-09-16 12:58:54,328 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.513894
2019-09-16 12:58:54,329 :: INFO :: checking types : 
2019-09-16 12:58:54,329 :: INFO :: - checking date type for date_min column
2019-09-16 12:58:54,465 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 12:58:54,582 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.253594
2019-09-16 12:58:54,583 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 12:58:58,967 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.383954
2019-09-16 12:58:58,967 :: INFO :: checking date validity :
2019-09-16 12:58:58,967 :: INFO :: - checking if date_max column is missing
2019-09-16 12:58:58,967 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000291
2019-09-16 12:58:58,967 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 12:58:59,023 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.055864
2019-09-16 12:58:59,024 :: INFO :: checking count_min and count_max : 
2019-09-16 12:58:59,024 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000400
2019-09-16 12:58:59,025 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000059
2019-09-16 12:58:59,025 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000023
2019-09-16 12:58:59,026 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 12:58:59,026 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 12:58:59,026 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 12:58:59,027 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 12:58:59,027 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 12:58:59,027 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 12:58:59,074 :: INFO :: * END DATA CLEANING
2019-09-16 12:58:59,075 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 12:58:59,075 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 12:59:05,833 :: INFO :: dask df converted in pandas df - time : 0:00:06.757726
2019-09-16 12:59:05,834 :: INFO :: loading dataframe into DB table:
2019-09-16 12:59:41,948 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 12:59:42,669 :: INFO ::  * Restarting with stat
2019-09-16 12:59:45,235 :: WARNING ::  * Debugger is active!
2019-09-16 12:59:45,236 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:02:10,779 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:02:10,780 :: INFO ::  * Restarting with stat
2019-09-16 13:02:12,603 :: WARNING ::  * Debugger is active!
2019-09-16 13:02:12,603 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:02:53,429 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:02:53] "GET /gn_commons/modules HTTP/1.1" 200 -
2019-09-16 13:02:53,505 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:02:53] "GET /gn_commons/modules HTTP/1.1" 200 -
2019-09-16 13:02:53,683 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:02:53] "GET /synthese/general_stats HTTP/1.1" 200 -
2019-09-16 13:02:53,991 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:02:53] "GET /synthese/for_web?limit=100 HTTP/1.1" 200 -
2019-09-16 13:02:55,650 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:02:55] "GET /import/delete_step1 HTTP/1.1" 200 -
2019-09-16 13:02:55,674 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:02:55] "GET /import HTTP/1.1" 200 -
2019-09-16 13:02:57,096 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:02:57] "GET /import/datasets HTTP/1.1" 200 -
2019-09-16 13:03:07,730 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 13:03:07,731 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 13:03:07,881 :: INFO :: File size = 10.378695487976074 Mo
2019-09-16 13:03:07,901 :: DEBUG :: original file name = data_pf_observado_original.csv
2019-09-16 13:03:07,901 :: INFO :: Upload : file saved in directory - time : 0:00:00.170085
2019-09-16 13:03:07,901 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 13:03:07,901 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 13:03:10,928 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'gn_date', 'heure', 'date_encodage', 'gn_timestamp', 'type_observation', 'nombre', 'gn_min', 'gn_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'lat', 'gn_long', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'gn_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 13:03:10,928 :: DEBUG :: row_count = 26516
2019-09-16 13:03:10,928 :: INFO :: User file validity checked - time : 0:00:03.027144
2019-09-16 13:03:10,928 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 13:03:10,969 :: DEBUG :: id_import = 392
2019-09-16 13:03:10,969 :: DEBUG :: id_role = 1
2019-09-16 13:03:11,013 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_original_392
2019-09-16 13:03:11,314 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 13:03:11,740 :: INFO :: CSV loaded to DB table - time : 0:00:00.425745
2019-09-16 13:03:11,740 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 13:03:11,741 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 13:03:12,212 :: INFO :: CSV loaded to DB table - time : 0:00:00.470440
2019-09-16 13:03:12,213 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 13:03:12,416 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 13:03:12,423 :: INFO :: Total time to post user file and fill metadata - time : 0:00:04.692913
2019-09-16 13:03:12,435 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:03:12] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 13:03:12,492 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:03:12] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 13:03:22,513 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:03:22] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:03:22,558 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:03:22,576 :: DEBUG :: import_id = 392
2019-09-16 13:03:22,576 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:03:22,577 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:03:22,597 :: DEBUG :: row number = 26515
2019-09-16 13:03:22,598 :: INFO :: type of dataframe = dask
2019-09-16 13:03:22,598 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:03:22,625 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:03:22,723 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.123835
2019-09-16 13:03:22,723 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:03:22,723 :: INFO :: * START DATA CLEANING
2019-09-16 13:03:22,810 :: INFO :: checking missing values : 
2019-09-16 13:03:22,913 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:03:23,071 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:03:23,253 :: INFO :: - checking missing values for date_min column
2019-09-16 13:03:23,477 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.667641
2019-09-16 13:03:23,478 :: INFO :: checking types : 
2019-09-16 13:03:23,478 :: INFO :: - checking date type for date_min column
2019-09-16 13:03:23,646 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:03:23,783 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.304818
2019-09-16 13:03:23,783 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:03:30,005 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:06.222063
2019-09-16 13:03:30,005 :: INFO :: checking date validity :
2019-09-16 13:03:30,005 :: INFO :: - checking if date_max column is missing
2019-09-16 13:03:30,006 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000317
2019-09-16 13:03:30,006 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:03:30,105 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.098917
2019-09-16 13:03:30,105 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 13:03:30,105 :: ERROR :: check_counts() takes 4 positional arguments but 5 were given
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 560, in postMapping
    transform_errors = data_cleaning(df, selected_columns, MISSING_VALUES, DEFAULT_COUNT_VALUE, df_type)
  File "/home/ju/geonature/external_modules/import/backend/transform/transform.py", line 40, in data_cleaning
    error_check_counts = check_counts(df, selected_columns, synthese_info, def_count_val, df_type)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
TypeError: check_counts() takes 4 positional arguments but 5 were given
2019-09-16 13:03:30,206 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:03:30] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 13:03:47,725 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 13:03:48,025 :: INFO ::  * Restarting with stat
2019-09-16 13:03:49,931 :: WARNING ::  * Debugger is active!
2019-09-16 13:03:49,932 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:03:52,003 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:03:52] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:03:52,032 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:03:52,053 :: DEBUG :: import_id = 392
2019-09-16 13:03:52,053 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:03:52,053 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:03:52,066 :: DEBUG :: row number = 26515
2019-09-16 13:03:52,067 :: INFO :: type of dataframe = dask
2019-09-16 13:03:52,068 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:03:52,104 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:03:52,205 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.136456
2019-09-16 13:03:52,205 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:03:52,205 :: INFO :: * START DATA CLEANING
2019-09-16 13:03:52,283 :: INFO :: checking missing values : 
2019-09-16 13:03:52,383 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:03:52,567 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:03:52,707 :: INFO :: - checking missing values for date_min column
2019-09-16 13:03:52,856 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.573224
2019-09-16 13:03:52,857 :: INFO :: checking types : 
2019-09-16 13:03:52,857 :: INFO :: - checking date type for date_min column
2019-09-16 13:03:53,004 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:03:53,130 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.273122
2019-09-16 13:03:53,130 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:03:57,184 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.053520
2019-09-16 13:03:57,184 :: INFO :: checking date validity :
2019-09-16 13:03:57,184 :: INFO :: - checking if date_max column is missing
2019-09-16 13:03:57,184 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000312
2019-09-16 13:03:57,184 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:03:57,246 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.062001
2019-09-16 13:03:57,247 :: INFO :: checking count_min and count_max : 
2019-09-16 13:03:57,247 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000164
2019-09-16 13:03:57,247 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-16 13:03:57,247 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 13:03:57,247 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:03:57,247 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:03:57,247 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:03:57,248 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:03:57,248 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:03:57,248 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:03:57,283 :: INFO :: * END DATA CLEANING
2019-09-16 13:03:57,284 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:03:57,284 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:04:03,768 :: INFO :: dask df converted in pandas df - time : 0:00:06.484371
2019-09-16 13:04:03,769 :: INFO :: loading dataframe into DB table:
2019-09-16 13:06:20,617 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:06:20,617 :: INFO ::  * Restarting with stat
2019-09-16 13:06:22,176 :: WARNING ::  * Debugger is active!
2019-09-16 13:06:22,177 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:06:25,881 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:06:25] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:06:25,912 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:06:25,930 :: DEBUG :: import_id = 392
2019-09-16 13:06:25,931 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:06:25,931 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:06:25,942 :: DEBUG :: row number = 26515
2019-09-16 13:06:25,942 :: INFO :: type of dataframe = dask
2019-09-16 13:06:25,942 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:06:25,961 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:06:26,037 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.094179
2019-09-16 13:06:26,037 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:06:26,037 :: INFO :: * START DATA CLEANING
2019-09-16 13:06:26,099 :: INFO :: checking missing values : 
2019-09-16 13:06:26,183 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:06:26,334 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:06:26,498 :: INFO :: - checking missing values for date_min column
2019-09-16 13:06:26,662 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.562465
2019-09-16 13:06:26,663 :: INFO :: checking types : 
2019-09-16 13:06:26,663 :: INFO :: - checking date type for date_min column
2019-09-16 13:06:26,813 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:06:26,971 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.308849
2019-09-16 13:06:26,972 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:06:31,420 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.448484
2019-09-16 13:06:31,420 :: INFO :: checking date validity :
2019-09-16 13:06:31,421 :: INFO :: - checking if date_max column is missing
2019-09-16 13:06:31,421 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000269
2019-09-16 13:06:31,421 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:06:31,499 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.078354
2019-09-16 13:06:31,499 :: INFO :: checking count_min and count_max : 
2019-09-16 13:06:31,500 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000150
2019-09-16 13:06:31,500 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 13:06:31,500 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000008
2019-09-16 13:06:31,500 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:06:31,500 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:06:31,500 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:06:31,500 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:06:31,500 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:06:31,501 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:06:31,536 :: INFO :: * END DATA CLEANING
2019-09-16 13:06:31,536 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:06:31,536 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:06:38,171 :: INFO :: dask df converted in pandas df - time : 0:00:06.634475
2019-09-16 13:06:38,171 :: INFO :: loading dataframe into DB table:
2019-09-16 13:10:30,145 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:10:30,145 :: INFO ::  * Restarting with stat
2019-09-16 13:10:31,709 :: WARNING ::  * Debugger is active!
2019-09-16 13:10:31,709 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:10:36,804 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 13:10:36,804 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 13:10:36,952 :: INFO :: File size = 10.378695487976074 Mo
2019-09-16 13:10:36,986 :: DEBUG :: original file name = data_pf_observado_original.csv
2019-09-16 13:10:36,987 :: INFO :: Upload : file saved in directory - time : 0:00:00.182797
2019-09-16 13:10:36,987 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 13:10:36,987 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 13:10:40,373 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'gn_date', 'heure', 'date_encodage', 'gn_timestamp', 'type_observation', 'nombre', 'gn_min', 'gn_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'lat', 'gn_long', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'gn_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 13:10:40,373 :: DEBUG :: row_count = 26516
2019-09-16 13:10:40,374 :: INFO :: User file validity checked - time : 0:00:03.386506
2019-09-16 13:10:40,374 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 13:10:40,403 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_original_392
2019-09-16 13:10:40,881 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 13:10:41,252 :: INFO :: CSV loaded to DB table - time : 0:00:00.370159
2019-09-16 13:10:41,252 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 13:10:41,252 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 13:10:42,072 :: INFO :: CSV loaded to DB table - time : 0:00:00.820027
2019-09-16 13:10:42,073 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 13:10:42,448 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 13:10:42,452 :: INFO :: Total time to post user file and fill metadata - time : 0:00:05.648780
2019-09-16 13:10:42,456 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:10:42] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 13:10:42,518 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:10:42] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 13:10:51,736 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 13:10:52,025 :: INFO ::  * Restarting with stat
2019-09-16 13:10:54,200 :: WARNING ::  * Debugger is active!
2019-09-16 13:10:54,200 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:10:58,681 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:10:58] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:10:58,712 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:10:58,729 :: DEBUG :: import_id = 392
2019-09-16 13:10:58,730 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:10:58,730 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:10:58,744 :: DEBUG :: row number = 26515
2019-09-16 13:10:58,744 :: INFO :: type of dataframe = dask
2019-09-16 13:10:58,745 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:10:58,767 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:10:58,836 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.091358
2019-09-16 13:10:58,836 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:10:58,836 :: INFO :: * START DATA CLEANING
2019-09-16 13:10:58,906 :: INFO :: checking missing values : 
2019-09-16 13:10:59,016 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:10:59,167 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:10:59,319 :: INFO :: - checking missing values for date_min column
2019-09-16 13:10:59,467 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.560403
2019-09-16 13:10:59,467 :: INFO :: checking types : 
2019-09-16 13:10:59,467 :: INFO :: - checking date type for date_min column
2019-09-16 13:10:59,630 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:10:59,773 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.305736
2019-09-16 13:10:59,773 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:11:03,865 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.092364
2019-09-16 13:11:03,866 :: INFO :: checking date validity :
2019-09-16 13:11:03,866 :: INFO :: - checking if date_max column is missing
2019-09-16 13:11:03,866 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000502
2019-09-16 13:11:03,867 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:11:03,934 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.066865
2019-09-16 13:11:03,934 :: INFO :: checking count_min and count_max : 
2019-09-16 13:11:03,935 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000467
2019-09-16 13:11:03,935 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000022
2019-09-16 13:11:03,935 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000019
2019-09-16 13:11:03,936 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:11:03,936 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:11:03,936 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:11:03,937 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:11:03,937 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:11:03,937 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:11:03,972 :: INFO :: * END DATA CLEANING
2019-09-16 13:11:03,973 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:11:03,973 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:11:10,663 :: INFO :: dask df converted in pandas df - time : 0:00:06.690102
2019-09-16 13:11:10,664 :: INFO :: loading dataframe into DB table:
2019-09-16 13:11:27,518 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 13:11:28,155 :: INFO ::  * Restarting with stat
2019-09-16 13:11:30,775 :: WARNING ::  * Debugger is active!
2019-09-16 13:11:30,775 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:11:31,557 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:11:31] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:11:31,588 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:11:31,603 :: DEBUG :: import_id = 392
2019-09-16 13:11:31,603 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:11:31,603 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:11:31,614 :: DEBUG :: row number = 26515
2019-09-16 13:11:31,615 :: INFO :: type of dataframe = dask
2019-09-16 13:11:31,615 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:11:31,636 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:11:31,704 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.088803
2019-09-16 13:11:31,705 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:11:31,705 :: INFO :: * START DATA CLEANING
2019-09-16 13:11:31,785 :: INFO :: checking missing values : 
2019-09-16 13:11:31,896 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:11:32,039 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:11:32,204 :: INFO :: - checking missing values for date_min column
2019-09-16 13:11:32,367 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.582227
2019-09-16 13:11:32,368 :: INFO :: checking types : 
2019-09-16 13:11:32,368 :: INFO :: - checking date type for date_min column
2019-09-16 13:11:32,519 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:11:32,648 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.279842
2019-09-16 13:11:32,648 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:11:36,668 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.020547
2019-09-16 13:11:36,669 :: INFO :: checking date validity :
2019-09-16 13:11:36,669 :: INFO :: - checking if date_max column is missing
2019-09-16 13:11:36,669 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000417
2019-09-16 13:11:36,669 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:11:36,727 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.058010
2019-09-16 13:11:36,728 :: INFO :: checking count_min and count_max : 
2019-09-16 13:11:36,729 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000551
2019-09-16 13:11:36,729 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000015
2019-09-16 13:11:36,729 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000011
2019-09-16 13:11:36,730 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:11:36,730 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:11:36,730 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:11:36,730 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:11:36,730 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:11:36,730 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:11:36,756 :: INFO :: * END DATA CLEANING
2019-09-16 13:11:36,756 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:11:36,757 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:11:43,005 :: INFO :: dask df converted in pandas df - time : 0:00:06.247585
2019-09-16 13:11:43,005 :: INFO :: loading dataframe into DB table:
2019-09-16 13:14:19,208 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 13:14:20,016 :: INFO ::  * Restarting with stat
2019-09-16 13:14:22,157 :: WARNING ::  * Debugger is active!
2019-09-16 13:14:22,158 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:14:26,346 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:14:26] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:14:26,390 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:14:26,420 :: DEBUG :: import_id = 392
2019-09-16 13:14:26,422 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:14:26,423 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:14:26,445 :: DEBUG :: row number = 26515
2019-09-16 13:14:26,445 :: INFO :: type of dataframe = dask
2019-09-16 13:14:26,445 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:14:26,488 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:14:26,601 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.155474
2019-09-16 13:14:26,602 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:14:26,603 :: INFO :: * START DATA CLEANING
2019-09-16 13:14:26,688 :: INFO :: checking missing values : 
2019-09-16 13:14:26,803 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:14:26,983 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:14:27,132 :: INFO :: - checking missing values for date_min column
2019-09-16 13:14:27,283 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.594744
2019-09-16 13:14:27,283 :: INFO :: checking types : 
2019-09-16 13:14:27,283 :: INFO :: - checking date type for date_min column
2019-09-16 13:14:27,476 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:14:27,655 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.371641
2019-09-16 13:14:27,655 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:14:32,694 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.038262
2019-09-16 13:14:32,694 :: INFO :: checking date validity :
2019-09-16 13:14:32,695 :: INFO :: - checking if date_max column is missing
2019-09-16 13:14:32,695 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000810
2019-09-16 13:14:32,695 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:14:32,769 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.073670
2019-09-16 13:14:32,770 :: INFO :: checking count_min and count_max : 
2019-09-16 13:14:32,770 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000344
2019-09-16 13:14:32,770 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000017
2019-09-16 13:14:32,771 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000014
2019-09-16 13:14:32,771 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:14:32,771 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:14:32,771 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:14:32,771 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:14:32,772 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:14:32,772 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:14:32,812 :: INFO :: * END DATA CLEANING
2019-09-16 13:14:32,812 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:14:32,813 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:14:39,553 :: INFO :: dask df converted in pandas df - time : 0:00:06.740098
2019-09-16 13:14:39,554 :: INFO :: loading dataframe into DB table:
2019-09-16 13:20:12,137 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:20:12,137 :: INFO ::  * Restarting with stat
2019-09-16 13:20:13,704 :: WARNING ::  * Debugger is active!
2019-09-16 13:20:13,704 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:20:19,045 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:20:19] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:20:19,069 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:20:19,081 :: DEBUG :: import_id = 392
2019-09-16 13:20:19,081 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:20:19,081 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:20:19,090 :: DEBUG :: row number = 26515
2019-09-16 13:20:19,090 :: INFO :: type of dataframe = dask
2019-09-16 13:20:19,090 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:20:19,106 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:20:19,155 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.064826
2019-09-16 13:20:19,155 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:20:19,155 :: INFO :: * START DATA CLEANING
2019-09-16 13:20:19,201 :: INFO :: checking missing values : 
2019-09-16 13:20:19,264 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:20:19,377 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:20:19,498 :: INFO :: - checking missing values for date_min column
2019-09-16 13:20:19,625 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.423964
2019-09-16 13:20:19,625 :: INFO :: checking types : 
2019-09-16 13:20:19,625 :: INFO :: - checking date type for date_min column
2019-09-16 13:20:19,745 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:20:19,855 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.230223
2019-09-16 13:20:19,855 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:20:23,743 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.887642
2019-09-16 13:20:23,743 :: INFO :: checking date validity :
2019-09-16 13:20:23,744 :: INFO :: - checking if date_max column is missing
2019-09-16 13:20:23,744 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000500
2019-09-16 13:20:23,744 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:20:23,796 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.052304
2019-09-16 13:20:23,797 :: INFO :: checking count_min and count_max : 
2019-09-16 13:20:23,797 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000113
2019-09-16 13:20:23,797 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 13:20:23,797 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 13:20:23,797 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:20:23,797 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:20:23,797 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:20:23,797 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:20:23,798 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:20:23,798 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:20:23,823 :: INFO :: * END DATA CLEANING
2019-09-16 13:20:23,823 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:20:23,824 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:20:30,261 :: INFO :: dask df converted in pandas df - time : 0:00:06.436949
2019-09-16 13:20:30,261 :: INFO :: loading dataframe into DB table:
2019-09-16 13:21:42,216 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:21:42,216 :: INFO ::  * Restarting with stat
2019-09-16 13:21:43,847 :: WARNING ::  * Debugger is active!
2019-09-16 13:21:43,847 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:21:45,659 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:21:45] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:21:45,688 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:21:45,702 :: DEBUG :: import_id = 392
2019-09-16 13:21:45,703 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:21:45,703 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:21:45,711 :: DEBUG :: row number = 26515
2019-09-16 13:21:45,711 :: INFO :: type of dataframe = dask
2019-09-16 13:21:45,711 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:21:45,728 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:21:45,776 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.064535
2019-09-16 13:21:45,776 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:21:45,776 :: INFO :: * START DATA CLEANING
2019-09-16 13:21:45,831 :: INFO :: checking missing values : 
2019-09-16 13:21:45,918 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:21:46,064 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:21:46,224 :: INFO :: - checking missing values for date_min column
2019-09-16 13:21:46,366 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.535689
2019-09-16 13:21:46,367 :: INFO :: checking types : 
2019-09-16 13:21:46,367 :: INFO :: - checking date type for date_min column
2019-09-16 13:21:46,525 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:21:46,656 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.289568
2019-09-16 13:21:46,656 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:21:50,879 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.223048
2019-09-16 13:21:50,880 :: INFO :: checking date validity :
2019-09-16 13:21:50,880 :: INFO :: - checking if date_max column is missing
2019-09-16 13:21:50,881 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000734
2019-09-16 13:21:50,881 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:21:50,940 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.059363
2019-09-16 13:21:50,941 :: INFO :: checking count_min and count_max : 
2019-09-16 13:21:50,941 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000547
2019-09-16 13:21:50,941 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 13:21:50,941 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 13:21:50,942 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:21:50,942 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:21:50,942 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:21:50,942 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:21:50,942 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:21:50,942 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:21:50,966 :: INFO :: * END DATA CLEANING
2019-09-16 13:21:50,967 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:21:50,967 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 13:21:50,968 :: ERROR :: load() takes 7 positional arguments but 8 were given
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 576, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col, df_type)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
TypeError: load() takes 7 positional arguments but 8 were given
2019-09-16 13:21:50,995 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:21:50] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 13:22:08,585 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:22:08,956 :: INFO ::  * Restarting with stat
2019-09-16 13:22:10,958 :: WARNING ::  * Debugger is active!
2019-09-16 13:22:10,959 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:22:11,812 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:22:11] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:22:11,905 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:22:11,928 :: DEBUG :: import_id = 392
2019-09-16 13:22:11,929 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:22:11,929 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:22:11,941 :: DEBUG :: row number = 26515
2019-09-16 13:22:11,941 :: INFO :: type of dataframe = dask
2019-09-16 13:22:11,941 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:22:11,968 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:22:12,026 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.084195
2019-09-16 13:22:12,026 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:22:12,026 :: INFO :: * START DATA CLEANING
2019-09-16 13:22:12,078 :: INFO :: checking missing values : 
2019-09-16 13:22:12,156 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:22:12,291 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:22:12,438 :: INFO :: - checking missing values for date_min column
2019-09-16 13:22:12,597 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.518460
2019-09-16 13:22:12,597 :: INFO :: checking types : 
2019-09-16 13:22:12,597 :: INFO :: - checking date type for date_min column
2019-09-16 13:22:12,757 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:22:12,897 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.300429
2019-09-16 13:22:12,898 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:22:16,708 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.809841
2019-09-16 13:22:16,708 :: INFO :: checking date validity :
2019-09-16 13:22:16,708 :: INFO :: - checking if date_max column is missing
2019-09-16 13:22:16,709 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000329
2019-09-16 13:22:16,709 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:22:16,762 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.053041
2019-09-16 13:22:16,762 :: INFO :: checking count_min and count_max : 
2019-09-16 13:22:16,762 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000193
2019-09-16 13:22:16,762 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000010
2019-09-16 13:22:16,763 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 13:22:16,763 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:22:16,763 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:22:16,763 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:22:16,763 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:22:16,763 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:22:16,763 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:22:16,788 :: INFO :: * END DATA CLEANING
2019-09-16 13:22:16,789 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:22:16,789 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:22:22,892 :: INFO :: dask df converted in pandas df - time : 0:00:06.103184
2019-09-16 13:22:22,893 :: INFO :: loading dataframe into DB table:
2019-09-16 13:23:10,997 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:23:11,489 :: INFO ::  * Restarting with stat
2019-09-16 13:23:13,794 :: WARNING ::  * Debugger is active!
2019-09-16 13:23:13,795 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:23:16,520 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:23:16] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:23:16,592 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:23:16,617 :: DEBUG :: import_id = 392
2019-09-16 13:23:16,618 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:23:16,618 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:23:16,618 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:23:16,655 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:23:16,748 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.129523
2019-09-16 13:23:16,748 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:23:16,749 :: INFO :: * START DATA CLEANING
2019-09-16 13:23:16,749 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 13:23:16,749 :: ERROR :: data_cleaning() missing 1 required positional argument: 'df_type'
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 558, in postMapping
    transform_errors = data_cleaning(df, selected_columns, MISSING_VALUES, DEFAULT_COUNT_VALUE)
TypeError: data_cleaning() missing 1 required positional argument: 'df_type'
2019-09-16 13:23:16,765 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:23:16] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 13:23:33,538 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 13:23:33,817 :: INFO ::  * Restarting with stat
2019-09-16 13:23:35,511 :: WARNING ::  * Debugger is active!
2019-09-16 13:23:35,511 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:23:38,789 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:23:38] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:23:38,825 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:23:38,839 :: DEBUG :: import_id = 392
2019-09-16 13:23:38,839 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:23:38,839 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:23:38,840 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:23:38,852 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:23:38,905 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.065358
2019-09-16 13:23:38,906 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:23:38,906 :: INFO :: * START DATA CLEANING
2019-09-16 13:23:38,961 :: INFO :: checking missing values : 
2019-09-16 13:23:39,036 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:23:39,160 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:23:39,294 :: INFO :: - checking missing values for date_min column
2019-09-16 13:23:39,428 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.467656
2019-09-16 13:23:39,429 :: INFO :: checking types : 
2019-09-16 13:23:39,429 :: INFO :: - checking date type for date_min column
2019-09-16 13:23:39,564 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:23:39,680 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.251832
2019-09-16 13:23:39,681 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:23:43,511 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.829868
2019-09-16 13:23:43,511 :: INFO :: checking date validity :
2019-09-16 13:23:43,511 :: INFO :: - checking if date_max column is missing
2019-09-16 13:23:43,511 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000331
2019-09-16 13:23:43,511 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:23:43,571 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.060158
2019-09-16 13:23:43,572 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 13:23:43,572 :: ERROR :: name 'df_type' is not defined
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 558, in postMapping
    transform_errors = data_cleaning(df, selected_columns, MISSING_VALUES, DEFAULT_COUNT_VALUE)
  File "/home/ju/geonature/external_modules/import/backend/transform/transform.py", line 40, in data_cleaning
    error_check_counts = check_counts(df, selected_columns, synthese_info, def_count_val, df_type)
NameError: name 'df_type' is not defined
2019-09-16 13:23:43,598 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:23:43] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 13:23:49,841 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 13:23:50,134 :: INFO ::  * Restarting with stat
2019-09-16 13:23:52,028 :: WARNING ::  * Debugger is active!
2019-09-16 13:23:52,029 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:23:58,274 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 13:23:58,561 :: INFO ::  * Restarting with stat
2019-09-16 13:24:00,520 :: WARNING ::  * Debugger is active!
2019-09-16 13:24:00,521 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:24:01,907 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:24:01] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:24:01,939 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:24:01,954 :: DEBUG :: import_id = 392
2019-09-16 13:24:01,954 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:24:01,954 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:24:01,954 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:24:01,970 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:24:02,024 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.069595
2019-09-16 13:24:02,024 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:24:02,024 :: INFO :: * START DATA CLEANING
2019-09-16 13:24:02,075 :: INFO :: checking missing values : 
2019-09-16 13:24:02,162 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:24:02,308 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:24:02,456 :: INFO :: - checking missing values for date_min column
2019-09-16 13:24:02,595 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.519835
2019-09-16 13:24:02,595 :: INFO :: checking types : 
2019-09-16 13:24:02,595 :: INFO :: - checking date type for date_min column
2019-09-16 13:24:02,759 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:24:02,871 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.276698
2019-09-16 13:24:02,872 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:24:07,160 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.288235
2019-09-16 13:24:07,160 :: INFO :: checking date validity :
2019-09-16 13:24:07,161 :: INFO :: - checking if date_max column is missing
2019-09-16 13:24:07,161 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000674
2019-09-16 13:24:07,161 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:24:07,226 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.065448
2019-09-16 13:24:07,227 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 13:24:07,227 :: ERROR :: check_counts() missing 1 required positional argument: 'df_type'
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 558, in postMapping
    transform_errors = data_cleaning(df, selected_columns, MISSING_VALUES, DEFAULT_COUNT_VALUE)
  File "/home/ju/geonature/external_modules/import/backend/transform/transform.py", line 40, in data_cleaning
    error_check_counts = check_counts(df, selected_columns, synthese_info, def_count_val)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
TypeError: check_counts() missing 1 required positional argument: 'df_type'
2019-09-16 13:24:07,257 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:24:07] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 13:24:21,840 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 13:24:22,140 :: INFO ::  * Restarting with stat
2019-09-16 13:24:23,835 :: WARNING ::  * Debugger is active!
2019-09-16 13:24:23,837 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:24:25,709 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:24:25] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:24:25,743 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:24:25,757 :: DEBUG :: import_id = 392
2019-09-16 13:24:25,758 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:24:25,758 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:24:25,758 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:24:25,774 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:24:25,830 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.072374
2019-09-16 13:24:25,831 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:24:25,831 :: INFO :: * START DATA CLEANING
2019-09-16 13:24:25,883 :: INFO :: checking missing values : 
2019-09-16 13:24:25,971 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:24:26,083 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:24:26,213 :: INFO :: - checking missing values for date_min column
2019-09-16 13:24:26,347 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.463469
2019-09-16 13:24:26,347 :: INFO :: checking types : 
2019-09-16 13:24:26,347 :: INFO :: - checking date type for date_min column
2019-09-16 13:24:26,496 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:24:26,614 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.266985
2019-09-16 13:24:26,614 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:24:30,709 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.095184
2019-09-16 13:24:30,710 :: INFO :: checking date validity :
2019-09-16 13:24:30,710 :: INFO :: - checking if date_max column is missing
2019-09-16 13:24:30,710 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000286
2019-09-16 13:24:30,710 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:24:30,766 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.055829
2019-09-16 13:24:30,766 :: INFO :: checking count_min and count_max : 
2019-09-16 13:24:30,768 :: INFO :: Data cleaning : counts checked - time : 0:00:00.001185
2019-09-16 13:24:30,768 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000016
2019-09-16 13:24:30,768 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000012
2019-09-16 13:24:30,768 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:24:30,769 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:24:30,769 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:24:30,769 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:24:30,769 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:24:30,769 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:24:30,792 :: INFO :: * END DATA CLEANING
2019-09-16 13:24:30,792 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:24:30,793 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:24:36,818 :: INFO :: dask df converted in pandas df - time : 0:00:06.024823
2019-09-16 13:24:36,818 :: INFO :: loading dataframe into DB table:
2019-09-16 13:24:38,332 :: INFO :: dask df loaded to db table - time : 0:00:01.512992
2019-09-16 13:24:38,474 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:07.681465
2019-09-16 13:24:38,492 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:24:38,556 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:24:38,615 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:24:38] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:25:31,881 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 13:25:32,241 :: INFO ::  * Restarting with stat
2019-09-16 13:25:34,411 :: WARNING ::  * Debugger is active!
2019-09-16 13:25:34,411 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:25:35,064 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:25:35] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:25:35,102 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:25:35,116 :: DEBUG :: import_id = 392
2019-09-16 13:25:35,116 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:25:35,117 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:25:35,117 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:25:35,132 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:25:35,193 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.075678
2019-09-16 13:25:35,193 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:25:35,193 :: INFO :: * START DATA CLEANING
2019-09-16 13:25:35,248 :: INFO :: checking missing values : 
2019-09-16 13:25:35,324 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:25:35,463 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:25:35,605 :: INFO :: - checking missing values for date_min column
2019-09-16 13:25:35,767 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.518913
2019-09-16 13:25:35,767 :: INFO :: checking types : 
2019-09-16 13:25:35,767 :: INFO :: - checking date type for date_min column
2019-09-16 13:25:35,924 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:25:36,075 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.307854
2019-09-16 13:25:36,075 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:25:39,732 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.657311
2019-09-16 13:25:39,733 :: INFO :: checking date validity :
2019-09-16 13:25:39,733 :: INFO :: - checking if date_max column is missing
2019-09-16 13:25:39,733 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000263
2019-09-16 13:25:39,733 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:25:39,785 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.052176
2019-09-16 13:25:39,785 :: INFO :: checking count_min and count_max : 
2019-09-16 13:25:39,786 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000197
2019-09-16 13:25:39,786 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000019
2019-09-16 13:25:39,786 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000018
2019-09-16 13:25:39,787 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:25:39,787 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:25:39,787 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:25:39,788 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:25:39,788 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:25:39,788 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:25:39,817 :: INFO :: * END DATA CLEANING
2019-09-16 13:25:39,817 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:25:39,817 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:25:45,412 :: INFO :: dask df converted in pandas df - time : 0:00:05.594944
2019-09-16 13:25:45,412 :: INFO :: loading dataframe into DB table:
2019-09-16 13:25:46,771 :: INFO :: dask df loaded to db table - time : 0:00:01.358048
2019-09-16 13:25:46,897 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:07.080208
2019-09-16 13:25:46,927 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:25:46,997 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:25:47,050 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:25:47] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:26:31,093 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 13:26:31,423 :: INFO ::  * Restarting with stat
2019-09-16 13:26:33,415 :: WARNING ::  * Debugger is active!
2019-09-16 13:26:33,415 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:26:52,170 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 13:26:52,464 :: INFO ::  * Restarting with stat
2019-09-16 13:26:55,375 :: WARNING ::  * Debugger is active!
2019-09-16 13:26:55,375 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:26:55,381 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:26:55] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:26:55,404 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:26:55,422 :: DEBUG :: import_id = 392
2019-09-16 13:26:55,422 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:26:55,422 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:26:55,422 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:26:55,442 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:26:55,506 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.083943
2019-09-16 13:26:55,507 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:26:55,507 :: INFO :: * START DATA CLEANING
2019-09-16 13:26:55,578 :: INFO :: checking missing values : 
2019-09-16 13:26:55,685 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:26:55,925 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:26:56,125 :: INFO :: - checking missing values for date_min column
2019-09-16 13:26:56,273 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.695355
2019-09-16 13:26:56,274 :: INFO :: checking types : 
2019-09-16 13:26:56,274 :: INFO :: - checking date type for date_min column
2019-09-16 13:26:56,384 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:26:56,491 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.217611
2019-09-16 13:26:56,491 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:27:00,138 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.646528
2019-09-16 13:27:00,138 :: INFO :: checking date validity :
2019-09-16 13:27:00,139 :: INFO :: - checking if date_max column is missing
2019-09-16 13:27:00,139 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000477
2019-09-16 13:27:00,139 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:27:00,200 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.060822
2019-09-16 13:27:00,200 :: INFO :: checking count_min and count_max : 
2019-09-16 13:27:00,200 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000137
2019-09-16 13:27:00,200 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 13:27:00,201 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 13:27:00,201 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:27:00,201 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:27:00,201 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:27:00,201 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:27:00,201 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:27:00,201 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:27:00,224 :: INFO :: * END DATA CLEANING
2019-09-16 13:27:00,224 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:27:00,225 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:27:06,499 :: INFO :: dask df converted in pandas df - time : 0:00:06.274016
2019-09-16 13:27:06,499 :: INFO :: loading dataframe into DB table:
2019-09-16 13:27:07,871 :: INFO :: dask df loaded to db table - time : 0:00:01.371665
2019-09-16 13:27:08,016 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:07.791876
2019-09-16 13:27:08,035 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:27:08,108 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:27:08,161 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:27:08] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:27:32,420 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 13:27:32,878 :: INFO ::  * Restarting with stat
2019-09-16 13:27:35,086 :: WARNING ::  * Debugger is active!
2019-09-16 13:27:35,086 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:27:45,410 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:27:45,707 :: INFO ::  * Restarting with stat
2019-09-16 13:27:47,738 :: WARNING ::  * Debugger is active!
2019-09-16 13:27:47,738 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:27:49,180 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:27:49] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:27:49,220 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:27:49,235 :: DEBUG :: import_id = 392
2019-09-16 13:27:49,235 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:27:49,236 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:27:49,236 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:27:49,252 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:27:49,303 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.067508
2019-09-16 13:27:49,304 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:27:49,304 :: INFO :: * START DATA CLEANING
2019-09-16 13:27:49,358 :: INFO :: checking missing values : 
2019-09-16 13:27:49,444 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:27:49,564 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:27:49,681 :: INFO :: - checking missing values for date_min column
2019-09-16 13:27:49,810 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.451097
2019-09-16 13:27:49,811 :: INFO :: checking types : 
2019-09-16 13:27:49,811 :: INFO :: - checking date type for date_min column
2019-09-16 13:27:49,956 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:27:50,078 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.266761
2019-09-16 13:27:50,078 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:27:53,725 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.647403
2019-09-16 13:27:53,726 :: INFO :: checking date validity :
2019-09-16 13:27:53,727 :: INFO :: - checking if date_max column is missing
2019-09-16 13:27:53,727 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000601
2019-09-16 13:27:53,727 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:27:53,781 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.054379
2019-09-16 13:27:53,782 :: INFO :: checking count_min and count_max : 
2019-09-16 13:27:53,782 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000146
2019-09-16 13:27:53,782 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 13:27:53,782 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000005
2019-09-16 13:27:53,782 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:27:53,782 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:27:53,782 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:27:53,782 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:27:53,783 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:27:53,783 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:27:53,803 :: INFO :: * END DATA CLEANING
2019-09-16 13:27:53,804 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:27:53,804 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:27:59,496 :: INFO :: dask df converted in pandas df - time : 0:00:05.691852
2019-09-16 13:27:59,496 :: INFO :: loading dataframe into DB table:
2019-09-16 13:28:01,098 :: INFO :: dask df loaded to db table - time : 0:00:01.601384
2019-09-16 13:28:01,244 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:07.440445
2019-09-16 13:28:01,275 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:28:01,348 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:28:01,403 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:28:01] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:28:14,227 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:28:14,722 :: INFO ::  * Restarting with stat
2019-09-16 13:28:16,558 :: WARNING ::  * Debugger is active!
2019-09-16 13:28:16,558 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:28:37,374 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:28:37,683 :: INFO ::  * Restarting with stat
2019-09-16 13:28:39,318 :: WARNING ::  * Debugger is active!
2019-09-16 13:28:39,319 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:28:42,354 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:28:42] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:28:42,382 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:28:42,397 :: DEBUG :: import_id = 392
2019-09-16 13:28:42,397 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:28:42,398 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:28:42,409 :: DEBUG :: row number = 26515
2019-09-16 13:28:42,409 :: INFO :: type of dataframe = dask
2019-09-16 13:28:42,409 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:28:42,426 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:28:42,484 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.074058
2019-09-16 13:28:42,485 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:28:42,485 :: INFO :: * START DATA CLEANING
2019-09-16 13:28:42,551 :: INFO :: checking missing values : 
2019-09-16 13:28:42,645 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:28:42,778 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:28:42,920 :: INFO :: - checking missing values for date_min column
2019-09-16 13:28:43,057 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.506012
2019-09-16 13:28:43,057 :: INFO :: checking types : 
2019-09-16 13:28:43,057 :: INFO :: - checking date type for date_min column
2019-09-16 13:28:43,189 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:28:43,295 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.238694
2019-09-16 13:28:43,296 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:28:46,812 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.516821
2019-09-16 13:28:46,813 :: INFO :: checking date validity :
2019-09-16 13:28:46,813 :: INFO :: - checking if date_max column is missing
2019-09-16 13:28:46,813 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000234
2019-09-16 13:28:46,813 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:28:46,863 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.049683
2019-09-16 13:28:46,863 :: INFO :: checking count_min and count_max : 
2019-09-16 13:28:46,863 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000135
2019-09-16 13:28:46,863 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 13:28:46,863 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000009
2019-09-16 13:28:46,864 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:28:46,864 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:28:46,864 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:28:46,865 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:28:46,865 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:28:46,865 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:28:46,890 :: INFO :: * END DATA CLEANING
2019-09-16 13:28:46,891 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:28:46,891 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:28:53,220 :: INFO :: dask df converted in pandas df - time : 0:00:06.328827
2019-09-16 13:28:53,220 :: INFO :: loading dataframe into DB table:
2019-09-16 13:29:23,724 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:29:24,341 :: INFO ::  * Restarting with stat
2019-09-16 13:29:26,100 :: WARNING ::  * Debugger is active!
2019-09-16 13:29:26,100 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:29:29,353 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:29:29] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:29:29,387 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:29:29,402 :: DEBUG :: import_id = 392
2019-09-16 13:29:29,402 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:29:29,403 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:29:29,411 :: DEBUG :: row number = 26515
2019-09-16 13:29:29,411 :: INFO :: type of dataframe = dask
2019-09-16 13:29:29,412 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:29:29,429 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:29:29,483 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.071502
2019-09-16 13:29:29,484 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:30:09,900 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:30:10,285 :: INFO ::  * Restarting with stat
2019-09-16 13:30:12,169 :: WARNING ::  * Debugger is active!
2019-09-16 13:30:12,169 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:30:22,527 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:30:22,812 :: INFO ::  * Restarting with stat
2019-09-16 13:30:24,688 :: WARNING ::  * Debugger is active!
2019-09-16 13:30:24,688 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:30:28,105 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:30:28] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:30:28,137 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:30:28,153 :: DEBUG :: import_id = 392
2019-09-16 13:30:28,154 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:30:28,154 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:30:28,163 :: DEBUG :: row number = 26515
2019-09-16 13:30:28,164 :: INFO :: type of dataframe = dask
2019-09-16 13:30:28,164 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:30:28,181 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:30:28,235 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.070908
2019-09-16 13:30:28,235 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:30:28,235 :: INFO :: * START DATA CLEANING
2019-09-16 13:30:28,306 :: INFO :: checking missing values : 
2019-09-16 13:30:28,411 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:30:28,559 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:30:28,689 :: INFO :: - checking missing values for date_min column
2019-09-16 13:30:28,839 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.532615
2019-09-16 13:30:28,839 :: INFO :: checking types : 
2019-09-16 13:30:28,840 :: INFO :: - checking date type for date_min column
2019-09-16 13:30:28,984 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:30:29,149 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.309238
2019-09-16 13:30:29,149 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:30:32,905 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.755672
2019-09-16 13:30:32,905 :: INFO :: checking date validity :
2019-09-16 13:30:32,906 :: INFO :: - checking if date_max column is missing
2019-09-16 13:30:32,907 :: INFO :: Data cleaning : dates checked - time : 0:00:00.001273
2019-09-16 13:30:32,907 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:30:32,959 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.051702
2019-09-16 13:30:32,959 :: INFO :: checking count_min and count_max : 
2019-09-16 13:30:32,959 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000130
2019-09-16 13:30:32,959 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 13:30:32,959 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 13:30:32,960 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:30:32,960 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:30:32,960 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:30:32,960 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:30:32,960 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:30:32,960 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:30:32,984 :: INFO :: * END DATA CLEANING
2019-09-16 13:30:32,984 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:30:32,984 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:30:38,699 :: INFO :: dask df converted in pandas df - time : 0:00:05.715212
2019-09-16 13:30:38,699 :: INFO :: loading dataframe into DB table:
2019-09-16 13:31:05,080 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:31:05,701 :: INFO ::  * Restarting with stat
2019-09-16 13:31:07,512 :: WARNING ::  * Debugger is active!
2019-09-16 13:31:07,512 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:31:09,586 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:31:09,896 :: INFO ::  * Restarting with stat
2019-09-16 13:31:11,669 :: WARNING ::  * Debugger is active!
2019-09-16 13:31:11,669 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:31:15,519 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:31:15] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:31:15,544 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:31:15,561 :: DEBUG :: import_id = 392
2019-09-16 13:31:15,562 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:31:15,562 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:31:15,574 :: DEBUG :: row number = 26515
2019-09-16 13:31:15,574 :: INFO :: type of dataframe = dask
2019-09-16 13:31:15,574 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:31:15,591 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:31:15,645 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.070409
2019-09-16 13:31:15,645 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:31:15,645 :: INFO :: * START DATA CLEANING
2019-09-16 13:31:15,702 :: INFO :: checking missing values : 
2019-09-16 13:31:15,784 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:31:15,930 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:31:16,071 :: INFO :: - checking missing values for date_min column
2019-09-16 13:31:16,208 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.505538
2019-09-16 13:31:16,208 :: INFO :: checking types : 
2019-09-16 13:31:16,208 :: INFO :: - checking date type for date_min column
2019-09-16 13:31:16,340 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:31:16,441 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.232503
2019-09-16 13:31:16,441 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:31:20,417 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.976604
2019-09-16 13:31:20,418 :: INFO :: checking date validity :
2019-09-16 13:31:20,418 :: INFO :: - checking if date_max column is missing
2019-09-16 13:31:20,418 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000324
2019-09-16 13:31:20,418 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:31:20,471 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.053096
2019-09-16 13:31:20,472 :: INFO :: checking count_min and count_max : 
2019-09-16 13:31:20,472 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000187
2019-09-16 13:31:20,472 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000008
2019-09-16 13:31:20,472 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000009
2019-09-16 13:31:20,472 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:31:20,473 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:31:20,473 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:31:20,473 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:31:20,473 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:31:20,473 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:31:20,498 :: INFO :: * END DATA CLEANING
2019-09-16 13:31:20,499 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:31:20,499 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:31:26,860 :: INFO :: dask df converted in pandas df - time : 0:00:06.361419
2019-09-16 13:31:26,861 :: INFO :: loading dataframe into DB table:
2019-09-16 13:31:44,735 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:31:45,346 :: INFO ::  * Restarting with stat
2019-09-16 13:31:47,142 :: WARNING ::  * Debugger is active!
2019-09-16 13:31:47,142 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:31:53,736 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:31:53] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:31:53,765 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:31:53,778 :: DEBUG :: import_id = 392
2019-09-16 13:31:53,779 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:31:53,779 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:31:53,779 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:31:53,798 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:31:53,855 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.076251
2019-09-16 13:31:53,855 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:31:53,856 :: INFO :: * START DATA CLEANING
2019-09-16 13:31:53,911 :: INFO :: checking missing values : 
2019-09-16 13:31:53,991 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:31:54,131 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:31:54,255 :: INFO :: - checking missing values for date_min column
2019-09-16 13:31:54,381 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.469942
2019-09-16 13:31:54,381 :: INFO :: checking types : 
2019-09-16 13:31:54,381 :: INFO :: - checking date type for date_min column
2019-09-16 13:31:54,507 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:31:54,622 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.241158
2019-09-16 13:31:54,623 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:31:58,414 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.791391
2019-09-16 13:31:58,414 :: INFO :: checking date validity :
2019-09-16 13:31:58,415 :: INFO :: - checking if date_max column is missing
2019-09-16 13:31:58,415 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000410
2019-09-16 13:31:58,415 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:31:58,470 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.054800
2019-09-16 13:31:58,470 :: INFO :: checking count_min and count_max : 
2019-09-16 13:31:58,470 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000248
2019-09-16 13:31:58,471 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000010
2019-09-16 13:31:58,471 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000011
2019-09-16 13:31:58,471 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:31:58,471 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:31:58,471 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:31:58,472 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:31:58,472 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:31:58,472 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:31:58,496 :: INFO :: * END DATA CLEANING
2019-09-16 13:31:58,496 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:31:58,496 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:32:04,453 :: INFO :: dask df converted in pandas df - time : 0:00:05.956279
2019-09-16 13:32:04,453 :: INFO :: loading dataframe into DB table:
2019-09-16 13:32:05,841 :: INFO :: dask df loaded to db table - time : 0:00:01.387799
2019-09-16 13:32:05,980 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:07.484181
2019-09-16 13:32:06,001 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:32:06,062 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:32:06,118 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:32:06] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:32:39,773 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:32:40,133 :: INFO ::  * Restarting with stat
2019-09-16 13:32:41,769 :: WARNING ::  * Debugger is active!
2019-09-16 13:32:41,769 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:32:45,663 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:32:45] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:32:45,701 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:32:45,719 :: DEBUG :: import_id = 392
2019-09-16 13:32:45,720 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:32:45,720 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:32:57,075 :: DEBUG :: row number = 26515
2019-09-16 13:33:12,081 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:33:12,765 :: INFO ::  * Restarting with stat
2019-09-16 13:33:18,351 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:33:18,351 :: INFO ::  * Restarting with stat
2019-09-16 13:33:20,962 :: WARNING ::  * Debugger is active!
2019-09-16 13:33:20,963 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:33:20,969 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:33:20] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:33:20,987 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:33:21,003 :: DEBUG :: import_id = 392
2019-09-16 13:33:21,004 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:33:21,004 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:33:30,716 :: DEBUG :: row number = 26515
2019-09-16 13:34:07,991 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:34:07,992 :: INFO ::  * Restarting with stat
2019-09-16 13:34:09,954 :: WARNING ::  * Debugger is active!
2019-09-16 13:34:09,955 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:34:09,959 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:34:09] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:34:09,981 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:34:09,993 :: DEBUG :: import_id = 392
2019-09-16 13:34:09,994 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:34:09,994 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:34:10,005 :: DEBUG :: row number = 26515
2019-09-16 13:34:10,005 :: INFO :: type of dataframe = dask
2019-09-16 13:34:10,005 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:34:10,022 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:34:10,074 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.068534
2019-09-16 13:34:10,074 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:34:10,074 :: INFO :: * START DATA CLEANING
2019-09-16 13:34:10,121 :: INFO :: checking missing values : 
2019-09-16 13:34:10,196 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:34:10,310 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:34:10,441 :: INFO :: - checking missing values for date_min column
2019-09-16 13:34:10,572 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.450584
2019-09-16 13:34:10,572 :: INFO :: checking types : 
2019-09-16 13:34:10,572 :: INFO :: - checking date type for date_min column
2019-09-16 13:34:10,685 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:34:10,790 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.217819
2019-09-16 13:34:10,790 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:34:14,659 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.869461
2019-09-16 13:34:14,660 :: INFO :: checking date validity :
2019-09-16 13:34:14,660 :: INFO :: - checking if date_max column is missing
2019-09-16 13:34:14,661 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000837
2019-09-16 13:34:14,661 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:34:14,708 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.047166
2019-09-16 13:34:14,708 :: INFO :: checking count_min and count_max : 
2019-09-16 13:34:14,709 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000314
2019-09-16 13:34:14,709 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000016
2019-09-16 13:34:14,709 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000016
2019-09-16 13:34:14,710 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:34:14,710 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:34:14,710 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:34:14,711 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:34:14,711 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:34:14,711 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:34:14,735 :: INFO :: * END DATA CLEANING
2019-09-16 13:34:14,735 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:34:14,735 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:34:20,641 :: INFO :: dask df converted in pandas df - time : 0:00:05.905744
2019-09-16 13:34:20,642 :: INFO :: loading dataframe into DB table:
2019-09-16 13:34:36,354 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:34:37,033 :: INFO ::  * Restarting with stat
2019-09-16 13:34:39,083 :: WARNING ::  * Debugger is active!
2019-09-16 13:34:39,084 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:34:40,511 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:34:40] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:34:40,543 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:34:40,555 :: DEBUG :: import_id = 392
2019-09-16 13:34:40,555 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:34:40,556 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:34:40,556 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:34:40,572 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:34:40,627 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.070913
2019-09-16 13:34:40,627 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:34:40,627 :: INFO :: * START DATA CLEANING
2019-09-16 13:34:40,681 :: INFO :: checking missing values : 
2019-09-16 13:34:40,775 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:34:40,904 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:34:41,024 :: INFO :: - checking missing values for date_min column
2019-09-16 13:34:41,162 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.481265
2019-09-16 13:34:41,162 :: INFO :: checking types : 
2019-09-16 13:34:41,163 :: INFO :: - checking date type for date_min column
2019-09-16 13:34:41,302 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:34:41,434 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.272104
2019-09-16 13:34:41,435 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:34:45,178 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.743555
2019-09-16 13:34:45,179 :: INFO :: checking date validity :
2019-09-16 13:34:45,179 :: INFO :: - checking if date_max column is missing
2019-09-16 13:34:45,179 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000558
2019-09-16 13:34:45,180 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:34:45,231 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.051056
2019-09-16 13:34:45,231 :: INFO :: checking count_min and count_max : 
2019-09-16 13:34:45,231 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000243
2019-09-16 13:34:45,232 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000011
2019-09-16 13:34:45,232 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000008
2019-09-16 13:34:45,232 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:34:45,232 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:34:45,232 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:34:45,232 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:34:45,233 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:34:45,233 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:34:45,252 :: INFO :: * END DATA CLEANING
2019-09-16 13:34:45,253 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:34:45,253 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:34:50,941 :: INFO :: dask df converted in pandas df - time : 0:00:05.687887
2019-09-16 13:34:50,942 :: INFO :: loading dataframe into DB table:
2019-09-16 13:34:52,299 :: INFO :: dask df loaded to db table - time : 0:00:01.356026
2019-09-16 13:34:52,513 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:07.260483
2019-09-16 13:34:52,532 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:34:52,586 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:34:52,646 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:34:52] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:35:12,874 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:35:13,352 :: INFO ::  * Restarting with stat
2019-09-16 13:35:15,577 :: WARNING ::  * Debugger is active!
2019-09-16 13:35:15,577 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:35:15,581 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:35:15] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:35:15,601 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:35:15,617 :: DEBUG :: import_id = 392
2019-09-16 13:35:15,618 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:35:15,618 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:35:15,631 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:35:15,647 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:35:15,699 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.067312
2019-09-16 13:35:15,699 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:35:15,699 :: INFO :: * START DATA CLEANING
2019-09-16 13:35:15,755 :: INFO :: checking missing values : 
2019-09-16 13:35:15,830 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:35:15,962 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:35:16,084 :: INFO :: - checking missing values for date_min column
2019-09-16 13:35:16,221 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.466094
2019-09-16 13:35:16,221 :: INFO :: checking types : 
2019-09-16 13:35:16,221 :: INFO :: - checking date type for date_min column
2019-09-16 13:35:16,400 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:35:16,587 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.365817
2019-09-16 13:35:16,587 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:35:20,121 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.533650
2019-09-16 13:35:20,121 :: INFO :: checking date validity :
2019-09-16 13:35:20,121 :: INFO :: - checking if date_max column is missing
2019-09-16 13:35:20,122 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000346
2019-09-16 13:35:20,122 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:35:20,175 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.053600
2019-09-16 13:35:20,176 :: INFO :: checking count_min and count_max : 
2019-09-16 13:35:20,176 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000176
2019-09-16 13:35:20,176 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 13:35:20,176 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 13:35:20,176 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:35:20,176 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:35:20,176 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:35:20,176 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:35:20,177 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:35:20,177 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:35:20,200 :: INFO :: * END DATA CLEANING
2019-09-16 13:35:20,201 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:35:20,201 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:35:25,585 :: INFO :: dask df converted in pandas df - time : 0:00:05.383169
2019-09-16 13:35:25,585 :: INFO :: loading dataframe into DB table:
2019-09-16 13:36:27,687 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:36:28,380 :: INFO ::  * Restarting with stat
2019-09-16 13:36:30,724 :: WARNING ::  * Debugger is active!
2019-09-16 13:36:30,725 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:36:30,733 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:36:30] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:36:30,754 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:36:30,767 :: DEBUG :: import_id = 392
2019-09-16 13:36:30,768 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:36:30,768 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:36:30,768 :: DEBUG :: row number = 26515
2019-09-16 13:36:30,768 :: INFO :: type of dataframe = dask
2019-09-16 13:36:30,768 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:36:30,786 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:36:30,845 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.076934
2019-09-16 13:36:30,846 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:36:30,846 :: INFO :: * START DATA CLEANING
2019-09-16 13:36:30,902 :: INFO :: checking missing values : 
2019-09-16 13:36:30,998 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:36:31,148 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:36:31,291 :: INFO :: - checking missing values for date_min column
2019-09-16 13:36:31,470 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.568560
2019-09-16 13:36:31,470 :: INFO :: checking types : 
2019-09-16 13:36:31,470 :: INFO :: - checking date type for date_min column
2019-09-16 13:36:31,647 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:36:31,770 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.300037
2019-09-16 13:36:31,771 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:36:35,448 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.676881
2019-09-16 13:36:35,448 :: INFO :: checking date validity :
2019-09-16 13:36:35,448 :: INFO :: - checking if date_max column is missing
2019-09-16 13:36:35,448 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000279
2019-09-16 13:36:35,449 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:36:35,500 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.051694
2019-09-16 13:36:35,500 :: INFO :: checking count_min and count_max : 
2019-09-16 13:36:35,501 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000133
2019-09-16 13:36:35,501 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000005
2019-09-16 13:36:35,501 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000007
2019-09-16 13:36:35,501 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:36:35,501 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:36:35,501 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:36:35,501 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:36:35,502 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:36:35,502 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:36:35,526 :: INFO :: * END DATA CLEANING
2019-09-16 13:36:35,527 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:36:35,527 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:36:41,327 :: INFO :: dask df converted in pandas df - time : 0:00:05.800129
2019-09-16 13:36:41,328 :: INFO :: loading dataframe into DB table:
2019-09-16 13:36:42,598 :: INFO :: dask df loaded to db table - time : 0:00:01.270347
2019-09-16 13:36:42,734 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:07.206741
2019-09-16 13:36:42,756 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:36:42,846 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:36:42,884 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:36:42] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:36:49,950 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:36:49,950 :: INFO ::  * Restarting with stat
2019-09-16 13:36:51,623 :: WARNING ::  * Debugger is active!
2019-09-16 13:36:51,624 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:37:06,180 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:37:06,616 :: INFO ::  * Restarting with stat
2019-09-16 13:37:11,603 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:37:11,604 :: INFO ::  * Restarting with stat
2019-09-16 13:37:13,523 :: WARNING ::  * Debugger is active!
2019-09-16 13:37:13,524 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:37:16,574 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:37:16] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:37:16,604 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:37:16,617 :: DEBUG :: import_id = 392
2019-09-16 13:37:16,617 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:37:16,617 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:39:49,201 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:39:49,894 :: INFO ::  * Restarting with stat
2019-09-16 13:39:52,336 :: WARNING ::  * Debugger is active!
2019-09-16 13:39:52,336 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:39:53,379 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:39:53,663 :: INFO ::  * Restarting with stat
2019-09-16 13:39:59,715 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:39:59,715 :: INFO ::  * Restarting with stat
2019-09-16 13:40:01,867 :: WARNING ::  * Debugger is active!
2019-09-16 13:40:01,868 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:40:03,745 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:40:03] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:40:03,877 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:40:03,895 :: DEBUG :: import_id = 392
2019-09-16 13:40:03,895 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:40:03,895 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:43:02,023 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:43:02,600 :: INFO ::  * Restarting with stat
2019-09-16 13:43:04,566 :: WARNING ::  * Debugger is active!
2019-09-16 13:43:04,566 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:43:05,596 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:43:05,884 :: INFO ::  * Restarting with stat
2019-09-16 13:43:07,912 :: WARNING ::  * Debugger is active!
2019-09-16 13:43:07,912 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:43:17,453 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:43:17] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:43:17,479 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:43:17,494 :: DEBUG :: import_id = 392
2019-09-16 13:43:17,494 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:43:17,494 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:43:17,503 :: DEBUG :: row number = 26515
2019-09-16 13:43:17,503 :: INFO :: type of dataframe = dask
2019-09-16 13:43:17,503 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:43:17,527 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:43:17,597 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.093341
2019-09-16 13:43:17,597 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:43:17,597 :: INFO :: * START DATA CLEANING
2019-09-16 13:43:17,677 :: INFO :: checking missing values : 
2019-09-16 13:43:17,820 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:43:17,997 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:43:18,155 :: INFO :: - checking missing values for date_min column
2019-09-16 13:43:18,311 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.634766
2019-09-16 13:43:18,312 :: INFO :: checking types : 
2019-09-16 13:43:18,312 :: INFO :: - checking date type for date_min column
2019-09-16 13:43:18,495 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:43:18,632 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.320213
2019-09-16 13:43:18,632 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:43:23,815 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.182819
2019-09-16 13:43:23,815 :: INFO :: checking date validity :
2019-09-16 13:43:23,815 :: INFO :: - checking if date_max column is missing
2019-09-16 13:43:23,816 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000256
2019-09-16 13:43:23,816 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:43:23,877 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.060998
2019-09-16 13:43:23,877 :: INFO :: checking count_min and count_max : 
2019-09-16 13:43:23,877 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000165
2019-09-16 13:43:23,877 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000010
2019-09-16 13:43:23,877 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000010
2019-09-16 13:43:23,878 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:43:23,878 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:43:23,878 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:43:23,878 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:43:23,878 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:43:23,878 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:43:23,907 :: INFO :: * END DATA CLEANING
2019-09-16 13:43:23,908 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:43:23,908 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:43:30,679 :: INFO :: dask df converted in pandas df - time : 0:00:06.770927
2019-09-16 13:43:30,680 :: INFO :: loading dataframe into DB table:
2019-09-16 13:43:46,538 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:43:47,557 :: INFO ::  * Restarting with stat
2019-09-16 13:43:49,680 :: WARNING ::  * Debugger is active!
2019-09-16 13:43:49,680 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:43:52,308 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:43:52] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:43:52,353 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:43:52,373 :: DEBUG :: import_id = 392
2019-09-16 13:43:52,373 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:43:52,373 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:43:52,379 :: DEBUG :: row number = 26515
2019-09-16 13:43:52,379 :: INFO :: type of dataframe = dask
2019-09-16 13:43:52,380 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:43:52,404 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:43:52,504 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.123732
2019-09-16 13:43:52,505 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:43:52,505 :: INFO :: * START DATA CLEANING
2019-09-16 13:43:52,590 :: INFO :: checking missing values : 
2019-09-16 13:43:52,729 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:43:52,934 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:43:53,110 :: INFO :: - checking missing values for date_min column
2019-09-16 13:43:53,260 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.670710
2019-09-16 13:43:53,261 :: INFO :: checking types : 
2019-09-16 13:43:53,261 :: INFO :: - checking date type for date_min column
2019-09-16 13:43:53,416 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:43:53,552 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.290870
2019-09-16 13:43:53,552 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:43:57,549 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.996883
2019-09-16 13:43:57,549 :: INFO :: checking date validity :
2019-09-16 13:43:57,550 :: INFO :: - checking if date_max column is missing
2019-09-16 13:43:57,550 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000343
2019-09-16 13:43:57,550 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:43:57,611 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.060792
2019-09-16 13:43:57,611 :: INFO :: checking count_min and count_max : 
2019-09-16 13:43:57,612 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000275
2019-09-16 13:43:57,612 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000010
2019-09-16 13:43:57,612 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000009
2019-09-16 13:43:57,612 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:43:57,612 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:43:57,613 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:43:57,613 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:43:57,613 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:43:57,613 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:43:57,643 :: INFO :: * END DATA CLEANING
2019-09-16 13:43:57,644 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:43:57,644 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:44:04,340 :: INFO :: dask df converted in pandas df - time : 0:00:06.695506
2019-09-16 13:44:04,340 :: INFO :: loading dataframe into DB table:
2019-09-16 13:44:05,877 :: INFO :: dask df loaded to db table - time : 0:00:01.537188
2019-09-16 13:44:06,691 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:09.046850
2019-09-16 13:44:06,716 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:44:06,796 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:44:06,866 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:44:06] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:44:14,800 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:44:15,114 :: INFO ::  * Restarting with stat
2019-09-16 13:44:16,832 :: WARNING ::  * Debugger is active!
2019-09-16 13:44:16,832 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:44:19,961 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:44:19] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:44:19,985 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:44:20,006 :: DEBUG :: import_id = 392
2019-09-16 13:44:20,007 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:44:20,008 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:44:20,021 :: DEBUG :: row number = 26515
2019-09-16 13:44:20,021 :: INFO :: type of dataframe = dask
2019-09-16 13:44:20,021 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:44:20,048 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:44:20,140 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.118400
2019-09-16 13:44:20,141 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:44:20,141 :: INFO :: * START DATA CLEANING
2019-09-16 13:44:20,215 :: INFO :: checking missing values : 
2019-09-16 13:44:20,331 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:44:20,557 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:44:20,713 :: INFO :: - checking missing values for date_min column
2019-09-16 13:44:20,865 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.649193
2019-09-16 13:44:20,865 :: INFO :: checking types : 
2019-09-16 13:44:20,865 :: INFO :: - checking date type for date_min column
2019-09-16 13:44:21,027 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:44:21,192 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.327398
2019-09-16 13:44:21,193 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:44:25,244 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.051381
2019-09-16 13:44:25,244 :: INFO :: checking date validity :
2019-09-16 13:44:25,245 :: INFO :: - checking if date_max column is missing
2019-09-16 13:44:25,245 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000817
2019-09-16 13:44:25,245 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:44:25,327 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.082107
2019-09-16 13:44:25,328 :: INFO :: checking count_min and count_max : 
2019-09-16 13:44:25,328 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000226
2019-09-16 13:44:25,328 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000008
2019-09-16 13:44:25,329 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000008
2019-09-16 13:44:25,329 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:44:25,329 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:44:25,329 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:44:25,329 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:44:25,329 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:44:25,329 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:44:25,364 :: INFO :: * END DATA CLEANING
2019-09-16 13:44:25,364 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:44:25,365 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:44:32,142 :: INFO :: dask df converted in pandas df - time : 0:00:06.776990
2019-09-16 13:44:32,142 :: INFO :: loading dataframe into DB table:
2019-09-16 13:46:10,424 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:46:10,801 :: INFO ::  * Restarting with stat
2019-09-16 13:46:12,995 :: WARNING ::  * Debugger is active!
2019-09-16 13:46:12,995 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:46:15,234 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:46:15] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:46:15,269 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:46:15,284 :: DEBUG :: import_id = 392
2019-09-16 13:46:15,284 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:46:15,284 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:46:15,295 :: DEBUG :: row number = 26515
2019-09-16 13:46:15,295 :: INFO :: type of dataframe = dask
2019-09-16 13:46:15,295 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:46:15,315 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:46:15,398 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.102358
2019-09-16 13:46:15,398 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:46:15,398 :: INFO :: * START DATA CLEANING
2019-09-16 13:46:15,480 :: INFO :: checking missing values : 
2019-09-16 13:46:15,616 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:46:15,803 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:46:15,972 :: INFO :: - checking missing values for date_min column
2019-09-16 13:46:16,115 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.634155
2019-09-16 13:46:16,115 :: INFO :: checking types : 
2019-09-16 13:46:16,115 :: INFO :: - checking date type for date_min column
2019-09-16 13:46:16,280 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:46:16,430 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.315092
2019-09-16 13:46:16,430 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:46:20,444 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.013318
2019-09-16 13:46:20,444 :: INFO :: checking date validity :
2019-09-16 13:46:20,444 :: INFO :: - checking if date_max column is missing
2019-09-16 13:46:20,445 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000381
2019-09-16 13:46:20,445 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:46:20,526 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.081384
2019-09-16 13:46:20,526 :: INFO :: checking count_min and count_max : 
2019-09-16 13:46:20,527 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000225
2019-09-16 13:46:20,527 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000010
2019-09-16 13:46:20,527 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000011
2019-09-16 13:46:20,527 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:46:20,528 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:46:20,528 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:46:20,528 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:46:20,529 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:46:20,529 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:46:20,568 :: INFO :: * END DATA CLEANING
2019-09-16 13:46:20,569 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:46:20,569 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:46:27,265 :: INFO :: dask df converted in pandas df - time : 0:00:06.695255
2019-09-16 13:46:27,266 :: INFO :: loading dataframe into DB table:
2019-09-16 13:49:12,333 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 13:49:12,333 :: INFO ::  * Restarting with stat
2019-09-16 13:49:14,226 :: WARNING ::  * Debugger is active!
2019-09-16 13:49:14,227 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:49:15,046 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:49:15] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:49:15,072 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:49:15,088 :: DEBUG :: import_id = 392
2019-09-16 13:49:15,089 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:49:15,089 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:49:35,275 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:49:35,955 :: INFO ::  * Restarting with stat
2019-09-16 13:49:38,143 :: WARNING ::  * Debugger is active!
2019-09-16 13:49:38,143 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:49:40,254 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:49:40] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:49:40,283 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:49:40,299 :: DEBUG :: import_id = 392
2019-09-16 13:49:40,299 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:49:40,300 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:49:46,629 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:49:47,136 :: INFO ::  * Restarting with stat
2019-09-16 13:49:49,312 :: WARNING ::  * Debugger is active!
2019-09-16 13:49:49,312 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:49:51,131 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:49:51] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:49:51,159 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:49:51,173 :: DEBUG :: import_id = 392
2019-09-16 13:49:51,173 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:49:51,174 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:49:51,184 :: DEBUG :: row number = 26515
2019-09-16 13:49:51,184 :: INFO :: type of dataframe = dask
2019-09-16 13:49:51,184 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:49:51,206 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:49:51,282 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.098095
2019-09-16 13:49:51,283 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:49:51,283 :: INFO :: * START DATA CLEANING
2019-09-16 13:49:51,378 :: INFO :: checking missing values : 
2019-09-16 13:49:51,519 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:49:51,718 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:49:51,912 :: INFO :: - checking missing values for date_min column
2019-09-16 13:49:52,101 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.722505
2019-09-16 13:49:52,101 :: INFO :: checking types : 
2019-09-16 13:49:52,101 :: INFO :: - checking date type for date_min column
2019-09-16 13:49:52,328 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:49:52,507 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.406277
2019-09-16 13:49:52,508 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:49:56,549 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.041670
2019-09-16 13:49:56,550 :: INFO :: checking date validity :
2019-09-16 13:49:56,550 :: INFO :: - checking if date_max column is missing
2019-09-16 13:49:56,550 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000351
2019-09-16 13:49:56,550 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:49:56,613 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.062416
2019-09-16 13:49:56,613 :: INFO :: checking count_min and count_max : 
2019-09-16 13:49:56,613 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000173
2019-09-16 13:49:56,613 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000010
2019-09-16 13:49:56,613 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000010
2019-09-16 13:49:56,614 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:49:56,614 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:49:56,614 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:49:56,614 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:49:56,614 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:49:56,614 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:49:56,644 :: INFO :: * END DATA CLEANING
2019-09-16 13:49:56,644 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:49:56,644 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:50:03,448 :: INFO :: dask df converted in pandas df - time : 0:00:06.803082
2019-09-16 13:50:03,448 :: INFO :: loading dataframe into DB table:
2019-09-16 13:55:00,436 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:55:00,784 :: INFO ::  * Restarting with stat
2019-09-16 13:55:02,594 :: WARNING ::  * Debugger is active!
2019-09-16 13:55:02,595 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:55:07,648 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:55:07] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:55:07,682 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:55:07,706 :: DEBUG :: import_id = 392
2019-09-16 13:55:07,707 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:55:07,707 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:55:07,724 :: DEBUG :: row number = 26515
2019-09-16 13:55:07,725 :: INFO :: type of dataframe = dask
2019-09-16 13:55:07,725 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:55:07,749 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:55:07,841 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.116412
2019-09-16 13:55:07,842 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:55:07,842 :: INFO :: * START DATA CLEANING
2019-09-16 13:55:07,941 :: INFO :: checking missing values : 
2019-09-16 13:55:08,088 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:55:08,313 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:55:08,532 :: INFO :: - checking missing values for date_min column
2019-09-16 13:55:08,729 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.788415
2019-09-16 13:55:08,730 :: INFO :: checking types : 
2019-09-16 13:55:08,730 :: INFO :: - checking date type for date_min column
2019-09-16 13:55:08,882 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:55:09,009 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.278870
2019-09-16 13:55:09,009 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:55:12,960 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.951289
2019-09-16 13:55:12,961 :: INFO :: checking date validity :
2019-09-16 13:55:12,961 :: INFO :: - checking if date_max column is missing
2019-09-16 13:55:12,961 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000273
2019-09-16 13:55:12,961 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:55:13,022 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.061075
2019-09-16 13:55:13,022 :: INFO :: checking count_min and count_max : 
2019-09-16 13:55:13,023 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000175
2019-09-16 13:55:13,023 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 13:55:13,023 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000007
2019-09-16 13:55:13,023 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:55:13,023 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:55:13,023 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:55:13,023 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:55:13,023 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:55:13,024 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:55:13,058 :: INFO :: * END DATA CLEANING
2019-09-16 13:55:13,058 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:55:13,058 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:55:19,790 :: INFO :: dask df converted in pandas df - time : 0:00:06.731635
2019-09-16 13:55:19,791 :: INFO :: loading dataframe into DB table:
2019-09-16 13:55:21,363 :: INFO :: dask df loaded to db table - time : 0:00:01.572246
2019-09-16 13:55:21,892 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:08.833216
2019-09-16 13:55:21,913 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:55:22,132 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:55:22,208 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:55:22] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:55:34,438 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:55:34,744 :: INFO ::  * Restarting with stat
2019-09-16 13:55:36,428 :: WARNING ::  * Debugger is active!
2019-09-16 13:55:36,429 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:55:37,479 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/db/query.py', reloading
2019-09-16 13:55:37,811 :: INFO ::  * Restarting with stat
2019-09-16 13:55:39,893 :: WARNING ::  * Debugger is active!
2019-09-16 13:55:39,894 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:56:03,704 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:56:04,029 :: INFO ::  * Restarting with stat
2019-09-16 13:56:05,933 :: WARNING ::  * Debugger is active!
2019-09-16 13:56:05,934 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:56:07,813 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:56:07] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:56:07,843 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:56:07,859 :: DEBUG :: import_id = 392
2019-09-16 13:56:07,859 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:56:07,859 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:56:07,872 :: DEBUG :: row number = 26515
2019-09-16 13:56:07,873 :: INFO :: type of dataframe = dask
2019-09-16 13:56:07,873 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:56:07,893 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:56:07,983 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.109496
2019-09-16 13:56:07,983 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:56:07,984 :: INFO :: * START DATA CLEANING
2019-09-16 13:56:08,093 :: INFO :: checking missing values : 
2019-09-16 13:56:08,228 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:56:08,403 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:56:08,562 :: INFO :: - checking missing values for date_min column
2019-09-16 13:56:08,712 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.619309
2019-09-16 13:56:08,713 :: INFO :: checking types : 
2019-09-16 13:56:08,713 :: INFO :: - checking date type for date_min column
2019-09-16 13:56:08,877 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:56:09,015 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.302300
2019-09-16 13:56:09,015 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:56:13,081 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.066350
2019-09-16 13:56:13,082 :: INFO :: checking date validity :
2019-09-16 13:56:13,082 :: INFO :: - checking if date_max column is missing
2019-09-16 13:56:13,082 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000295
2019-09-16 13:56:13,082 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:56:13,146 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.063797
2019-09-16 13:56:13,146 :: INFO :: checking count_min and count_max : 
2019-09-16 13:56:13,146 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000192
2019-09-16 13:56:13,147 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 13:56:13,147 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 13:56:13,147 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:56:13,147 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:56:13,147 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:56:13,147 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:56:13,147 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:56:13,147 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:56:13,180 :: INFO :: * END DATA CLEANING
2019-09-16 13:56:13,181 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:56:13,181 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:56:19,921 :: INFO :: dask df converted in pandas df - time : 0:00:06.739411
2019-09-16 13:56:19,921 :: INFO :: loading dataframe into DB table:
2019-09-16 13:56:21,313 :: INFO :: dask df loaded to db table - time : 0:00:01.390740
2019-09-16 13:56:21,451 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:08.269804
2019-09-16 13:56:21,478 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:56:21,544 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:56:21,621 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:56:21] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:56:39,731 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:56:40,124 :: INFO ::  * Restarting with stat
2019-09-16 13:56:41,883 :: WARNING ::  * Debugger is active!
2019-09-16 13:56:41,883 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:56:57,476 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:56:57,782 :: INFO ::  * Restarting with stat
2019-09-16 13:56:59,539 :: WARNING ::  * Debugger is active!
2019-09-16 13:56:59,540 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:57:30,752 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 13:57:31,130 :: INFO ::  * Restarting with stat
2019-09-16 13:57:33,087 :: WARNING ::  * Debugger is active!
2019-09-16 13:57:33,088 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:57:36,216 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:57:36,548 :: INFO ::  * Restarting with stat
2019-09-16 13:57:38,676 :: WARNING ::  * Debugger is active!
2019-09-16 13:57:38,678 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:57:39,911 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:57:39] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:57:39,960 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:57:39,979 :: DEBUG :: import_id = 392
2019-09-16 13:57:39,979 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:57:39,979 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:57:39,991 :: DEBUG :: row number = 26515
2019-09-16 13:57:39,991 :: INFO :: type of dataframe = dask
2019-09-16 13:57:39,992 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:57:40,015 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:57:40,101 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.108210
2019-09-16 13:57:40,101 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:57:40,101 :: INFO :: * START DATA CLEANING
2019-09-16 13:57:40,193 :: INFO :: checking missing values : 
2019-09-16 13:57:40,365 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:57:40,550 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:57:40,706 :: INFO :: - checking missing values for date_min column
2019-09-16 13:57:40,860 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.667038
2019-09-16 13:57:40,860 :: INFO :: checking types : 
2019-09-16 13:57:40,860 :: INFO :: - checking date type for date_min column
2019-09-16 13:57:41,031 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:57:41,171 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.310216
2019-09-16 13:57:41,171 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:57:45,136 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.964552
2019-09-16 13:57:45,136 :: INFO :: checking date validity :
2019-09-16 13:57:45,136 :: INFO :: - checking if date_max column is missing
2019-09-16 13:57:45,136 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000390
2019-09-16 13:57:45,136 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:57:45,201 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.064669
2019-09-16 13:57:45,202 :: INFO :: checking count_min and count_max : 
2019-09-16 13:57:45,202 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000271
2019-09-16 13:57:45,202 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-16 13:57:45,202 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000007
2019-09-16 13:57:45,202 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:57:45,203 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:57:45,203 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:57:45,203 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:57:45,203 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:57:45,203 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:57:45,241 :: INFO :: * END DATA CLEANING
2019-09-16 13:57:45,241 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:57:45,241 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:57:52,002 :: INFO :: dask df converted in pandas df - time : 0:00:06.760377
2019-09-16 13:57:52,003 :: INFO :: loading dataframe into DB table:
2019-09-16 13:57:54,100 :: INFO :: dask df loaded to db table - time : 0:00:02.097125
2019-09-16 13:57:54,680 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 13:57:54,681 :: ERROR :: name 'df2' is not defined
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 577, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col, my_type)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 102, in load
    return df2
NameError: name 'df2' is not defined
2019-09-16 13:57:54,714 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:57:54] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 13:58:59,833 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 13:59:00,142 :: INFO ::  * Restarting with stat
2019-09-16 13:59:01,816 :: WARNING ::  * Debugger is active!
2019-09-16 13:59:01,817 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 13:59:04,620 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:59:04] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:59:04,654 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 13:59:04,670 :: DEBUG :: import_id = 392
2019-09-16 13:59:04,671 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 13:59:04,672 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 13:59:04,686 :: DEBUG :: row number = 26515
2019-09-16 13:59:04,686 :: INFO :: type of dataframe = dask
2019-09-16 13:59:04,687 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:59:04,710 :: WARNING :: ncores used by Dask = 4
2019-09-16 13:59:04,794 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.107195
2019-09-16 13:59:04,794 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 13:59:04,795 :: INFO :: * START DATA CLEANING
2019-09-16 13:59:04,868 :: INFO :: checking missing values : 
2019-09-16 13:59:05,013 :: INFO :: - checking missing values for cd_nom column
2019-09-16 13:59:05,197 :: INFO :: - checking missing values for nom_cite column
2019-09-16 13:59:05,360 :: INFO :: - checking missing values for date_min column
2019-09-16 13:59:05,564 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.695405
2019-09-16 13:59:05,564 :: INFO :: checking types : 
2019-09-16 13:59:05,564 :: INFO :: - checking date type for date_min column
2019-09-16 13:59:05,783 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 13:59:05,959 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.394477
2019-09-16 13:59:05,959 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 13:59:11,643 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.683516
2019-09-16 13:59:11,643 :: INFO :: checking date validity :
2019-09-16 13:59:11,644 :: INFO :: - checking if date_max column is missing
2019-09-16 13:59:11,644 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000713
2019-09-16 13:59:11,644 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 13:59:11,718 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.073341
2019-09-16 13:59:11,718 :: INFO :: checking count_min and count_max : 
2019-09-16 13:59:11,718 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000142
2019-09-16 13:59:11,718 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 13:59:11,718 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000005
2019-09-16 13:59:11,718 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 13:59:11,719 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 13:59:11,719 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 13:59:11,719 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 13:59:11,719 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 13:59:11,719 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 13:59:11,759 :: INFO :: * END DATA CLEANING
2019-09-16 13:59:11,759 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:59:11,761 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 13:59:18,870 :: INFO :: dask df converted in pandas df - time : 0:00:07.107535
2019-09-16 13:59:18,870 :: INFO :: loading dataframe into DB table:
2019-09-16 13:59:22,961 :: INFO :: dask df loaded to db table - time : 0:00:04.090178
2019-09-16 13:59:23,682 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:11.920759
2019-09-16 13:59:23,700 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 13:59:23,838 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 13:59:23,907 :: INFO :: 127.0.0.1 - - [16/Sep/2019 13:59:23] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 13:59:49,457 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 13:59:49,956 :: INFO ::  * Restarting with stat
2019-09-16 13:59:52,343 :: WARNING ::  * Debugger is active!
2019-09-16 13:59:52,344 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:00:46,500 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 14:00:46,783 :: INFO ::  * Restarting with stat
2019-09-16 14:00:48,505 :: WARNING ::  * Debugger is active!
2019-09-16 14:00:48,505 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:00:53,709 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:00:53] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:00:53,751 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:00:53,772 :: DEBUG :: import_id = 392
2019-09-16 14:00:53,772 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:00:53,772 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:00:53,785 :: DEBUG :: row number = 26515
2019-09-16 14:00:53,785 :: INFO :: type of dataframe = pandas
2019-09-16 14:00:53,785 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:00:53,816 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:00:53,902 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.116440
2019-09-16 14:00:53,902 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:00:53,902 :: INFO :: * START DATA CLEANING
2019-09-16 14:00:53,983 :: INFO :: checking missing values : 
2019-09-16 14:00:54,116 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:00:54,295 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:00:54,478 :: INFO :: - checking missing values for date_min column
2019-09-16 14:00:54,643 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.659219
2019-09-16 14:00:54,643 :: INFO :: checking types : 
2019-09-16 14:00:54,643 :: INFO :: - checking date type for date_min column
2019-09-16 14:00:54,815 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 14:00:54,952 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.309045
2019-09-16 14:00:54,952 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 14:00:59,019 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.066705
2019-09-16 14:00:59,019 :: INFO :: checking date validity :
2019-09-16 14:00:59,019 :: INFO :: - checking if date_max column is missing
2019-09-16 14:00:59,020 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000382
2019-09-16 14:00:59,020 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 14:00:59,079 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.059589
2019-09-16 14:00:59,080 :: INFO :: checking count_min and count_max : 
2019-09-16 14:00:59,080 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000158
2019-09-16 14:00:59,080 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-16 14:00:59,080 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000010
2019-09-16 14:00:59,080 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 14:00:59,080 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 14:00:59,081 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 14:00:59,081 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 14:00:59,081 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 14:00:59,081 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 14:00:59,112 :: INFO :: * END DATA CLEANING
2019-09-16 14:00:59,113 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:00:59,113 :: INFO :: loading dataframe into DB table:
2019-09-16 14:00:59,321 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 14:00:59,322 :: ERROR :: local variable 'cur' referenced before assignment
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 31, in load_df_to_sql
    df.to_csv(fbuf, index=False, header=True, sep=separator)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/core.py", line 1299, in to_csv
    return to_csv(self, filename, **kwargs)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/io/csv.py", line 741, in to_csv
    **(storage_options or {})
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/bytes/core.py", line 302, in open_files
    urlpath, mode, num=num, name_function=name_function, storage_options=kwargs
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/bytes/core.py", line 435, in get_fs_token_paths
    raise TypeError("url type not understood: %s" % urlpath)
TypeError: url type not understood: <_io.StringIO object at 0x7fa427005678>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 45, in load_df_to_sql
    conn.rollback()
UnboundLocalError: local variable 'conn' referenced before assignment

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 577, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col, my_type)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 85, in load
    load_df_to_sql(df, table_name, full_table_name, engine, schema_name, ';', import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 48, in load_df_to_sql
    cur.close()
UnboundLocalError: local variable 'cur' referenced before assignment
2019-09-16 14:00:59,478 :: ERROR :: Table gn_imports.i_data_pf_observado_original_392 vide  cause d'une erreur de copie, refaire l'upload et le mapping
2019-09-16 14:00:59,508 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:00:59] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 14:01:52,289 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 14:01:52,629 :: INFO ::  * Restarting with stat
2019-09-16 14:01:54,400 :: WARNING ::  * Debugger is active!
2019-09-16 14:01:54,401 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:02:05,286 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:02:05] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:02:05,324 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:02:05,342 :: DEBUG :: import_id = 392
2019-09-16 14:02:05,343 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:02:05,344 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:02:05,347 :: DEBUG :: row number = 0
2019-09-16 14:02:05,348 :: INFO :: type of dataframe = pandas
2019-09-16 14:02:05,348 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:02:05,369 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:02:05,457 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 14:02:05,457 :: ERROR :: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 551, in postMapping
    df = extract(table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, column_names, index_col, import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/extract/extract.py", line 31, in extract
    df = dd.read_sql_table(table=table_name, index_col=index_dask, meta=empty_df, npartitions=ncores, uri=str(DB.engine.url), schema=schema_name, bytes_per_chunk=100000)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/io/sql.py", line 183, in read_sql_table
    "index column type must be numeric or datetime.".format(dtype)
TypeError: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
2019-09-16 14:02:05,461 :: ERROR :: Table gn_imports.i_data_pf_observado_original_392 vide  cause d'une erreur de copie, refaire l'upload et le mapping
2019-09-16 14:02:05,467 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:02:05] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 14:02:10,432 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 14:02:10,433 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:02:10,583 :: INFO :: File size = 10.378695487976074 Mo
2019-09-16 14:02:10,601 :: DEBUG :: original file name = data_pf_observado_original.csv
2019-09-16 14:02:10,601 :: INFO :: Upload : file saved in directory - time : 0:00:00.167955
2019-09-16 14:02:10,601 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:02:10,601 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 14:02:14,145 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'gn_date', 'heure', 'date_encodage', 'gn_timestamp', 'type_observation', 'nombre', 'gn_min', 'gn_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'lat', 'gn_long', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'gn_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 14:02:14,146 :: DEBUG :: row_count = 26516
2019-09-16 14:02:14,146 :: INFO :: User file validity checked - time : 0:00:03.545025
2019-09-16 14:02:14,147 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 14:02:14,165 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_original_392
2019-09-16 14:02:14,411 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:02:14,927 :: INFO :: CSV loaded to DB table - time : 0:00:00.514833
2019-09-16 14:02:14,927 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:02:14,927 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:02:15,597 :: INFO :: CSV loaded to DB table - time : 0:00:00.670025
2019-09-16 14:02:15,598 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:02:15,695 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 14:02:15,703 :: INFO :: Total time to post user file and fill metadata - time : 0:00:05.270795
2019-09-16 14:02:15,714 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:02:15] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 14:02:15,764 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:02:15] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 14:02:18,102 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:02:18] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:02:18,124 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:02:18,137 :: DEBUG :: import_id = 392
2019-09-16 14:02:18,137 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:02:18,137 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:02:18,147 :: DEBUG :: row number = 26515
2019-09-16 14:02:18,147 :: INFO :: type of dataframe = pandas
2019-09-16 14:02:18,148 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:02:18,167 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:02:18,252 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.104614
2019-09-16 14:02:18,253 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:02:18,253 :: INFO :: * START DATA CLEANING
2019-09-16 14:02:18,395 :: INFO :: checking missing values : 
2019-09-16 14:02:18,566 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:02:18,758 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:02:18,915 :: INFO :: - checking missing values for date_min column
2019-09-16 14:02:19,063 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.668814
2019-09-16 14:02:19,064 :: INFO :: checking types : 
2019-09-16 14:02:19,064 :: INFO :: - checking date type for date_min column
2019-09-16 14:02:19,226 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 14:02:19,365 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.301520
2019-09-16 14:02:19,365 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 14:02:23,549 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.183188
2019-09-16 14:02:23,549 :: INFO :: checking date validity :
2019-09-16 14:02:23,549 :: INFO :: - checking if date_max column is missing
2019-09-16 14:02:23,549 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000346
2019-09-16 14:02:23,549 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 14:02:23,614 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.064691
2019-09-16 14:02:23,615 :: INFO :: checking count_min and count_max : 
2019-09-16 14:02:23,615 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000387
2019-09-16 14:02:23,616 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000018
2019-09-16 14:02:23,616 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000010
2019-09-16 14:02:23,616 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 14:02:23,616 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 14:02:23,616 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 14:02:23,616 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 14:02:23,617 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 14:02:23,617 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 14:02:23,650 :: INFO :: * END DATA CLEANING
2019-09-16 14:02:23,650 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:02:23,650 :: INFO :: loading dataframe into DB table:
2019-09-16 14:02:24,333 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 14:02:24,334 :: ERROR :: local variable 'cur' referenced before assignment
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 32, in load_df_to_sql
    df.to_csv(fbuf, index=False, header=True, sep=separator)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/core.py", line 1299, in to_csv
    return to_csv(self, filename, **kwargs)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/io/csv.py", line 741, in to_csv
    **(storage_options or {})
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/bytes/core.py", line 302, in open_files
    urlpath, mode, num=num, name_function=name_function, storage_options=kwargs
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/bytes/core.py", line 435, in get_fs_token_paths
    raise TypeError("url type not understood: %s" % urlpath)
TypeError: url type not understood: <_io.StringIO object at 0x7f3156def828>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 577, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col, my_type)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 85, in load
    load_df_to_sql(df, table_name, full_table_name, engine, schema_name, ';', import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 48, in load_df_to_sql
    cur.close()
UnboundLocalError: local variable 'cur' referenced before assignment
2019-09-16 14:02:24,344 :: ERROR :: Table gn_imports.i_data_pf_observado_original_392 vide  cause d'une erreur de copie, refaire l'upload et le mapping
2019-09-16 14:02:24,380 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:02:24] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 14:03:01,094 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 14:03:01,408 :: INFO ::  * Restarting with stat
2019-09-16 14:03:14,714 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 14:03:14,714 :: INFO ::  * Restarting with stat
2019-09-16 14:03:16,431 :: WARNING ::  * Debugger is active!
2019-09-16 14:03:16,431 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:03:19,955 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:03:19] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:03:19,985 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:03:20,001 :: DEBUG :: import_id = 392
2019-09-16 14:03:20,001 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:03:20,001 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:03:20,008 :: DEBUG :: row number = 0
2019-09-16 14:03:20,009 :: INFO :: type of dataframe = pandas
2019-09-16 14:03:20,009 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:03:20,035 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:03:20,114 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 14:03:20,115 :: ERROR :: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 551, in postMapping
    df = extract(table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, column_names, index_col, import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/extract/extract.py", line 31, in extract
    df = dd.read_sql_table(table=table_name, index_col=index_dask, meta=empty_df, npartitions=ncores, uri=str(DB.engine.url), schema=schema_name, bytes_per_chunk=100000)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/io/sql.py", line 183, in read_sql_table
    "index column type must be numeric or datetime.".format(dtype)
TypeError: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
2019-09-16 14:03:20,118 :: ERROR :: Table gn_imports.i_data_pf_observado_original_392 vide  cause d'une erreur de copie, refaire l'upload et le mapping
2019-09-16 14:03:20,120 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:03:20] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 14:03:24,765 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 14:03:24,766 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:03:24,901 :: INFO :: File size = 10.378695487976074 Mo
2019-09-16 14:03:24,918 :: DEBUG :: original file name = data_pf_observado_original.csv
2019-09-16 14:03:24,918 :: INFO :: Upload : file saved in directory - time : 0:00:00.152357
2019-09-16 14:03:24,918 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:03:24,918 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 14:03:28,353 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'gn_date', 'heure', 'date_encodage', 'gn_timestamp', 'type_observation', 'nombre', 'gn_min', 'gn_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'lat', 'gn_long', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'gn_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 14:03:28,353 :: DEBUG :: row_count = 26516
2019-09-16 14:03:28,353 :: INFO :: User file validity checked - time : 0:00:03.435151
2019-09-16 14:03:28,354 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 14:03:28,369 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_original_392
2019-09-16 14:03:28,646 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:03:29,083 :: INFO :: CSV loaded to DB table - time : 0:00:00.436961
2019-09-16 14:03:29,083 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:03:29,083 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:03:29,715 :: INFO :: CSV loaded to DB table - time : 0:00:00.631741
2019-09-16 14:03:29,715 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:03:29,850 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 14:03:29,853 :: INFO :: Total time to post user file and fill metadata - time : 0:00:05.087683
2019-09-16 14:03:29,858 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:03:29] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 14:03:29,901 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:03:29] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 14:03:32,535 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:03:32] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:03:32,566 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:03:32,575 :: DEBUG :: import_id = 392
2019-09-16 14:03:32,576 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:03:32,576 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:03:32,588 :: DEBUG :: row number = 26515
2019-09-16 14:03:32,588 :: INFO :: type of dataframe = pandas
2019-09-16 14:03:32,588 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:03:32,602 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:03:32,669 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.080405
2019-09-16 14:03:32,669 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:03:32,669 :: INFO :: * START DATA CLEANING
2019-09-16 14:03:32,779 :: INFO :: checking missing values : 
2019-09-16 14:03:32,897 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:03:33,065 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:03:33,211 :: INFO :: - checking missing values for date_min column
2019-09-16 14:03:33,358 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.578570
2019-09-16 14:03:33,359 :: INFO :: checking types : 
2019-09-16 14:03:33,359 :: INFO :: - checking date type for date_min column
2019-09-16 14:03:33,511 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 14:03:33,649 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.290360
2019-09-16 14:03:33,649 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 14:03:37,841 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.191885
2019-09-16 14:03:37,842 :: INFO :: checking date validity :
2019-09-16 14:03:37,843 :: INFO :: - checking if date_max column is missing
2019-09-16 14:03:37,843 :: INFO :: Data cleaning : dates checked - time : 0:00:00.001076
2019-09-16 14:03:37,844 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 14:03:37,902 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.058095
2019-09-16 14:03:37,902 :: INFO :: checking count_min and count_max : 
2019-09-16 14:03:37,902 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000347
2019-09-16 14:03:37,903 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000019
2019-09-16 14:03:37,903 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000027
2019-09-16 14:03:37,904 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 14:03:37,904 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 14:03:37,905 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 14:03:37,905 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 14:03:37,906 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 14:03:37,906 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 14:03:37,935 :: INFO :: * END DATA CLEANING
2019-09-16 14:03:37,935 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:03:37,936 :: INFO :: loading dataframe into DB table:
2019-09-16 14:03:38,184 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 14:03:38,185 :: ERROR :: url type not understood: <_io.StringIO object at 0x7f55e35ef828>
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 577, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col, my_type)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 85, in load
    load_df_to_sql(df, table_name, full_table_name, engine, schema_name, ';', import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/load/load.py", line 32, in load_df_to_sql
    df.to_csv(fbuf, index=False, header=True, sep=separator)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/core.py", line 1299, in to_csv
    return to_csv(self, filename, **kwargs)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/io/csv.py", line 741, in to_csv
    **(storage_options or {})
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/bytes/core.py", line 302, in open_files
    urlpath, mode, num=num, name_function=name_function, storage_options=kwargs
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/bytes/core.py", line 435, in get_fs_token_paths
    raise TypeError("url type not understood: %s" % urlpath)
TypeError: url type not understood: <_io.StringIO object at 0x7f55e35ef828>
2019-09-16 14:03:38,193 :: ERROR :: Table gn_imports.i_data_pf_observado_original_392 vide  cause d'une erreur de copie, refaire l'upload et le mapping
2019-09-16 14:03:38,227 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:03:38] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 14:03:57,760 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 14:03:58,080 :: INFO ::  * Restarting with stat
2019-09-16 14:03:59,809 :: WARNING ::  * Debugger is active!
2019-09-16 14:03:59,810 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:04:10,302 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 14:04:10,303 :: INFO ::  * Restarting with stat
2019-09-16 14:04:12,860 :: WARNING ::  * Debugger is active!
2019-09-16 14:04:12,861 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:04:12,869 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:04:12] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:04:12,895 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:04:12,917 :: DEBUG :: import_id = 392
2019-09-16 14:04:12,917 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:04:12,917 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:04:12,920 :: DEBUG :: row number = 0
2019-09-16 14:04:12,920 :: INFO :: type of dataframe = pandas
2019-09-16 14:04:12,920 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:04:12,941 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:04:12,996 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 14:04:12,996 :: ERROR :: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 551, in postMapping
    df = extract(table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, column_names, index_col, import_id)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
  File "/home/ju/geonature/external_modules/import/backend/extract/extract.py", line 31, in extract
    df = dd.read_sql_table(table=table_name, index_col=index_dask, meta=empty_df, npartitions=ncores, uri=str(DB.engine.url), schema=schema_name, bytes_per_chunk=100000)
  File "/home/ju/geonature/backend/venv/lib/python3.6/site-packages/dask/dataframe/io/sql.py", line 183, in read_sql_table
    "index column type must be numeric or datetime.".format(dtype)
TypeError: Provided index column is of type "object".  If divisions is not provided the index column type must be numeric or datetime.
2019-09-16 14:04:12,998 :: ERROR :: Table gn_imports.i_data_pf_observado_original_392 vide  cause d'une erreur de copie, refaire l'upload et le mapping
2019-09-16 14:04:13,000 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:04:13] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 14:04:17,840 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 14:04:17,840 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:04:17,969 :: INFO :: File size = 10.378695487976074 Mo
2019-09-16 14:04:17,987 :: DEBUG :: original file name = data_pf_observado_original.csv
2019-09-16 14:04:17,987 :: INFO :: Upload : file saved in directory - time : 0:00:00.146825
2019-09-16 14:04:17,987 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:04:17,987 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 14:04:21,331 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'gn_date', 'heure', 'date_encodage', 'gn_timestamp', 'type_observation', 'nombre', 'gn_min', 'gn_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'lat', 'gn_long', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'gn_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 14:04:21,332 :: DEBUG :: row_count = 26516
2019-09-16 14:04:21,332 :: INFO :: User file validity checked - time : 0:00:03.344369
2019-09-16 14:04:21,332 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 14:04:21,350 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_original_392
2019-09-16 14:04:21,622 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:04:21,904 :: INFO :: CSV loaded to DB table - time : 0:00:00.281516
2019-09-16 14:04:21,904 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:04:21,904 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:04:23,015 :: INFO :: CSV loaded to DB table - time : 0:00:01.110054
2019-09-16 14:04:23,015 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:04:23,198 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 14:04:23,208 :: INFO :: Total time to post user file and fill metadata - time : 0:00:05.368346
2019-09-16 14:04:23,219 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:04:23] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 14:04:23,268 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:04:23] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 14:04:25,668 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:04:25] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:04:25,699 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:04:25,713 :: DEBUG :: import_id = 392
2019-09-16 14:04:25,714 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:04:25,715 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:04:25,729 :: DEBUG :: row number = 26515
2019-09-16 14:04:25,729 :: INFO :: type of dataframe = pandas
2019-09-16 14:04:25,729 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:04:25,746 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:04:25,808 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.078822
2019-09-16 14:04:25,808 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:04:25,808 :: INFO :: * START DATA CLEANING
2019-09-16 14:04:25,900 :: INFO :: checking missing values : 
2019-09-16 14:04:26,052 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:04:26,263 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:04:26,459 :: INFO :: - checking missing values for date_min column
2019-09-16 14:04:26,623 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.722468
2019-09-16 14:04:26,624 :: INFO :: checking types : 
2019-09-16 14:04:26,624 :: INFO :: - checking date type for date_min column
2019-09-16 14:04:26,799 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 14:04:26,950 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.326712
2019-09-16 14:04:26,951 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 14:04:32,468 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:05.516800
2019-09-16 14:04:32,468 :: INFO :: checking date validity :
2019-09-16 14:04:32,468 :: INFO :: - checking if date_max column is missing
2019-09-16 14:04:32,468 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000411
2019-09-16 14:04:32,468 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 14:04:32,536 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.067066
2019-09-16 14:04:32,536 :: INFO :: checking count_min and count_max : 
2019-09-16 14:04:32,536 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000310
2019-09-16 14:04:32,536 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000012
2019-09-16 14:04:32,537 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000012
2019-09-16 14:04:32,537 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne species_id
2019-09-16 14:04:32,537 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_scientifique
2019-09-16 14:04:32,538 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne gn_timestamp
2019-09-16 14:04:32,538 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_gn_timestamp
2019-09-16 14:04:32,538 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_scientifique
2019-09-16 14:04:32,538 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 14:04:32,585 :: INFO :: * END DATA CLEANING
2019-09-16 14:04:32,585 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:04:32,585 :: INFO :: loading dataframe into DB table:
2019-09-16 14:05:55,550 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 14:05:56,171 :: INFO ::  * Restarting with stat
2019-09-16 14:05:58,345 :: WARNING ::  * Debugger is active!
2019-09-16 14:05:58,346 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:06:06,704 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:06:06] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:06:06,734 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:06:06,749 :: DEBUG :: import_id = 392
2019-09-16 14:06:06,749 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:06:06,750 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:06:06,765 :: DEBUG :: row number = 26515
2019-09-16 14:06:06,765 :: INFO :: type of dataframe = pandas
2019-09-16 14:06:06,765 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:06:06,783 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:06:06,858 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.092135
2019-09-16 14:06:08,067 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:06:08,067 :: INFO :: * START DATA CLEANING
2019-09-16 14:06:08,080 :: INFO :: checking missing values : 
2019-09-16 14:06:08,141 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:06:08,149 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:06:08,154 :: INFO :: - checking missing values for date_min column
2019-09-16 14:06:08,160 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.080271
2019-09-16 14:06:08,160 :: INFO :: checking types : 
2019-09-16 14:06:08,160 :: INFO :: - checking date type for date_min column
2019-09-16 14:06:08,263 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 14:06:08,311 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.151086
2019-09-16 14:06:08,311 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 14:06:09,805 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:01.493022
2019-09-16 14:06:09,805 :: INFO :: checking date validity :
2019-09-16 14:06:09,806 :: INFO :: - checking if date_max column is missing
2019-09-16 14:06:09,806 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000724
2019-09-16 14:06:09,807 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 14:06:09,996 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.189432
2019-09-16 14:06:09,996 :: INFO :: checking count_min and count_max : 
2019-09-16 14:06:09,996 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000120
2019-09-16 14:06:09,996 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 14:06:09,996 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 14:06:09,997 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 14:06:10,189 :: INFO :: * END DATA CLEANING
2019-09-16 14:06:10,189 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:06:10,190 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 14:06:10,190 :: ERROR :: name 'my_type' is not defined
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 578, in postMapping
    df = load(df, table_names['imports_table_name'], IMPORTS_SCHEMA_NAME, table_names['imports_full_table_name'], import_id, engine, index_col, my_type)
NameError: name 'my_type' is not defined
2019-09-16 14:06:10,315 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:06:10] "POST /import/mapping/392 HTTP/1.1" 500 -
2019-09-16 14:06:26,982 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 14:06:27,421 :: INFO ::  * Restarting with stat
2019-09-16 14:06:30,027 :: WARNING ::  * Debugger is active!
2019-09-16 14:06:30,027 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:06:30,918 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:06:30] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:06:30,956 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:06:30,977 :: DEBUG :: import_id = 392
2019-09-16 14:06:30,977 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:06:30,977 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:06:30,993 :: DEBUG :: row number = 26515
2019-09-16 14:06:30,993 :: INFO :: type of dataframe = pandas
2019-09-16 14:06:30,993 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:06:31,014 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:06:31,107 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.114058
2019-09-16 14:06:32,323 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:06:32,323 :: INFO :: * START DATA CLEANING
2019-09-16 14:06:32,330 :: INFO :: checking missing values : 
2019-09-16 14:06:32,372 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:06:32,376 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:06:32,379 :: INFO :: - checking missing values for date_min column
2019-09-16 14:06:32,382 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.051982
2019-09-16 14:06:32,382 :: INFO :: checking types : 
2019-09-16 14:06:32,382 :: INFO :: - checking date type for date_min column
2019-09-16 14:06:32,456 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 14:06:32,488 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.105878
2019-09-16 14:06:32,488 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 14:06:34,424 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:01.936143
2019-09-16 14:06:34,425 :: INFO :: checking date validity :
2019-09-16 14:06:34,425 :: INFO :: - checking if date_max column is missing
2019-09-16 14:06:34,425 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000411
2019-09-16 14:06:34,425 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 14:06:34,713 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.287885
2019-09-16 14:06:34,714 :: INFO :: checking count_min and count_max : 
2019-09-16 14:06:34,714 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000137
2019-09-16 14:06:34,714 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000008
2019-09-16 14:06:34,714 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000005
2019-09-16 14:06:34,714 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 14:06:34,932 :: INFO :: * END DATA CLEANING
2019-09-16 14:06:34,932 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:06:34,932 :: INFO :: loading dataframe into DB table:
2019-09-16 14:07:06,258 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 14:07:06,262 :: INFO ::  * Restarting with stat
2019-09-16 14:07:09,410 :: WARNING ::  * Debugger is active!
2019-09-16 14:07:09,414 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:07:46,936 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 14:07:47,227 :: INFO ::  * Restarting with stat
2019-09-16 14:07:49,392 :: WARNING ::  * Debugger is active!
2019-09-16 14:07:49,392 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:07:55,496 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:07:55] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:07:55,525 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:07:55,540 :: DEBUG :: import_id = 392
2019-09-16 14:07:55,540 :: DEBUG :: DB tabel name = i_data_pf_observado_original_392
2019-09-16 14:07:55,540 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'gn_timestamp'}
2019-09-16 14:07:55,550 :: DEBUG :: row number = 26515
2019-09-16 14:07:55,551 :: INFO :: type of dataframe = pandas
2019-09-16 14:07:55,551 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:07:55,567 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:07:55,631 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.079996
2019-09-16 14:07:56,964 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:07:56,964 :: INFO :: * START DATA CLEANING
2019-09-16 14:07:56,981 :: INFO :: checking missing values : 
2019-09-16 14:07:57,036 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:07:57,042 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:07:57,048 :: INFO :: - checking missing values for date_min column
2019-09-16 14:07:57,051 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.070026
2019-09-16 14:07:57,052 :: INFO :: checking types : 
2019-09-16 14:07:57,052 :: INFO :: - checking date type for date_min column
2019-09-16 14:07:57,214 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 14:07:57,261 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.209467
2019-09-16 14:07:57,261 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 14:07:58,637 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:01.375869
2019-09-16 14:07:58,638 :: INFO :: checking date validity :
2019-09-16 14:07:58,638 :: INFO :: - checking if date_max column is missing
2019-09-16 14:07:58,638 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000473
2019-09-16 14:07:58,638 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 14:07:58,821 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.182583
2019-09-16 14:07:58,821 :: INFO :: checking count_min and count_max : 
2019-09-16 14:07:58,822 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000155
2019-09-16 14:07:58,822 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000006
2019-09-16 14:07:58,822 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000010
2019-09-16 14:07:58,822 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 14:07:58,990 :: INFO :: * END DATA CLEANING
2019-09-16 14:07:58,991 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:07:58,991 :: INFO :: loading dataframe into DB table:
2019-09-16 14:08:01,217 :: INFO :: dask df loaded to db table - time : 0:00:02.225814
2019-09-16 14:08:02,874 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:03.882884
2019-09-16 14:08:02,874 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:08:03,097 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 14:08:03,149 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:08:03] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:08:54,847 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 14:08:54,847 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:08:57,566 :: INFO :: File size = 159.84538650512695 Mo
2019-09-16 14:08:57,847 :: DEBUG :: original file name = data_PINV.csv
2019-09-16 14:08:57,847 :: INFO :: Upload : file saved in directory - time : 0:00:02.999670
2019-09-16 14:08:57,848 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:08:57,848 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 14:09:22,213 :: DEBUG :: column_names = ['idSynthese', 'geojson_4326', 'geojson_local', 'jddId', 'id_digitiser', 'Id_SINP', 'Id_producteur', 'wkt', 'x_centroid', 'y_centroid', 'Alti_min', 'Alti_max', 'Jeu_donnees', 'Acteurs', 'Observateurs', 'Determinateur', 'Date_debut', 'Date_fin', 'Nom_valide', 'Nom_vern', 'Cd_Ref', 'Nom_cite', 'Cd_Nom', 'Methode_obs', 'Meth_deter', 'Preuve', 'Stade_vie', 'Sexe', 'Etat_bio', 'Nombre_min', 'Nombre_max', 'Objet_dnbr', 'Type_dnbr', 'Com_releve', 'Com_occurrence', 'Stat_Valid', 'Validateur', 'Integr_sinp', 'Dern_maj', 'id_acquisition_framework', 'lastact', 'vTAXREF', 'permIdGrp', 'sampleNumb', 'preuvNum', 'preuvNoNum', 'ObjGeoTyp', 'methGrp', 'obsTech', 'ocNat', 'difNivPrec', 'sensiNiv', 'statObs', 'dEEFlou', 'statSource', 'typInfGeo']
2019-09-16 14:09:22,214 :: DEBUG :: row_count = 161852
2019-09-16 14:09:22,215 :: INFO :: User file validity checked - time : 0:00:24.367010
2019-09-16 14:09:22,216 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 14:09:22,240 :: DEBUG :: full DB user table name = gn_imports.i_data_pinv_392
2019-09-16 14:09:22,527 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:09:41,232 :: INFO :: CSV loaded to DB table - time : 0:00:18.704388
2019-09-16 14:09:41,232 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:09:41,232 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:09:59,058 :: INFO :: CSV loaded to DB table - time : 0:00:17.825521
2019-09-16 14:09:59,059 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:09:59,175 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 14:09:59,211 :: INFO :: Total time to post user file and fill metadata - time : 0:01:04.364894
2019-09-16 14:09:59,235 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:09:59] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 14:09:59,275 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:09:59] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 14:10:52,858 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:10:52] "OPTIONS /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:10:52,886 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:10:52,897 :: DEBUG :: import_id = 392
2019-09-16 14:10:52,897 :: DEBUG :: DB tabel name = i_data_pinv_392
2019-09-16 14:10:52,897 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 14:10:53,083 :: DEBUG :: row number = 161851
2019-09-16 14:10:53,083 :: INFO :: type of dataframe = dask
2019-09-16 14:10:53,084 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:10:53,097 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:10:53,154 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.070098
2019-09-16 14:10:53,154 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:10:53,154 :: INFO :: * START DATA CLEANING
2019-09-16 14:10:53,220 :: INFO :: checking missing values : 
2019-09-16 14:10:53,329 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:10:53,472 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:10:53,623 :: INFO :: - checking missing values for date_min column
2019-09-16 14:10:53,782 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.562727
2019-09-16 14:10:53,783 :: INFO :: checking types : 
2019-09-16 14:10:53,783 :: INFO :: - checking date type for date_min column
2019-09-16 14:10:53,955 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 14:10:54,222 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.439463
2019-09-16 14:10:54,223 :: INFO :: checking cd_nom validity for cd_nom column
2019-09-16 14:10:58,589 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:04.366709
2019-09-16 14:10:58,590 :: INFO :: checking date validity :
2019-09-16 14:10:58,590 :: INFO :: - checking if date_max column is missing
2019-09-16 14:10:58,590 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000249
2019-09-16 14:10:58,590 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 14:10:58,651 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.061471
2019-09-16 14:10:58,652 :: INFO :: checking count_min and count_max : 
2019-09-16 14:10:58,652 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000144
2019-09-16 14:10:58,652 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 14:10:58,652 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 14:10:58,652 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne cd_nom
2019-09-16 14:10:58,652 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_cite
2019-09-16 14:10:58,652 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne date_debut
2019-09-16 14:10:58,653 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_date_debut
2019-09-16 14:10:58,653 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_cite
2019-09-16 14:10:58,653 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 14:10:58,676 :: INFO :: * END DATA CLEANING
2019-09-16 14:10:58,677 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:10:58,677 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 14:11:39,671 :: INFO :: dask df converted in pandas df - time : 0:00:40.994317
2019-09-16 14:11:39,798 :: INFO :: loading dataframe into DB table:
2019-09-16 14:12:17,091 :: INFO :: dask df loaded to db table - time : 0:00:37.292165
2019-09-16 14:12:20,167 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:01:21.490298
2019-09-16 14:12:20,184 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:12:20,357 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 14:12:21,076 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:12:21] "POST /import/mapping/392 HTTP/1.1" 200 -
2019-09-16 14:14:09,272 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 14:14:13,506 :: INFO ::  * Restarting with stat
2019-09-16 14:14:37,620 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 14:14:39,080 :: INFO ::  * Restarting with stat
2019-09-16 14:14:40,654 :: WARNING ::  * Debugger is active!
2019-09-16 14:14:40,813 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:17:08,216 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:08] "GET /gn_commons/modules HTTP/1.1" 403 -
2019-09-16 14:17:09,828 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:09] "OPTIONS /auth/login HTTP/1.1" 200 -
2019-09-16 14:17:10,705 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:10] "POST /auth/login HTTP/1.1" 200 -
2019-09-16 14:17:10,811 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:10] "GET /gn_commons/modules/GEONATURE HTTP/1.1" 404 -
2019-09-16 14:17:10,834 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:10] "GET /gn_commons/modules HTTP/1.1" 200 -
2019-09-16 14:17:11,114 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:11] "GET /synthese/general_stats HTTP/1.1" 200 -
2019-09-16 14:17:12,417 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:12] "GET /synthese/for_web?limit=100 HTTP/1.1" 200 -
2019-09-16 14:17:13,252 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:13] "GET /import HTTP/1.1" 200 -
2019-09-16 14:17:13,261 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:13] "GET /import/delete_step1 HTTP/1.1" 200 -
2019-09-16 14:17:14,583 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:17:14] "GET /import/datasets HTTP/1.1" 200 -
2019-09-16 14:17:26,029 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 14:17:26,029 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:17:28,166 :: INFO :: File size = 159.84538650512695 Mo
2019-09-16 14:17:28,430 :: DEBUG :: original file name = data_PINV.csv
2019-09-16 14:17:28,431 :: INFO :: Upload : file saved in directory - time : 0:00:02.401590
2019-09-16 14:17:28,431 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 14:17:28,431 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 14:17:50,919 :: DEBUG :: column_names = ['idSynthese', 'geojson_4326', 'geojson_local', 'jddId', 'id_digitiser', 'Id_SINP', 'Id_producteur', 'wkt', 'x_centroid', 'y_centroid', 'Alti_min', 'Alti_max', 'Jeu_donnees', 'Acteurs', 'Observateurs', 'Determinateur', 'Date_debut', 'Date_fin', 'Nom_valide', 'Nom_vern', 'Cd_Ref', 'Nom_cite', 'Cd_Nom', 'Methode_obs', 'Meth_deter', 'Preuve', 'Stade_vie', 'Sexe', 'Etat_bio', 'Nombre_min', 'Nombre_max', 'Objet_dnbr', 'Type_dnbr', 'Com_releve', 'Com_occurrence', 'Stat_Valid', 'Validateur', 'Integr_sinp', 'Dern_maj', 'id_acquisition_framework', 'lastact', 'vTAXREF', 'permIdGrp', 'sampleNumb', 'preuvNum', 'preuvNoNum', 'ObjGeoTyp', 'methGrp', 'obsTech', 'ocNat', 'difNivPrec', 'sensiNiv', 'statObs', 'dEEFlou', 'statSource', 'typInfGeo']
2019-09-16 14:17:50,920 :: DEBUG :: row_count = 161852
2019-09-16 14:17:50,921 :: INFO :: User file validity checked - time : 0:00:22.489883
2019-09-16 14:17:50,921 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 14:17:50,974 :: DEBUG :: id_import = 393
2019-09-16 14:17:50,976 :: DEBUG :: id_role = 1
2019-09-16 14:17:51,018 :: DEBUG :: full DB user table name = gn_imports.i_data_pinv_393
2019-09-16 14:17:51,322 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:18:09,516 :: INFO :: CSV loaded to DB table - time : 0:00:18.193160
2019-09-16 14:18:09,516 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 14:18:09,516 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:18:23,835 :: INFO :: CSV loaded to DB table - time : 0:00:14.317954
2019-09-16 14:18:23,835 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 14:18:24,302 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 14:18:24,347 :: INFO :: Total time to post user file and fill metadata - time : 0:00:58.318133
2019-09-16 14:18:24,379 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:18:24] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 14:18:24,433 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:18:24] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 14:18:58,041 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:18:58] "OPTIONS /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 14:18:58,070 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:18:58,087 :: DEBUG :: import_id = 393
2019-09-16 14:18:58,087 :: DEBUG :: DB tabel name = i_data_pinv_393
2019-09-16 14:18:58,087 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 14:18:58,352 :: DEBUG :: row number = 161851
2019-09-16 14:18:58,352 :: INFO :: type of dataframe = pandas
2019-09-16 14:18:58,353 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:18:58,370 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:18:58,450 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.097040
2019-09-16 14:19:08,334 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:19:08,335 :: INFO :: * START DATA CLEANING
2019-09-16 14:19:08,357 :: INFO :: checking missing values : 
2019-09-16 14:19:08,743 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:19:08,773 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:19:11,078 :: INFO :: - checking missing values for date_min column
2019-09-16 14:19:11,098 :: INFO :: Data cleaning : missing values checked - time : 0:00:02.741236
2019-09-16 14:19:11,099 :: INFO :: checking types : 
2019-09-16 14:19:11,099 :: INFO :: - checking date type for date_min column
2019-09-16 14:19:11,588 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 14:19:11,827 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.728130
2019-09-16 14:19:11,827 :: INFO :: checking cd_nom validity for cd_nom column
2019-09-16 14:19:14,944 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.116093
2019-09-16 14:19:14,944 :: INFO :: checking date validity :
2019-09-16 14:19:14,945 :: INFO :: - checking if date_max column is missing
2019-09-16 14:19:14,945 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000916
2019-09-16 14:19:14,945 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 14:19:15,983 :: INFO :: Data cleaning : uuid values checked - time : 0:00:01.037623
2019-09-16 14:19:15,983 :: INFO :: checking count_min and count_max : 
2019-09-16 14:19:15,983 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000188
2019-09-16 14:19:15,984 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-16 14:19:15,984 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000007
2019-09-16 14:19:15,984 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_cite
2019-09-16 14:19:15,984 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_cite
2019-09-16 14:19:18,044 :: INFO :: * END DATA CLEANING
2019-09-16 14:19:18,044 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:19:18,045 :: INFO :: loading dataframe into DB table:
2019-09-16 14:19:40,987 :: INFO :: dask df loaded to db table - time : 0:00:22.940995
2019-09-16 14:19:42,439 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:24.394121
2019-09-16 14:19:42,439 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 14:19:42,601 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 14:19:43,269 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:19:43] "POST /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 14:22:56,022 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_cd_nom.py', reloading
2019-09-16 14:22:56,384 :: INFO ::  * Restarting with stat
2019-09-16 14:22:58,873 :: WARNING ::  * Debugger is active!
2019-09-16 14:22:58,873 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:23:04,106 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 14:23:04,395 :: INFO ::  * Restarting with stat
2019-09-16 14:23:05,991 :: WARNING ::  * Debugger is active!
2019-09-16 14:23:05,991 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:23:55,689 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 14:23:56,141 :: INFO ::  * Restarting with stat
2019-09-16 14:23:58,030 :: WARNING ::  * Debugger is active!
2019-09-16 14:23:58,031 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:24:30,273 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_missing.py', reloading
2019-09-16 14:24:30,578 :: INFO ::  * Restarting with stat
2019-09-16 14:24:32,290 :: WARNING ::  * Debugger is active!
2019-09-16 14:24:32,290 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:25:17,926 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_missing.py', reloading
2019-09-16 14:25:18,254 :: INFO ::  * Restarting with stat
2019-09-16 14:25:19,907 :: WARNING ::  * Debugger is active!
2019-09-16 14:25:19,908 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:25:26,166 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_missing.py', reloading
2019-09-16 14:25:26,592 :: INFO ::  * Restarting with stat
2019-09-16 14:25:28,409 :: WARNING ::  * Debugger is active!
2019-09-16 14:25:28,410 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:25:53,238 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_cd_nom.py', reloading
2019-09-16 14:25:53,524 :: INFO ::  * Restarting with stat
2019-09-16 14:25:55,178 :: WARNING ::  * Debugger is active!
2019-09-16 14:25:55,178 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:26:43,764 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 14:26:44,076 :: INFO ::  * Restarting with stat
2019-09-16 14:26:46,458 :: WARNING ::  * Debugger is active!
2019-09-16 14:26:46,458 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:27:04,161 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_dates.py', reloading
2019-09-16 14:27:04,591 :: INFO ::  * Restarting with stat
2019-09-16 14:27:06,988 :: WARNING ::  * Debugger is active!
2019-09-16 14:27:06,989 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:27:18,378 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_dates.py', reloading
2019-09-16 14:27:18,693 :: INFO ::  * Restarting with stat
2019-09-16 14:27:20,806 :: WARNING ::  * Debugger is active!
2019-09-16 14:27:20,807 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:28:27,989 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_id_sinp.py', reloading
2019-09-16 14:28:28,282 :: INFO ::  * Restarting with stat
2019-09-16 14:28:30,269 :: WARNING ::  * Debugger is active!
2019-09-16 14:28:30,269 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:29:49,058 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_types.py', reloading
2019-09-16 14:29:49,396 :: INFO ::  * Restarting with stat
2019-09-16 14:29:51,147 :: WARNING ::  * Debugger is active!
2019-09-16 14:29:51,148 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:29:59,505 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_types.py', reloading
2019-09-16 14:29:59,807 :: INFO ::  * Restarting with stat
2019-09-16 14:30:01,536 :: WARNING ::  * Debugger is active!
2019-09-16 14:30:01,537 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:31:18,215 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_types.py', reloading
2019-09-16 14:31:18,535 :: INFO ::  * Restarting with stat
2019-09-16 14:31:20,490 :: WARNING ::  * Debugger is active!
2019-09-16 14:31:20,490 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:31:25,935 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 14:31:25,936 :: INFO ::  * Restarting with stat
2019-09-16 14:31:27,471 :: WARNING ::  * Debugger is active!
2019-09-16 14:31:27,471 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:31:30,214 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:31:30] "OPTIONS /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 14:31:30,241 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:31:30,256 :: DEBUG :: import_id = 393
2019-09-16 14:31:30,256 :: DEBUG :: DB tabel name = i_data_pinv_393
2019-09-16 14:31:30,256 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 14:31:30,357 :: DEBUG :: row number = 161851
2019-09-16 14:31:30,358 :: INFO :: type of dataframe = pandas
2019-09-16 14:31:30,358 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:31:30,377 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:31:30,442 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.084038
2019-09-16 14:31:42,638 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:31:42,639 :: INFO :: * START DATA CLEANING
2019-09-16 14:31:42,659 :: INFO :: checking missing values : 
2019-09-16 14:31:43,069 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:31:43,098 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:31:45,656 :: INFO :: - checking missing values for date_min column
2019-09-16 14:31:45,681 :: INFO :: Data cleaning : missing values checked - time : 0:00:03.021409
2019-09-16 14:31:45,682 :: ERROR :: *** ERROR IN CORRESPONDANCE MAPPING
2019-09-16 14:31:45,682 :: ERROR :: check_types() missing 1 required positional argument: 'df_type'
Traceback (most recent call last):
  File "/home/ju/geonature/external_modules/import/backend/blueprint.py", line 563, in postMapping
    transform_errors = data_cleaning(df, selected_columns, MISSING_VALUES, DEFAULT_COUNT_VALUE, df_type)
  File "/home/ju/geonature/external_modules/import/backend/transform/transform.py", line 36, in data_cleaning
    error_types = check_types(df, selected_columns, synthese_info, missing_val)
  File "/home/ju/geonature/external_modules/import/backend/wrappers.py", line 9, in wrapper
    res = func(*args, **kw)
TypeError: check_types() missing 1 required positional argument: 'df_type'
2019-09-16 14:31:46,529 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:31:46] "POST /import/mapping/393 HTTP/1.1" 500 -
2019-09-16 14:32:05,380 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_types.py', reloading
2019-09-16 14:32:05,758 :: INFO ::  * Restarting with stat
2019-09-16 14:32:07,382 :: WARNING ::  * Debugger is active!
2019-09-16 14:32:07,382 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:32:24,005 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 14:32:24,305 :: INFO ::  * Restarting with stat
2019-09-16 14:32:25,946 :: WARNING ::  * Debugger is active!
2019-09-16 14:32:25,946 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 14:32:29,256 :: INFO :: 127.0.0.1 - - [16/Sep/2019 14:32:29] "OPTIONS /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 14:32:29,290 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 14:32:29,304 :: DEBUG :: import_id = 393
2019-09-16 14:32:29,305 :: DEBUG :: DB tabel name = i_data_pinv_393
2019-09-16 14:32:29,305 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 14:32:29,420 :: DEBUG :: row number = 161851
2019-09-16 14:32:29,420 :: INFO :: type of dataframe = pandas
2019-09-16 14:32:29,420 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:32:29,439 :: WARNING :: ncores used by Dask = 4
2019-09-16 14:32:29,516 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.095308
2019-09-16 14:32:40,688 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 14:32:40,689 :: INFO :: * START DATA CLEANING
2019-09-16 14:32:40,711 :: INFO :: checking missing values : 
2019-09-16 14:32:41,108 :: INFO :: - checking missing values for cd_nom column
2019-09-16 14:32:41,139 :: INFO :: - checking missing values for nom_cite column
2019-09-16 14:32:43,558 :: INFO :: - checking missing values for date_min column
2019-09-16 14:32:43,582 :: INFO :: Data cleaning : missing values checked - time : 0:00:02.871481
2019-09-16 14:32:43,582 :: INFO :: checking types : 
2019-09-16 14:32:43,583 :: INFO :: - checking date type for date_min column
2019-09-16 14:32:44,747 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:00:44,309 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_types.py', reloading
2019-09-16 15:00:45,302 :: INFO ::  * Restarting with stat
2019-09-16 15:00:48,366 :: WARNING ::  * Debugger is active!
2019-09-16 15:00:48,367 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:00:56,264 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:00:56] "OPTIONS /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 15:00:56,303 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:00:56,319 :: DEBUG :: import_id = 393
2019-09-16 15:00:56,319 :: DEBUG :: DB tabel name = i_data_pinv_393
2019-09-16 15:00:56,319 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 15:00:56,421 :: DEBUG :: row number = 161851
2019-09-16 15:00:56,421 :: INFO :: type of dataframe = pandas
2019-09-16 15:00:56,422 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:00:56,442 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:00:56,512 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.090618
2019-09-16 15:01:04,433 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 15:01:04,434 :: INFO ::  * Restarting with stat
2019-09-16 15:01:06,591 :: WARNING ::  * Debugger is active!
2019-09-16 15:01:06,592 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:01:09,528 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:01:09] "OPTIONS /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 15:01:09,562 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:01:09,578 :: DEBUG :: import_id = 393
2019-09-16 15:01:09,579 :: DEBUG :: DB tabel name = i_data_pinv_393
2019-09-16 15:01:09,579 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 15:01:09,682 :: DEBUG :: row number = 161851
2019-09-16 15:01:09,682 :: INFO :: type of dataframe = pandas
2019-09-16 15:01:09,683 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:01:09,703 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:01:09,769 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.086128
2019-09-16 15:01:22,369 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:01:22,370 :: INFO :: * START DATA CLEANING
2019-09-16 15:01:22,393 :: INFO :: checking missing values : 
2019-09-16 15:01:22,791 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:01:22,822 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:01:25,405 :: INFO :: - checking missing values for date_min column
2019-09-16 15:01:25,430 :: INFO :: Data cleaning : missing values checked - time : 0:00:03.037017
2019-09-16 15:01:25,430 :: INFO :: checking types : 
2019-09-16 15:01:25,430 :: INFO :: - checking date type for date_min column
2019-09-16 15:01:26,632 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:06:59,918 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_types.py', reloading
2019-09-16 15:07:00,377 :: INFO ::  * Restarting with stat
2019-09-16 15:07:03,280 :: WARNING ::  * Debugger is active!
2019-09-16 15:07:03,280 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:07:05,668 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:07:05] "OPTIONS /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 15:07:05,700 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:07:05,714 :: DEBUG :: import_id = 393
2019-09-16 15:07:05,714 :: DEBUG :: DB tabel name = i_data_pinv_393
2019-09-16 15:07:05,714 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 15:07:05,848 :: DEBUG :: row number = 161851
2019-09-16 15:07:05,848 :: INFO :: type of dataframe = pandas
2019-09-16 15:07:05,848 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:07:05,874 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:07:05,959 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.110167
2019-09-16 15:07:17,536 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:07:17,537 :: INFO :: * START DATA CLEANING
2019-09-16 15:07:17,559 :: INFO :: checking missing values : 
2019-09-16 15:07:17,948 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:07:17,978 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:07:20,430 :: INFO :: - checking missing values for date_min column
2019-09-16 15:07:20,452 :: INFO :: Data cleaning : missing values checked - time : 0:00:02.892682
2019-09-16 15:07:20,453 :: INFO :: checking types : 
2019-09-16 15:07:20,453 :: INFO :: - checking date type for date_min column
2019-09-16 15:07:21,658 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:07:21,905 :: INFO :: Data cleaning : type of values checked - time : 0:00:01.452325
2019-09-16 15:07:21,905 :: INFO :: checking cd_nom validity for cd_nom column
2019-09-16 15:07:23,520 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:01.615018
2019-09-16 15:07:23,521 :: INFO :: checking date validity :
2019-09-16 15:07:23,521 :: INFO :: - checking if date_max column is missing
2019-09-16 15:07:23,521 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000443
2019-09-16 15:07:23,521 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 15:07:24,629 :: INFO :: Data cleaning : uuid values checked - time : 0:00:01.107819
2019-09-16 15:07:24,630 :: INFO :: checking count_min and count_max : 
2019-09-16 15:07:24,630 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000218
2019-09-16 15:07:24,630 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000013
2019-09-16 15:07:24,630 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000010
2019-09-16 15:07:24,630 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_cite
2019-09-16 15:07:24,631 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_cite
2019-09-16 15:07:25,047 :: INFO :: * END DATA CLEANING
2019-09-16 15:07:25,048 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:07:25,048 :: INFO :: loading dataframe into DB table:
2019-09-16 15:07:44,837 :: INFO :: dask df loaded to db table - time : 0:00:19.788287
2019-09-16 15:07:46,183 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:21.134551
2019-09-16 15:07:46,183 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:07:46,333 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 15:07:47,041 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:07:47] "POST /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 15:08:06,356 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 15:08:06,356 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:08:07,814 :: INFO :: File size = 159.84538650512695 Mo
2019-09-16 15:08:08,162 :: DEBUG :: original file name = data_PINV.csv
2019-09-16 15:08:08,162 :: INFO :: Upload : file saved in directory - time : 0:00:01.806022
2019-09-16 15:08:08,163 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:08:08,163 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 15:08:35,575 :: DEBUG :: column_names = ['idSynthese', 'geojson_4326', 'geojson_local', 'jddId', 'id_digitiser', 'Id_SINP', 'Id_producteur', 'wkt', 'x_centroid', 'y_centroid', 'Alti_min', 'Alti_max', 'Jeu_donnees', 'Acteurs', 'Observateurs', 'Determinateur', 'Date_debut', 'Date_fin', 'Nom_valide', 'Nom_vern', 'Cd_Ref', 'Nom_cite', 'Cd_Nom', 'Methode_obs', 'Meth_deter', 'Preuve', 'Stade_vie', 'Sexe', 'Etat_bio', 'Nombre_min', 'Nombre_max', 'Objet_dnbr', 'Type_dnbr', 'Com_releve', 'Com_occurrence', 'Stat_Valid', 'Validateur', 'Integr_sinp', 'Dern_maj', 'id_acquisition_framework', 'lastact', 'vTAXREF', 'permIdGrp', 'sampleNumb', 'preuvNum', 'preuvNoNum', 'ObjGeoTyp', 'methGrp', 'obsTech', 'ocNat', 'difNivPrec', 'sensiNiv', 'statObs', 'dEEFlou', 'statSource', 'typInfGeo']
2019-09-16 15:08:35,576 :: DEBUG :: row_count = 161852
2019-09-16 15:08:35,576 :: INFO :: User file validity checked - time : 0:00:27.413484
2019-09-16 15:08:35,577 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 15:08:35,611 :: DEBUG :: full DB user table name = gn_imports.i_data_pinv_393
2019-09-16 15:08:37,138 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:08:48,214 :: INFO :: CSV loaded to DB table - time : 0:00:11.075490
2019-09-16 15:08:48,215 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:08:48,215 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:09:02,137 :: INFO :: CSV loaded to DB table - time : 0:00:13.921765
2019-09-16 15:09:02,137 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:09:02,541 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 15:09:02,582 :: INFO :: Total time to post user file and fill metadata - time : 0:00:56.225875
2019-09-16 15:09:02,616 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:09:02] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 15:09:02,662 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:09:02] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 15:09:08,526 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:09:08] "OPTIONS /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 15:09:08,563 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:09:08,573 :: DEBUG :: import_id = 393
2019-09-16 15:09:08,574 :: DEBUG :: DB tabel name = i_data_pinv_393
2019-09-16 15:09:08,574 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 15:09:08,842 :: DEBUG :: row number = 161851
2019-09-16 15:09:08,842 :: INFO :: type of dataframe = pandas
2019-09-16 15:09:08,842 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:09:08,856 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:09:08,929 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.087075
2019-09-16 15:09:20,610 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:09:20,611 :: INFO :: * START DATA CLEANING
2019-09-16 15:09:20,637 :: INFO :: checking missing values : 
2019-09-16 15:09:21,064 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:09:21,096 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:09:24,296 :: INFO :: - checking missing values for date_min column
2019-09-16 15:09:24,324 :: INFO :: Data cleaning : missing values checked - time : 0:00:03.687144
2019-09-16 15:09:24,325 :: INFO :: checking types : 
2019-09-16 15:09:24,326 :: INFO :: - checking date type for date_min column
2019-09-16 15:09:24,832 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:09:25,199 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.873738
2019-09-16 15:09:25,199 :: INFO :: checking cd_nom validity for cd_nom column
2019-09-16 15:09:27,408 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:02.208908
2019-09-16 15:09:27,408 :: INFO :: checking date validity :
2019-09-16 15:09:27,408 :: INFO :: - checking if date_max column is missing
2019-09-16 15:09:27,409 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000333
2019-09-16 15:09:27,409 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 15:09:29,184 :: INFO :: Data cleaning : uuid values checked - time : 0:00:01.775039
2019-09-16 15:09:29,185 :: INFO :: checking count_min and count_max : 
2019-09-16 15:09:29,186 :: INFO :: Data cleaning : counts checked - time : 0:00:00.001151
2019-09-16 15:09:29,188 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000089
2019-09-16 15:09:29,189 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000023
2019-09-16 15:09:29,189 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_cite
2019-09-16 15:09:29,189 :: DEBUG :: USER ERRORS : texte trop long dans la colonne nom_cite
2019-09-16 15:09:31,379 :: INFO :: * END DATA CLEANING
2019-09-16 15:09:31,379 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:09:31,379 :: INFO :: loading dataframe into DB table:
2019-09-16 15:09:57,005 :: INFO :: dask df loaded to db table - time : 0:00:25.625952
2019-09-16 15:09:58,964 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:27.585195
2019-09-16 15:09:58,965 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:09:59,120 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 15:09:59,828 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:09:59] "POST /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 15:10:19,679 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_types.py', reloading
2019-09-16 15:10:20,363 :: INFO ::  * Restarting with stat
2019-09-16 15:10:44,504 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 15:10:44,528 :: INFO ::  * Restarting with stat
2019-09-16 15:10:46,795 :: WARNING ::  * Debugger is active!
2019-09-16 15:10:46,822 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:11:05,588 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:11:05] "OPTIONS /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 15:11:05,616 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:11:05,634 :: DEBUG :: import_id = 393
2019-09-16 15:11:05,634 :: DEBUG :: DB tabel name = i_data_pinv_393
2019-09-16 15:11:05,634 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 15:11:05,765 :: DEBUG :: row number = 161851
2019-09-16 15:11:05,766 :: INFO :: type of dataframe = pandas
2019-09-16 15:11:05,766 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:11:05,785 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:11:05,874 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.108428
2019-09-16 15:11:20,132 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:11:20,133 :: INFO :: * START DATA CLEANING
2019-09-16 15:11:20,155 :: INFO :: checking missing values : 
2019-09-16 15:11:20,587 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:11:20,624 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:11:23,165 :: INFO :: - checking missing values for date_min column
2019-09-16 15:11:23,186 :: INFO :: Data cleaning : missing values checked - time : 0:00:03.030623
2019-09-16 15:11:23,186 :: INFO :: checking types : 
2019-09-16 15:11:23,186 :: INFO :: - checking date type for date_min column
2019-09-16 15:11:24,415 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:13:56,187 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 15:13:56,188 :: INFO ::  * Restarting with stat
2019-09-16 15:13:58,034 :: WARNING ::  * Debugger is active!
2019-09-16 15:13:58,034 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:16:10,870 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 15:16:10,871 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:16:10,875 :: INFO :: File size = 0.05043315887451172 Mo
2019-09-16 15:16:10,878 :: DEBUG :: original file name = data_exemple_nom_cite.csv
2019-09-16 15:16:10,879 :: INFO :: Upload : file saved in directory - time : 0:00:00.007945
2019-09-16 15:16:10,880 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:16:10,880 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 15:16:11,485 :: DEBUG :: column_names = ['id_synthese', 'unique_id_sinp', 'unique_id_sinp_grp', 'id_source', 'entity_source_pk_value', 'id_dataset', 'id_nomenclature_geo_object_nature', 'id_nomenclature_grp_typ', 'id_nomenclature_obs_meth', 'id_nomenclature_obs_technique', 'id_nomenclature_bio_status', 'id_nomenclature_bio_condition', 'id_nomenclature_naturalness', 'id_nomenclature_exist_proof', 'id_nomenclature_valid_status', 'id_nomenclature_diffusion_level', 'id_nomenclature_life_stage', 'id_nomenclature_sex', 'id_nomenclature_obj_count', 'id_nomenclature_type_count', 'id_nomenclature_sensitivity', 'id_nomenclature_observation_status', 'id_nomenclature_blurring', 'id_nomenclature_source_status', 'id_nomenclature_info_geo_type', 'count_min', 'count_max', 'cd_nom', 'nom_cite', 'meta_v_taxref', 'sample_number_proof', 'digital_proof', 'non_digital_proof', 'altitude_min', 'altitude_max', 'the_geom_4326', 'the_geom_point', 'the_geom_local', 'date_min', 'date_max', 'validator', 'validation_comment', 'observers', 'determiner', 'id_digitiser', 'id_nomenclature_determination_method', 'meta_validation_date', 'meta_create_date', 'meta_update_date', 'last_action', 'id_module', 'comment_context', 'comment_description']
2019-09-16 15:16:11,485 :: DEBUG :: row_count = 98
2019-09-16 15:16:11,485 :: INFO :: User file validity checked - time : 0:00:00.605084
2019-09-16 15:16:11,486 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 15:16:11,496 :: DEBUG :: full DB user table name = gn_imports.i_data_exemple_nom_cite_393
2019-09-16 15:16:12,739 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:16:12,746 :: INFO :: CSV loaded to DB table - time : 0:00:00.006020
2019-09-16 15:16:12,747 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:16:12,747 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:16:12,751 :: INFO :: CSV loaded to DB table - time : 0:00:00.003783
2019-09-16 15:16:12,752 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:16:12,795 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 15:16:12,796 :: INFO :: Total time to post user file and fill metadata - time : 0:00:01.925370
2019-09-16 15:16:12,799 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:16:12] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 15:16:12,842 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:16:12] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 15:16:21,335 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:16:21] "OPTIONS /import/mapping/393 HTTP/1.1" 200 -
2019-09-16 15:16:21,360 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:16:21,373 :: DEBUG :: import_id = 393
2019-09-16 15:16:21,373 :: DEBUG :: DB tabel name = i_data_exemple_nom_cite_393
2019-09-16 15:16:21,374 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_min'}
2019-09-16 15:16:21,377 :: DEBUG :: row number = 97
2019-09-16 15:16:21,377 :: INFO :: type of dataframe = pandas
2019-09-16 15:16:21,377 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:16:21,410 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:16:21,516 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.138833
2019-09-16 15:16:21,835 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:16:21,836 :: INFO :: * START DATA CLEANING
2019-09-16 15:16:21,847 :: INFO :: checking missing values : 
2019-09-16 15:16:21,854 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:16:21,856 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:16:21,878 :: INFO :: - checking missing values for date_min column
2019-09-16 15:16:21,880 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.032320
2019-09-16 15:16:21,880 :: INFO :: checking types : 
2019-09-16 15:16:21,880 :: INFO :: - checking date type for date_min column
2019-09-16 15:16:21,918 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:21:33,185 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 15:21:33,186 :: INFO ::  * Restarting with stat
2019-09-16 15:21:34,904 :: WARNING ::  * Debugger is active!
2019-09-16 15:21:34,904 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:21:35,897 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:21:35] "GET /import/cancel_import/393 HTTP/1.1" 200 -
2019-09-16 15:21:35,989 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:21:35] "GET /import/delete_step1 HTTP/1.1" 200 -
2019-09-16 15:21:36,015 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:21:36] "GET /import HTTP/1.1" 200 -
2019-09-16 15:21:40,967 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:21:40] "GET /import/datasets HTTP/1.1" 200 -
2019-09-16 15:22:07,956 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 15:22:07,956 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:22:08,126 :: INFO :: File size = 10.37869644165039 Mo
2019-09-16 15:22:08,144 :: DEBUG :: original file name = data_pf_observado_corrected.csv
2019-09-16 15:22:08,144 :: INFO :: Upload : file saved in directory - time : 0:00:00.187630
2019-09-16 15:22:08,145 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:22:08,145 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 15:22:11,256 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'my_date', 'heure', 'date_encodage', 'my_timestamp', 'type_observation', 'nombre', 'my_min', 'my_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'latit', 'longit', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'my_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 15:22:11,257 :: DEBUG :: row_count = 26516
2019-09-16 15:22:11,257 :: INFO :: User file validity checked - time : 0:00:03.112306
2019-09-16 15:22:11,258 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 15:22:11,456 :: DEBUG :: id_import = 394
2019-09-16 15:22:11,457 :: DEBUG :: id_role = 1
2019-09-16 15:22:11,817 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_corrected_394
2019-09-16 15:22:12,849 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:22:13,518 :: INFO :: CSV loaded to DB table - time : 0:00:00.668567
2019-09-16 15:22:13,518 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:22:13,518 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:22:14,383 :: INFO :: CSV loaded to DB table - time : 0:00:00.864429
2019-09-16 15:22:14,383 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:22:14,791 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 15:22:14,795 :: INFO :: Total time to post user file and fill metadata - time : 0:00:06.838480
2019-09-16 15:22:14,798 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:22:14] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 15:22:14,844 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:22:14] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 15:25:16,845 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 15:25:16,845 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:25:17,041 :: INFO :: File size = 10.37869644165039 Mo
2019-09-16 15:25:17,081 :: DEBUG :: original file name = data_pf_observado_corrected.csv
2019-09-16 15:25:17,083 :: INFO :: Upload : file saved in directory - time : 0:00:00.237537
2019-09-16 15:25:17,084 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:25:17,084 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 15:25:20,926 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'my_date', 'heure', 'date_encodage', 'my_timestamp', 'type_observation', 'nombre', 'my_min', 'my_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'latit', 'longit', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'my_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 15:25:20,926 :: DEBUG :: row_count = 26516
2019-09-16 15:25:20,927 :: INFO :: User file validity checked - time : 0:00:03.841501
2019-09-16 15:25:20,927 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 15:25:20,935 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_corrected_394
2019-09-16 15:25:22,780 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:25:23,689 :: INFO :: CSV loaded to DB table - time : 0:00:00.908437
2019-09-16 15:25:23,691 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:25:23,692 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:25:25,018 :: INFO :: CSV loaded to DB table - time : 0:00:01.324649
2019-09-16 15:25:25,019 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:25:25,377 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 15:25:25,390 :: INFO :: Total time to post user file and fill metadata - time : 0:00:08.545309
2019-09-16 15:25:25,409 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:25:25] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 15:25:25,514 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:25:25] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 15:28:42,841 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:28:42] "OPTIONS /import/mapping/394 HTTP/1.1" 200 -
2019-09-16 15:28:42,876 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:28:42,895 :: DEBUG :: import_id = 394
2019-09-16 15:28:42,897 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_394
2019-09-16 15:28:42,897 :: DEBUG :: selected columns in correspondance mapping = {'entity_source_pk_value': 'id', 'count_min': 'my_min', 'count_max': 'my_max', 'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'methode_comptage', 'meta_validation_date': 'my_timestamp'}
2019-09-16 15:28:42,915 :: DEBUG :: row number = 26515
2019-09-16 15:28:42,917 :: INFO :: type of dataframe = pandas
2019-09-16 15:28:42,918 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:28:42,978 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:28:43,126 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.206345
2019-09-16 15:28:44,946 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:28:44,947 :: INFO :: * START DATA CLEANING
2019-09-16 15:28:44,960 :: INFO :: checking missing values : 
2019-09-16 15:28:45,137 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:28:45,151 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:28:45,162 :: INFO :: - checking missing values for date_min column
2019-09-16 15:28:45,172 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.211771
2019-09-16 15:28:45,173 :: INFO :: checking types : 
2019-09-16 15:28:45,175 :: INFO :: - checking date type for date_min column
2019-09-16 15:28:51,432 :: INFO :: - checking date type for meta_validation_date column
2019-09-16 15:28:51,554 :: INFO :: - checking varchar type for entity_source_pk_value column
2019-09-16 15:28:51,555 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:28:51,716 :: INFO :: - checking integer type for count_min column
2019-09-16 15:28:51,825 :: INFO :: - checking integer type for count_max column
2019-09-16 15:28:51,949 :: INFO :: Data cleaning : type of values checked - time : 0:00:06.776451
2019-09-16 15:28:51,950 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 15:28:54,182 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:02.231643
2019-09-16 15:28:54,182 :: INFO :: checking date validity :
2019-09-16 15:28:54,182 :: INFO :: - checking if date_max column is missing
2019-09-16 15:28:54,182 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000305
2019-09-16 15:28:54,183 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 15:28:54,570 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.387040
2019-09-16 15:28:54,570 :: INFO :: checking count_min and count_max : 
2019-09-16 15:28:54,575 :: INFO :: checking if count_max >= count_min
2019-09-16 15:28:54,684 :: INFO :: Data cleaning : counts checked - time : 0:00:00.114445
2019-09-16 15:28:54,685 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000011
2019-09-16 15:28:54,685 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000011
2019-09-16 15:28:54,685 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_methode_comptage
2019-09-16 15:28:54,686 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 15:28:54,973 :: INFO :: * END DATA CLEANING
2019-09-16 15:28:54,975 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:28:54,975 :: INFO :: loading dataframe into DB table:
2019-09-16 15:28:57,106 :: INFO :: dask df loaded to db table - time : 0:00:02.130748
2019-09-16 15:28:57,230 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:02.254753
2019-09-16 15:28:57,231 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:28:57,296 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 15:28:57,464 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:28:57] "POST /import/mapping/394 HTTP/1.1" 200 -
2019-09-16 15:30:29,345 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:30:29] "OPTIONS /import/mapping/394 HTTP/1.1" 200 -
2019-09-16 15:30:29,381 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:30:29,396 :: DEBUG :: import_id = 394
2019-09-16 15:30:29,397 :: DEBUG :: DB tabel name = i_data_pf_observado_corrected_394
2019-09-16 15:30:29,397 :: DEBUG :: selected columns in correspondance mapping = {'entity_source_pk_value': 'id', 'count_min': 'my_min', 'count_max': 'my_max', 'cd_nom': 'species_id', 'nom_cite': 'nom_scientifique', 'date_min': 'my_timestamp', 'meta_validation_date': 'my_timestamp'}
2019-09-16 15:30:29,416 :: DEBUG :: row number = 26515
2019-09-16 15:30:29,417 :: INFO :: type of dataframe = pandas
2019-09-16 15:30:29,417 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:30:29,449 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:30:29,554 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.137133
2019-09-16 15:30:31,452 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:30:31,453 :: INFO :: * START DATA CLEANING
2019-09-16 15:30:31,474 :: INFO :: checking missing values : 
2019-09-16 15:30:31,634 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:30:31,646 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:30:31,653 :: INFO :: - checking missing values for date_min column
2019-09-16 15:30:31,662 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.187800
2019-09-16 15:30:31,663 :: INFO :: checking types : 
2019-09-16 15:30:31,664 :: INFO :: - checking date type for date_min column
2019-09-16 15:30:31,928 :: INFO :: - checking date type for meta_validation_date column
2019-09-16 15:30:32,080 :: INFO :: - checking varchar type for entity_source_pk_value column
2019-09-16 15:30:32,082 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:30:32,171 :: INFO :: - checking integer type for count_min column
2019-09-16 15:30:32,283 :: INFO :: - checking integer type for count_max column
2019-09-16 15:30:32,397 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.733653
2019-09-16 15:30:32,397 :: INFO :: checking cd_nom validity for species_id column
2019-09-16 15:30:34,520 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:02.123477
2019-09-16 15:30:34,521 :: INFO :: checking date validity :
2019-09-16 15:30:34,521 :: INFO :: - checking if date_max column is missing
2019-09-16 15:30:34,521 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000250
2019-09-16 15:30:34,521 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 15:30:34,842 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.320822
2019-09-16 15:30:34,842 :: INFO :: checking count_min and count_max : 
2019-09-16 15:30:34,848 :: INFO :: checking if count_max >= count_min
2019-09-16 15:30:34,937 :: INFO :: Data cleaning : counts checked - time : 0:00:00.094938
2019-09-16 15:30:34,937 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000009
2019-09-16 15:30:34,938 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000007
2019-09-16 15:30:34,938 :: DEBUG :: USER ERRORS : Des cd_nom sont invalides
2019-09-16 15:30:35,200 :: INFO :: * END DATA CLEANING
2019-09-16 15:30:35,201 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:30:35,202 :: INFO :: loading dataframe into DB table:
2019-09-16 15:30:38,189 :: INFO :: dask df loaded to db table - time : 0:00:02.986426
2019-09-16 15:30:39,073 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:03.870858
2019-09-16 15:30:39,075 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:30:39,293 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 15:30:39,426 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:30:39] "POST /import/mapping/394 HTTP/1.1" 200 -
2019-09-16 15:31:42,711 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:31:42] "GET /import/cancel_import/394 HTTP/1.1" 200 -
2019-09-16 15:31:42,799 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:31:42] "GET /import/delete_step1 HTTP/1.1" 200 -
2019-09-16 15:31:42,870 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:31:42] "GET /import HTTP/1.1" 200 -
2019-09-16 15:32:06,318 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:32:06] "GET /import/datasets HTTP/1.1" 200 -
2019-09-16 15:32:31,378 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:32:31] "GET /import/datasets HTTP/1.1" 200 -
2019-09-16 15:32:52,995 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 15:32:52,995 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:32:53,242 :: INFO :: File size = 10.37869644165039 Mo
2019-09-16 15:32:53,291 :: DEBUG :: original file name = data_pf_observado_corrected.csv
2019-09-16 15:32:53,292 :: INFO :: Upload : file saved in directory - time : 0:00:00.296031
2019-09-16 15:32:53,292 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:32:53,293 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 15:32:56,901 :: DEBUG :: column_names = ['ID', 'nom_scientifique', 'nom', 'euring', 'species_id', 'famille', 'groupe_taxonomique', 'statut_sp', 'my_date', 'heure', 'date_encodage', 'my_timestamp', 'type_observation', 'nombre', 'my_min', 'my_max', 'sexe', 'methode_comptage', 'stade', 'comportement', 'coord_x', 'coord_y', 'latit', 'longit', 'biotope', 'site', 'commune', 'province', 'certain', 'echappe_captivite', 'link', 'precisie', 'loc_method', 'protocol', 'my_status', 'photos', 'numero_collection', 'plante_hote', 'substrate', 'methode', 'remarques', 'lieu', 'favoris', 'transects']
2019-09-16 15:32:56,902 :: DEBUG :: row_count = 26516
2019-09-16 15:32:56,902 :: INFO :: User file validity checked - time : 0:00:03.608645
2019-09-16 15:32:56,902 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 15:32:56,907 :: DEBUG :: id_import = 395
2019-09-16 15:32:56,908 :: DEBUG :: id_role = 1
2019-09-16 15:32:56,912 :: DEBUG :: full DB user table name = gn_imports.i_data_pf_observado_corrected_395
2019-09-16 15:32:58,062 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:32:58,713 :: INFO :: CSV loaded to DB table - time : 0:00:00.650377
2019-09-16 15:32:58,713 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:32:58,713 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:32:59,879 :: INFO :: CSV loaded to DB table - time : 0:00:01.165849
2019-09-16 15:32:59,880 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:33:00,512 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 15:33:00,516 :: INFO :: Total time to post user file and fill metadata - time : 0:00:07.521038
2019-09-16 15:33:00,522 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:33:00] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 15:33:00,628 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:33:00] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 15:42:23,193 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 15:42:23,194 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:42:23,196 :: INFO :: File size = 0.05043315887451172 Mo
2019-09-16 15:42:23,197 :: DEBUG :: original file name = data_exemple_nom_cite.csv
2019-09-16 15:42:23,197 :: INFO :: Upload : file saved in directory - time : 0:00:00.002709
2019-09-16 15:42:23,197 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:42:23,197 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 15:42:23,300 :: DEBUG :: column_names = ['id_synthese', 'unique_id_sinp', 'unique_id_sinp_grp', 'id_source', 'entity_source_pk_value', 'id_dataset', 'id_nomenclature_geo_object_nature', 'id_nomenclature_grp_typ', 'id_nomenclature_obs_meth', 'id_nomenclature_obs_technique', 'id_nomenclature_bio_status', 'id_nomenclature_bio_condition', 'id_nomenclature_naturalness', 'id_nomenclature_exist_proof', 'id_nomenclature_valid_status', 'id_nomenclature_diffusion_level', 'id_nomenclature_life_stage', 'id_nomenclature_sex', 'id_nomenclature_obj_count', 'id_nomenclature_type_count', 'id_nomenclature_sensitivity', 'id_nomenclature_observation_status', 'id_nomenclature_blurring', 'id_nomenclature_source_status', 'id_nomenclature_info_geo_type', 'count_min', 'count_max', 'cd_nom', 'nom_cite', 'meta_v_taxref', 'sample_number_proof', 'digital_proof', 'non_digital_proof', 'altitude_min', 'altitude_max', 'the_geom_4326', 'the_geom_point', 'the_geom_local', 'date_min', 'date_max', 'validator', 'validation_comment', 'observers', 'determiner', 'id_digitiser', 'id_nomenclature_determination_method', 'meta_validation_date', 'meta_create_date', 'meta_update_date', 'last_action', 'id_module', 'comment_context', 'comment_description']
2019-09-16 15:42:23,301 :: DEBUG :: row_count = 98
2019-09-16 15:42:23,302 :: INFO :: User file validity checked - time : 0:00:00.105102
2019-09-16 15:42:23,303 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 15:42:23,316 :: DEBUG :: full DB user table name = gn_imports.i_data_exemple_nom_cite_395
2019-09-16 15:42:24,082 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:42:24,094 :: INFO :: CSV loaded to DB table - time : 0:00:00.010336
2019-09-16 15:42:24,095 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:42:24,096 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:42:24,107 :: INFO :: CSV loaded to DB table - time : 0:00:00.009523
2019-09-16 15:42:24,108 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:42:24,205 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 15:42:24,207 :: INFO :: Total time to post user file and fill metadata - time : 0:00:01.013095
2019-09-16 15:42:24,213 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:42:24] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 15:42:24,334 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:42:24] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 15:42:41,481 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:42:41] "OPTIONS /import/mapping/395 HTTP/1.1" 200 -
2019-09-16 15:42:41,508 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:42:41,520 :: DEBUG :: import_id = 395
2019-09-16 15:42:41,520 :: DEBUG :: DB tabel name = i_data_exemple_nom_cite_395
2019-09-16 15:42:41,520 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'id_nomenclature_sex'}
2019-09-16 15:42:41,523 :: DEBUG :: row number = 97
2019-09-16 15:42:41,523 :: INFO :: type of dataframe = pandas
2019-09-16 15:42:41,524 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:42:41,544 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:42:41,668 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.144614
2019-09-16 15:42:42,111 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:42:42,112 :: INFO :: * START DATA CLEANING
2019-09-16 15:42:42,123 :: INFO :: checking missing values : 
2019-09-16 15:42:42,134 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:42:42,138 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:42:42,172 :: INFO :: - checking missing values for date_min column
2019-09-16 15:42:42,176 :: INFO :: Data cleaning : missing values checked - time : 0:00:00.053494
2019-09-16 15:42:42,177 :: INFO :: checking types : 
2019-09-16 15:42:42,179 :: INFO :: - checking date type for date_min column
2019-09-16 15:42:42,253 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:42:42,293 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.115619
2019-09-16 15:42:42,295 :: INFO :: checking cd_nom validity for cd_nom column
2019-09-16 15:42:44,553 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:02.258223
2019-09-16 15:42:44,554 :: INFO :: checking date validity :
2019-09-16 15:42:44,554 :: INFO :: - checking if date_max column is missing
2019-09-16 15:42:44,554 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000304
2019-09-16 15:42:44,554 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 15:42:44,557 :: INFO :: Data cleaning : uuid values checked - time : 0:00:00.002653
2019-09-16 15:42:44,557 :: INFO :: checking count_min and count_max : 
2019-09-16 15:42:44,557 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000157
2019-09-16 15:42:44,557 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 15:42:44,557 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000007
2019-09-16 15:42:44,558 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_cite
2019-09-16 15:42:44,558 :: DEBUG :: USER ERRORS : Des dates sont invalides dans la colonne gn_id_nomenclature_sex
2019-09-16 15:42:44,563 :: INFO :: * END DATA CLEANING
2019-09-16 15:42:44,564 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:42:44,564 :: INFO :: loading dataframe into DB table:
2019-09-16 15:42:44,800 :: INFO :: dask df loaded to db table - time : 0:00:00.235092
2019-09-16 15:42:44,894 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:00.330164
2019-09-16 15:42:44,896 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:42:44,915 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 15:42:44,919 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:42:44] "POST /import/mapping/395 HTTP/1.1" 200 -
2019-09-16 15:45:15,242 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_types.py', reloading
2019-09-16 15:45:15,630 :: INFO ::  * Restarting with stat
2019-09-16 15:45:17,747 :: WARNING ::  * Debugger is active!
2019-09-16 15:45:17,747 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:45:41,170 :: INFO :: *** START UPLOAD USER FILE
2019-09-16 15:45:41,170 :: INFO :: * START SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:45:42,503 :: INFO :: File size = 159.84538650512695 Mo
2019-09-16 15:45:42,814 :: DEBUG :: original file name = data_PINV.csv
2019-09-16 15:45:42,814 :: INFO :: Upload : file saved in directory - time : 0:00:01.644128
2019-09-16 15:45:42,815 :: INFO :: * END SAVE USER FILE IN UPLOAD DIRECTORY
2019-09-16 15:45:42,815 :: INFO :: * START CHECK USER FILE VALIDITY
2019-09-16 15:46:06,379 :: DEBUG :: column_names = ['idSynthese', 'geojson_4326', 'geojson_local', 'jddId', 'id_digitiser', 'Id_SINP', 'Id_producteur', 'wkt', 'x_centroid', 'y_centroid', 'Alti_min', 'Alti_max', 'Jeu_donnees', 'Acteurs', 'Observateurs', 'Determinateur', 'Date_debut', 'Date_fin', 'Nom_valide', 'Nom_vern', 'Cd_Ref', 'Nom_cite', 'Cd_Nom', 'Methode_obs', 'Meth_deter', 'Preuve', 'Stade_vie', 'Sexe', 'Etat_bio', 'Nombre_min', 'Nombre_max', 'Objet_dnbr', 'Type_dnbr', 'Com_releve', 'Com_occurrence', 'Stat_Valid', 'Validateur', 'Integr_sinp', 'Dern_maj', 'id_acquisition_framework', 'lastact', 'vTAXREF', 'permIdGrp', 'sampleNumb', 'preuvNum', 'preuvNoNum', 'ObjGeoTyp', 'methGrp', 'obsTech', 'ocNat', 'difNivPrec', 'sensiNiv', 'statObs', 'dEEFlou', 'statSource', 'typInfGeo']
2019-09-16 15:46:06,379 :: DEBUG :: row_count = 161852
2019-09-16 15:46:06,380 :: INFO :: User file validity checked - time : 0:00:23.564695
2019-09-16 15:46:06,380 :: INFO :: * END CHECK USER FILE VALIDITY
2019-09-16 15:46:06,390 :: DEBUG :: full DB user table name = gn_imports.i_data_pinv_395
2019-09-16 15:46:07,235 :: INFO :: * START COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:46:15,790 :: INFO :: CSV loaded to DB table - time : 0:00:08.554258
2019-09-16 15:46:15,790 :: INFO :: * END COPY FROM CSV TO ARCHIVES SCHEMA
2019-09-16 15:46:15,790 :: INFO :: * START COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:46:28,938 :: INFO :: CSV loaded to DB table - time : 0:00:13.147518
2019-09-16 15:46:28,939 :: INFO :: * END COPY FROM CSV TO IMPORTS SCHEMA
2019-09-16 15:46:29,500 :: INFO :: *** END UPLOAD USER FILE
2019-09-16 15:46:29,649 :: INFO :: Total time to post user file and fill metadata - time : 0:00:48.479322
2019-09-16 15:46:29,685 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:46:29] "POST /import/uploads HTTP/1.1" 200 -
2019-09-16 15:46:29,732 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:46:29] "GET /import/syntheseInfo HTTP/1.1" 200 -
2019-09-16 15:46:52,747 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:46:52] "OPTIONS /import/mapping/395 HTTP/1.1" 200 -
2019-09-16 15:46:52,774 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:46:52,787 :: DEBUG :: import_id = 395
2019-09-16 15:46:52,787 :: DEBUG :: DB tabel name = i_data_pinv_395
2019-09-16 15:46:52,788 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 15:46:53,104 :: DEBUG :: row number = 161851
2019-09-16 15:46:53,104 :: INFO :: type of dataframe = pandas
2019-09-16 15:46:53,104 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:46:53,123 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:46:53,223 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.118788
2019-09-16 15:47:05,107 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:47:05,108 :: INFO :: * START DATA CLEANING
2019-09-16 15:47:05,128 :: INFO :: checking missing values : 
2019-09-16 15:47:05,542 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:47:05,574 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:47:07,991 :: INFO :: - checking missing values for date_min column
2019-09-16 15:47:08,017 :: INFO :: Data cleaning : missing values checked - time : 0:00:02.888708
2019-09-16 15:47:08,017 :: INFO :: checking types : 
2019-09-16 15:47:08,018 :: INFO :: - checking date type for date_min column
2019-09-16 15:47:08,537 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:47:08,805 :: INFO :: Data cleaning : type of values checked - time : 0:00:00.787835
2019-09-16 15:47:08,805 :: INFO :: checking cd_nom validity for cd_nom column
2019-09-16 15:47:10,583 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:01.777909
2019-09-16 15:47:10,584 :: INFO :: checking date validity :
2019-09-16 15:47:10,584 :: INFO :: - checking if date_max column is missing
2019-09-16 15:47:10,584 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000458
2019-09-16 15:47:10,584 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 15:47:11,636 :: INFO :: Data cleaning : uuid values checked - time : 0:00:01.051667
2019-09-16 15:47:11,636 :: INFO :: checking count_min and count_max : 
2019-09-16 15:47:11,637 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000122
2019-09-16 15:47:11,637 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 15:47:11,637 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000006
2019-09-16 15:47:11,637 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_cite
2019-09-16 15:47:13,766 :: INFO :: * END DATA CLEANING
2019-09-16 15:47:13,767 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:47:13,767 :: INFO :: loading dataframe into DB table:
2019-09-16 15:47:39,493 :: INFO :: dask df loaded to db table - time : 0:00:25.725549
2019-09-16 15:47:41,151 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:27.383582
2019-09-16 15:47:41,151 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:47:41,320 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 15:47:42,264 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:47:42] "POST /import/mapping/395 HTTP/1.1" 200 -
2019-09-16 15:49:16,878 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 15:49:17,691 :: INFO ::  * Restarting with stat
2019-09-16 15:49:35,972 :: WARNING ::  * Debugger is active!
2019-09-16 15:49:35,984 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:49:36,016 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:49:36] "OPTIONS /import/mapping/395 HTTP/1.1" 200 -
2019-09-16 15:49:36,054 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 15:49:36,087 :: DEBUG :: import_id = 395
2019-09-16 15:49:36,087 :: DEBUG :: DB tabel name = i_data_pinv_395
2019-09-16 15:49:36,088 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 15:49:36,258 :: DEBUG :: row number = 161851
2019-09-16 15:49:36,258 :: INFO :: type of dataframe = dask
2019-09-16 15:49:36,258 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:49:36,287 :: WARNING :: ncores used by Dask = 4
2019-09-16 15:49:36,406 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.147962
2019-09-16 15:49:36,406 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 15:49:36,407 :: INFO :: * START DATA CLEANING
2019-09-16 15:49:36,534 :: INFO :: checking missing values : 
2019-09-16 15:49:36,689 :: INFO :: - checking missing values for cd_nom column
2019-09-16 15:49:57,859 :: INFO :: - checking missing values for nom_cite column
2019-09-16 15:50:22,909 :: INFO :: - checking missing values for date_min column
2019-09-16 15:50:51,999 :: INFO :: Data cleaning : missing values checked - time : 0:01:15.465077
2019-09-16 15:50:51,999 :: INFO :: checking types : 
2019-09-16 15:50:51,999 :: INFO :: - checking date type for date_min column
2019-09-16 15:51:27,383 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 15:52:06,831 :: INFO :: Data cleaning : type of values checked - time : 0:01:14.831599
2019-09-16 15:52:06,831 :: INFO :: checking cd_nom validity for cd_nom column
2019-09-16 15:52:57,823 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:50.991682
2019-09-16 15:52:57,824 :: INFO :: checking date validity :
2019-09-16 15:52:57,824 :: INFO :: - checking if date_max column is missing
2019-09-16 15:52:57,824 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000765
2019-09-16 15:52:57,824 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 15:52:58,919 :: INFO :: Data cleaning : uuid values checked - time : 0:00:01.094914
2019-09-16 15:52:58,920 :: INFO :: checking count_min and count_max : 
2019-09-16 15:52:58,920 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000182
2019-09-16 15:52:58,920 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000007
2019-09-16 15:52:58,920 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000010
2019-09-16 15:52:58,920 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_cite
2019-09-16 15:52:58,958 :: INFO :: * END DATA CLEANING
2019-09-16 15:52:58,958 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:52:58,958 :: INFO :: converting dask dataframe to pandas dataframe:
2019-09-16 15:53:47,456 :: INFO :: dask df converted in pandas df - time : 0:00:48.497512
2019-09-16 15:53:47,456 :: INFO :: loading dataframe into DB table:
2019-09-16 15:54:15,154 :: INFO :: dask df loaded to db table - time : 0:00:27.697336
2019-09-16 15:54:19,847 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:01:20.889044
2019-09-16 15:54:19,865 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 15:54:20,542 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 15:54:21,666 :: INFO :: 127.0.0.1 - - [16/Sep/2019 15:54:21] "POST /import/mapping/395 HTTP/1.1" 200 -
2019-09-16 15:55:29,233 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/utils.py', reloading
2019-09-16 15:55:31,019 :: INFO ::  * Restarting with stat
2019-09-16 15:55:49,550 :: WARNING ::  * Debugger is active!
2019-09-16 15:55:49,569 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:55:55,798 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/utils.py', reloading
2019-09-16 15:55:56,119 :: INFO ::  * Restarting with stat
2019-09-16 15:55:57,863 :: WARNING ::  * Debugger is active!
2019-09-16 15:55:57,863 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 15:57:07,591 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/load/load.py', reloading
2019-09-16 15:57:07,920 :: INFO ::  * Restarting with stat
2019-09-16 15:58:19,055 :: INFO ::  * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)
2019-09-16 15:58:19,055 :: INFO ::  * Restarting with stat
2019-09-16 15:58:20,419 :: WARNING ::  * Debugger is active!
2019-09-16 15:58:20,419 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 16:21:43,439 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/transform.py', reloading
2019-09-16 16:21:43,707 :: INFO ::  * Restarting with stat
2019-09-16 16:21:45,915 :: WARNING ::  * Debugger is active!
2019-09-16 16:21:45,916 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 16:21:56,289 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 16:21:56,539 :: INFO ::  * Restarting with stat
2019-09-16 16:21:57,971 :: WARNING ::  * Debugger is active!
2019-09-16 16:21:57,971 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 16:24:18,306 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/blueprint.py', reloading
2019-09-16 16:24:18,556 :: INFO ::  * Restarting with stat
2019-09-16 16:24:19,961 :: WARNING ::  * Debugger is active!
2019-09-16 16:24:19,962 :: INFO ::  * Debugger PIN: 125-826-543
2019-09-16 16:24:30,695 :: INFO :: 127.0.0.1 - - [16/Sep/2019 16:24:30] "OPTIONS /import/mapping/395 HTTP/1.1" 200 -
2019-09-16 16:24:30,740 :: INFO :: *** START CORRESPONDANCE MAPPING
2019-09-16 16:24:30,764 :: DEBUG :: import_id = 395
2019-09-16 16:24:30,765 :: DEBUG :: DB tabel name = i_data_pinv_395
2019-09-16 16:24:30,765 :: DEBUG :: selected columns in correspondance mapping = {'cd_nom': 'cd_nom', 'nom_cite': 'nom_cite', 'date_min': 'date_debut'}
2019-09-16 16:24:30,877 :: DEBUG :: row number = 161851
2019-09-16 16:24:30,878 :: INFO :: type of dataframe = pandas
2019-09-16 16:24:30,878 :: INFO :: * START EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 16:24:30,896 :: WARNING :: ncores used by Dask = 4
2019-09-16 16:24:30,964 :: INFO :: Extracted (from DB table to Dask dataframe) - time : 0:00:00.085978
2019-09-16 16:24:42,610 :: INFO :: dask df converted in pandas df - time : 0:00:11.645937
2019-09-16 16:24:42,612 :: INFO :: * END EXTRACT FROM DB TABLE TO PYTHON
2019-09-16 16:24:42,613 :: INFO :: * START DATA CLEANING
2019-09-16 16:24:42,640 :: INFO :: checking missing values : 
2019-09-16 16:24:43,037 :: INFO :: - checking missing values for cd_nom column
2019-09-16 16:24:43,083 :: INFO :: - checking missing values for nom_cite column
2019-09-16 16:24:45,682 :: INFO :: - checking missing values for date_min column
2019-09-16 16:24:45,702 :: INFO :: Data cleaning : missing values checked - time : 0:00:03.061958
2019-09-16 16:24:45,703 :: INFO :: checking types : 
2019-09-16 16:24:45,703 :: INFO :: - checking date type for date_min column
2019-09-16 16:24:46,895 :: INFO :: - checking varchar type for nom_cite column
2019-09-16 16:24:47,143 :: INFO :: Data cleaning : type of values checked - time : 0:00:01.440366
2019-09-16 16:24:47,144 :: INFO :: checking cd_nom validity for cd_nom column
2019-09-16 16:24:50,287 :: INFO :: Data cleaning : cd_nom checked - time : 0:00:03.143267
2019-09-16 16:24:50,288 :: INFO :: checking date validity :
2019-09-16 16:24:50,288 :: INFO :: - checking if date_max column is missing
2019-09-16 16:24:50,288 :: INFO :: Data cleaning : dates checked - time : 0:00:00.000763
2019-09-16 16:24:50,289 :: INFO :: no unique_id_sinp in user data: creating uuid for each row
2019-09-16 16:24:51,474 :: INFO :: Data cleaning : uuid values checked - time : 0:00:01.184728
2019-09-16 16:24:51,474 :: INFO :: checking count_min and count_max : 
2019-09-16 16:24:51,474 :: INFO :: Data cleaning : counts checked - time : 0:00:00.000198
2019-09-16 16:24:51,475 :: INFO :: Data cleaning : altitudes checked - time : 0:00:00.000010
2019-09-16 16:24:51,475 :: INFO :: Data cleaning : other_field operations checked - time : 0:00:00.000009
2019-09-16 16:24:51,475 :: DEBUG :: USER ERRORS : Des valeurs manquantes dans la colonne nom_cite
2019-09-16 16:24:51,886 :: INFO :: * END DATA CLEANING
2019-09-16 16:24:51,886 :: INFO :: * START LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 16:24:51,887 :: INFO :: loading dataframe into DB table:
2019-09-16 16:25:11,803 :: INFO :: dask df loaded to db table - time : 0:00:19.916068
2019-09-16 16:25:14,403 :: INFO :: Loaded (from Python dataframe to DB table) - time : 0:00:22.516315
2019-09-16 16:25:14,404 :: INFO :: * END LOAD PYTHON DATAFRAME TO DB TABLE
2019-09-16 16:25:14,715 :: INFO :: *** END CORRESPONDANCE MAPPING
2019-09-16 16:25:15,438 :: INFO :: 127.0.0.1 - - [16/Sep/2019 16:25:15] "POST /import/mapping/395 HTTP/1.1" 200 -
2019-09-16 16:27:48,876 :: INFO ::  * Detected change in '/home/ju/geonature/external_modules/import/backend/transform/check_counts.py', reloading
2019-09-16 16:27:49,186 :: INFO ::  * Restarting with stat
2019-09-16 16:27:50,755 :: WARNING ::  * Debugger is active!
2019-09-16 16:27:50,756 :: INFO ::  * Debugger PIN: 125-826-543
